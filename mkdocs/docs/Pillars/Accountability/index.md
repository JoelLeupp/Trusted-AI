# Accountability and Transparency

While there has been a push to make developers more cognizant of the possible repercussions of their algorithms, others point out that public agencies and companies reliant on AI also need to be accountable.

“There is this emphasis on designers understanding a system. But it’s also about the people administering and implementing the system,” says Jason Schultz, a professor of law at New York University who works with the AI Now Institute on legal and policy issues. "That’s where the rubber meets the road in accountability. A government agency using AI has the most responsibility and they need to understand it, too. If you can’t understand the technology, you shouldn’t be able to use it.”

To that end, AI Now is promoting the use of “algorithmic impact assessments,” which would require public agencies to disclose the systems they’re using, and allow outside researchers to analyze them for potential problems. When it comes to police departments, some legal experts think it’s also important for them to clearly spell out how they’re using technology and be willing to share that with the local community.

Which metrics should be used in order to communicate the quality of a model to a 

Mangement requires interpretability to gain comfort and build confidence that they should deploy the system

Greater visibility uncovers unknown vulnerabilities and flaws.

Why does this enable governance?

Increased transparency provides information for AI consumers to better understand how an AI model or service was created and build in order to determine if the model is appropriate for their specific situation or need.

Safety, fairness, explainability and robustness are characteristics we demand from an AI system. Yet, to achieve trust in AI, engineering these traits into a final solution will not be enough; it must be accompanied with the ability to measure and communicate the performance levels of a system on each of these dimensions. By demonstrating how to effectively capture events and performance metrics of an AI system on hyperledger fabric, and introduce the concept of factsheets for AI services, which outlines how these metrics can be communicated to end-users as a way of informing their understanding of how the service works, evaluating its functionality, and comprehending its strengths and limitations.

Many decision making processes can be suported by machine learning models. For example if someone should go to jail or should be released until their trial. Or if an employee will perform well and therefore should be hired or not? The involvment of the machine can reduce the humane workload, ncrease accuracy and enlarge fairness.

Every person involved in the creation of AI at any step is accountable for considering the system’s impact in the world, as are the companies invested in its development.

*Transparency* => 
Information that lets you know what your system is doing. If you are more transparent as a company and openly communicate why certain design decisions were taken end users better understand what has been done.

## Recommended Actions
1. Make company policies clear and accessible to design and development teams from day one so that no one is confused about issues of responsibility or accountability. As an AI designer or developer, it is your responsibility to know.

2. Understand where the responsibility of the company/software ends. You may not have control over how data or a tool will be used by a user, client, or other external source.

3. Keep detailed records of your design processes and decision making. Determine a strategy for keeping records during the design and development process to encourage best practices and encourage iteration.

4. Adhere to your company’s business conduct guidelines. Also, understand national and international laws, regulations, and guidelines that your AI may have to work within. You can find other related resources in the IEEE Ethically Aligned Design Document [1].


## Summary

## Summary papers

Arnold et al. 2019: Arnold, M.; Bellamy, R. K. E.; Hind,
M.; Houde, S.; Mehta, S.; Mojsilovi ́c, A.; Nair, R.; Natesan-
Ramamurthy, K.; Olteanu, A.; Piorkowski, D.; Reimer, D.;
Richards, J.; Tsay, J.; and Varshney, K. R. 2019. FactSheets:
Increasing trust in AI services through suppliers declarations
of conformity. IBM Journal of Research & Development
63(4/5).

A FactSheet, as proposed by (Arnold et al. 2019), is a col-
lection of relevant information about an AI model or ser-
vice that is created during the machine learning life cycle.

It includes information from the business owner (e.g., in-
tended use and business justification), from the data gather-
ing/feature selection/data cleaning phase (e.g., data sets, fea-
tures used or created, cleaning operations), from the model

training phase (e.g., bias, robustness, and explainability in-
formation), and from the model validation and deployment

phase (e.g., key performance indicators). A FactSheet is as-
sociated with a model (or service) and is meant to be write

once, i.e., an update to a model would trigger a new Fact-
Sheet for the updated model. FactSheets can be consumed

by any role in the ML life cycle to confirm process gov-
ernance adherence or model performance, or by the ulti-
mate users of a model to provide increased transparency.

## Links 

[1] ... IBM 2019 - Accountability - https://www.ibm.com/design/ai/ethics/accountability/

## Taxonomy and Metrics
