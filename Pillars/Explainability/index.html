<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  
  
  <link rel="shortcut icon" href="../../img/favicon.ico">
  <title>Explainability - Trused AI</title>
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Lato:400,700|Roboto+Slab:400,700|Inconsolata:400,700" />

  <link rel="stylesheet" href="../../css/theme.css" />
  <link rel="stylesheet" href="../../css/theme_extra.css" />
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css" />
  <link href="../../extra.css" rel="stylesheet" />
  <link href="../Fairness/extra.css" rel="stylesheet" />
  
  <script>
    // Current page data
    var mkdocs_page_name = "Explainability";
    var mkdocs_page_input_path = "Pillars/Explainability.md";
    var mkdocs_page_url = null;
  </script>
  
  <script src="../../js/jquery-2.1.1.min.js" defer></script>
  <script src="../../js/modernizr-2.8.3.min.js" defer></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
  <script>hljs.initHighlightingOnLoad();</script> 
  
</head>

<body class="wy-body-for-nav" role="document">

  <div class="wy-grid-for-nav">

    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side stickynav">
    <div class="wy-side-scroll">
      <div class="wy-side-nav-search">
        <a href="../.." class="icon icon-home"> Trused AI</a>
        <div role="search">
  <form id ="rtd-search-form" class="wy-form" action="../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" title="Type search term here" />
  </form>
</div>
      </div>

      <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
                <ul>
                    <li class="toctree-l1"><a class="reference internal" href="../..">Home</a>
                    </li>
                </ul>
                <p class="caption"><span class="caption-text">Pillars</span></p>
                <ul class="current">
                    <li class="toctree-l1"><a class="reference internal" href="../Fairness/">Fairness</a>
                    </li>
                    <li class="toctree-l1 current"><a class="reference internal current" href="./">Explainability</a>
    <ul class="current">
    <li class="toctree-l2"><a class="reference internal" href="#introduction">Introduction</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#key-takaways-questions-and-limitations">Key Takaways, Questions and Limitations</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#summaries-of-paper-website">Summaries of Paper &amp; Website</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="#explainable-ai">Explainable AI</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#explainable-artificial-intelligence-xai">Explainable Artificial Intelligence (XAI)</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#improving-transparency-of-models">Improving transparency of models</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#metrics-for-explainable-ai-xai">Metrics for Explainable AI (XAI)</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#interpretability-to-whome">Interpretability to whome?</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#explainability-in-image-classification">Explainability in image classification</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#towards-a-rigorous-science-of-interpretable-machine-learning">Towards A Rigorous Science of Interpretable Machine Learning</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#comprehensible-classification-models">Comprehensible Classification Models</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#taxonomy">Taxonomy</a>
    </li>
    </ul>
                    </li>
                    <li class="toctree-l1"><a class="reference internal" href="../Robustness/">Robustness</a>
                    </li>
                    <li class="toctree-l1"><a class="reference internal" href="../Accountability/">Accountability</a>
                    </li>
                </ul>
                <ul>
                    <li class="toctree-l1"><a class="reference internal" href="../../Other_aspects/">Additional Aspects</a>
                    </li>
                </ul>
                <p class="caption"><span class="caption-text">Code</span></p>
                <ul>
                    <li class="toctree-l1"><a class="reference internal" href="../../AI_algo/">trusted AI algorithm</a>
                    </li>
                    <li class="toctree-l1"><a class="reference internal" href="../../code_documentation/">Implementation</a>
                    </li>
                </ul>
                <p class="caption"><span class="caption-text">References</span></p>
                <ul>
                    <li class="toctree-l1"><a class="reference internal" href="../../sources/">sources</a>
                    </li>
                </ul>
      </div>
    </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" role="navigation" aria-label="top navigation">
        <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
        <a href="../..">Trused AI</a>
      </nav>

      
      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    <li><a href="../..">Docs</a> &raquo;</li>
    
      
        
          <li>Pillars &raquo;</li>
        
      
    
    <li>Explainability</li>
    <li class="wy-breadcrumbs-aside">
      
    </li>
  </ul>
  
  <hr/>
</div>
          <div role="main">
            <div class="section">
              
                <h1 id="explainability">Explainability</h1>
<div class="admonition explainability">
<p class="admonition-title">Explainability</p>
<p>The level to which a system can provideclarification for the cause of its decisions/outputs.</p>
<p>Interpretability has become a well-recognized goal for machine learning models. The need for
interpretable models is certain to increase as machine learning pushes further into domains such as
medicine, criminal justice, and business, where such models complement human decision-makers and
decisions can have major consequences on human lives. Transparency is thus required for domain
experts to understand, critique, and trust models, and reasoning is required to explain individual
decisions. <a href="https://drive.google.com/drive/folders/1t74vQL453WeyKLj8fjkIAgBAiHjDUSbz">Sanjeeb et. Al</a></p>
</div>
<h2 id="introduction">Introduction</h2>
<p>"In many applications, trust in an AI system will come from its ability to ‘explain itself.’ Yet, when it comes to understanding and explaining the inner workings of an algorithm, one size does not fit all.Different stakeholders require explanations for different purposes and objectives, and explanations must be tailored to their needs. A physician might respond best to seeing examples of patient data similar to their patient’s. On the other hand, a developer training a neural net will benefit from seeing how information flows through the algorithm. While a regulator will aim to understand the system as a whole and probe into its logic, consumers affected by a specific decision will be interested only in factors impacting their case – for example, in a loan processing application, they will expect an explanation for why the request was denied and want to understand what changes could lead to approval. IBM Research is creating diverse explanations, including training highly optimized directly interpretable models, creating contrastive explanations of black box models, using information flow in a high-performing complex model to train simpler, interpretable classifiers, learning disentangled representations, and visualizing information flows in neural networks." <a href="https://www.research.ibm.com/artificial-intelligence/trusted-ai/#">IBM</a></p>
<h2 id="key-takaways-questions-and-limitations">Key Takaways, Questions and Limitations</h2>
<ul>
<li>
<p><em>Key Takaways</em></p>
<ul>
<li>There is a tradeoff between explainability and accuracy (complex models with many features tend to be more accurate but at the same time less transparent)</li>
<li>The level of explainability is mainly dominated by its model type and the number of features it uses.</li>
<li>The importance of model explainability is highly dependent on the context of the model and its consequences. (Revenue, Rate, Rigour, Regulation, Reputation and Risk)</li>
<li>Almost every model can be implemented in a way that makes it explainable and no models are explainable by themself but need an additional layer for the translation of the results to make it explainable for endusers. </li>
<li>Scenarios where explainability is important: Medical diagnosis, loan lending or general assisance in human desicion making. All scenarios are mainly about <strong>classification models</strong>.</li>
</ul>
</li>
<li>
<p><em>Limitations</em></p>
<ul>
<li>The scope of explainability is not only defined by the model and data selection / pre processing 
but often has to be embedded in the application as a whole (Build an explianable AI system with an explanation userinterface for example)</li>
<li>Explainability needs to be accessible. Even models which would be interpretable need to have a method the extract these information and make the explantaion explicite.</li>
</ul>
</li>
<li>
<p><em>Open Qustions</em></p>
<ul>
<li>How to treat techniques (from outside: model inducion, from inside: deep explanation) which enhance explainability in models which would be less explainable by nature? </li>
<li>What should our score on explainability tell? How easily the model could be interpreted and an XAI system could be build with it?</li>
</ul>
</li>
</ul>
<h2 id="summaries-of-paper-website">Summaries of Paper &amp; Website</h2>
<p>See all papers related to explainability from IBM here: <a href="https://www.research.ibm.com/artificial-intelligence/publications/?researcharea=explainability">publications</a></p>
<h3 id="explainable-ai"><a href="https://www.pwc.co.uk/audit-assurance/assets/explainable-ai.pdf">Explainable AI</a></h3>
<p>The paper is business oriented and talks about why explainability is an advatage to have embedded in AI
algorithms and in which usecases explainability should have a high priority. </p>
<ul>
<li>
<p>(p.8) The need for Explainability: Working as intended?, How sensitive is the impact?, Are you comfortable with
the level of control?</p>
</li>
<li>
<p>(p.9) factors to consider: Revenue, Rate, Rigour, Regulation, Reputation and Risk</p>
</li>
<li>
<p>(p.12) Explainable by design: Explainability needs to be considered up front and embedded into the design of
the AI application. It affects the choice ofmachine learning algorithm and mayimpact the way you choose to pre-process data.</p>
</li>
<li>
<p>(p.13) Trade-offs in explainability: Interpretability is a characteristic of a model that is generally considered to come at a cost. As a rule of thumb, the more complex the model, the more accurate it is, but the less interpretable it is. (Decision tree vs. DNN with many layers and features)</p>
</li>
<li>
<p>(p.14) Differnt explaination techniques: Sensitivity analysis,Local Interpretable Model Explanations (LIME), Shapley Additive Explanations (SHAP), Tree interpreters, Neural Network Interpreters</p>
</li>
<li>
<p>(Appendix 2)  Subjective scale of explainability of different classes of algorithms and learning techniques 
(with 1 being the most difficult and 5 being the easiest to explain) see a compriesed version of the table belwo:</p>
</li>
</ul>
<table>
<thead>
<tr>
<th>Learning technique</th>
<th align="center">Scale of explainability (1-5)</th>
</tr>
</thead>
<tbody>
<tr>
<td>Bayesian belief networks (BNNs)</td>
<td align="center">3.5</td>
</tr>
<tr>
<td>Decision trees</td>
<td align="center">4</td>
</tr>
<tr>
<td>Logistic regression</td>
<td align="center">3</td>
</tr>
<tr>
<td>Support vector machines (SVMs)</td>
<td align="center">2</td>
</tr>
<tr>
<td>K-means clustering</td>
<td align="center">3</td>
</tr>
<tr>
<td>Neural networks</td>
<td align="center">1</td>
</tr>
<tr>
<td>Random forest/boosting</td>
<td align="center">3</td>
</tr>
<tr>
<td>Q-learning</td>
<td align="center">2</td>
</tr>
<tr>
<td>Hidden Markov models</td>
<td align="center">3</td>
</tr>
</tbody>
</table>
<p><br></p>
<h3 id="explainable-artificial-intelligence-xai"><a href="https://www.darpa.mil/program/explainable-artificial-intelligence">Explainable Artificial Intelligence (XAI)</a></h3>
<p>XAI is a project from Defense Advanced Research Projects Agency which aims to produce more explainable models, while maintaining a high level of learning performance (prediction accuracy)and enable human users to understand, appropriately trust, and effectively manage the emerging generation of artificially intelligent partners</p>
<p>see their concept here: <a href="https://www.darpa.mil/attachments/XAIIndustryDay_Final.pptx">DARPA slide deck on explainability</a> (two key slides below)</p>
<p>See another slide deck on XAI with more examples here: <a href="https://drive.google.com/drive/folders/1t74vQL453WeyKLj8fjkIAgBAiHjDUSbz">second XAI slide deck</a></p>
<p><img alt="DARPA Concept" src="../images/DARPA_concept.png" /></p>
<p><img alt="DARPA Modles" src="../images/DARPA_explaibale_models.png" /></p>
<h3 id="improving-transparency-of-models"><a href="https://drive.google.com/drive/folders/1t74vQL453WeyKLj8fjkIAgBAiHjDUSbz">Improving transparency of models</a></h3>
<p>Context of AI applications were transparency and explainability is key: 
financial risk assessment (Goyal 2018), medical diagnosis and treatment
planning (Strickland 2019), hiring and promotion decisions
(Alsever 2017), social services eligibility determination
(Fishman, Eggers, and Kishnani 2019), predictive policing
(Ensign et al. 2017), and probation and sentencing recom-
mendations (Larson et al. 2016).</p>
<p>Recent work has outlined the need for increased transparency in AI for data sets(Gebru et al. 2018;
Bender and Friedman 2018; Holland et al. 2018), models (Mitchell et al. 2019),and services (Arnold et al. 2019).</p>
<h3 id="metrics-for-explainable-ai-xai"><a href="https://arxiv.org/pdf/1812.04608.pdf">Metrics for Explainable AI (XAI)</a></h3>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Explaibale AI is much more than just having an easily interpretable model but about a whole system that 
is able to explain itself and interact with the user to create a full unterstanding tailored to the user needs.</p>
</div>
<p>The papers elaborates metrics for the goodness of explanations, whether users are satisfied by explanations, how well  users  understand  the  AI  systems,  how  curiosity  motivates  the  search  for explanations, whether  the  user's  trust  and  reliance  on  the  AI  are  appropriate.</p>
<p><img alt="XAI user goals" src="../images/XAI_user_goals.png" /></p>
<h3 id="interpretability-to-whome"><a href="https://arxiv.org/pdf/1806.07552.pdf">Interpretability to whome?</a></h3>
<p>A machinelearning system’s interpretability should be de-fined in relation to a specific agent or task: we should not ask if the system is interpretable, but to whom is it interpretable</p>
<h3 id="explainability-in-image-classification"><a href="https://www.research.ibm.com/artificial-intelligence/publications/paper/?id=Sanity-Checks-for-Saliency-Metrics">Explainability in image classification</a></h3>
<p>Saliency maps are a popular approach to creating post-hoc explanations of image classifier outputs. These methods produce estimates of the relevance of each pixel to the classification output score, which can be displayed as a saliency map that highlights important pixels.</p>
<h3 id="towards-a-rigorous-science-of-interpretable-machine-learning"><a href="https://arxiv.org/pdf/1702.08608.pdf">Towards A Rigorous Science of Interpretable Machine Learning</a></h3>
<p>The need for interpretability stems  from  an incompleteness in  the  problem  formalization. Scensario where this can be the case:</p>
<ul>
<li>
<p>Scientific Understanding:  The human’s goal is to gain knowledge.  We do not have a completeway of stating what knowledge is;  thus the best we can do is ask for explanations we canconvert into knowledge.</p>
</li>
<li>
<p>Safety:  For complex tasks,  the end-to-end system is almost never completely testable;  onecannot  create  a  complete  list  of  scenarios  in  which  the  system  may  fail.   Enumerating  allpossible outputs given all possible inputs be computationally or logistically infeasible, and wemay be unable to flag all undesirable outputs.</p>
</li>
<li>
<p>Ethics:  The  human  may  want  to  guard  against  certain  kinds  of  discrimination,  and  theirnotion of fairness may be too abstract to be completely encoded into the system.  Even if we can encode protections for specific protected classes into the system, there might be biases that we did not consider a priori </p>
</li>
</ul>
<p>dimensions of interpretability:</p>
<ul>
<li>
<p>Global vs.  Local: Global interpretability implies knowing what patterns are present in general, while local interpretability implies knowing the reasons for a specific decision (such as why a particular loan application was rejected).The former may be important for when scientific understanding or bias detection is the goal;the latter when one needs a justification for a specific decision</p>
</li>
<li>
<p>Area,  Severity of Incompleteness: What part of the problem formulation is incomplete, andhow incomplete is it?</p>
</li>
<li>
<p>ime Constraints: How long can the user afford to spend to understand the explanation?  Adecision that needs to be made at the bedside or during the operation of a plant must beunderstood quickly, while in scientific or anti-discrimination applications, the end-user maybe willing to spend hours trying to fully understand an explanation.</p>
</li>
<li>
<p>User Expertise: How experienced is the user in the task?</p>
</li>
</ul>
<h3 id="comprehensible-classification-models"><a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.599.164&amp;rep=rep1&amp;type=pdf">Comprehensible Classification Models</a></h3>
<p>(p.2-5) Decision trees,  classification  rules,  decision  tables,  nearest  neighbors,  and Bayesian network classifiers, with respect to their interpretability</p>
<p>(p.6) The Drawbacks of Model Size as the Single Measure of Comprehensibility. Big decision trees with easily understandable features can be more comprehensible than small trees with less intuitive features. Models that are too small can also be to simple for users to accept and not lead to less comprehensibility and general trust in the model.</p>
<p>(p.7) Monotonicity Constraints in Classification Models. Users are more like to trust and accept classification models when they  are built by respecting monotonicity constraints in  the  application domain. A monotonic  relationship  between  a  feature and  the  class  attribute  occurs  when  increasing  the  value  of  the feature  tends  to  either  monotonically  increase  or  monotonically decrease the probability of an instance’s membership to a class</p>
<h2 id="taxonomy">Taxonomy</h2>
<p>Construct a set of measurable metrict which can be used to calculate a score that should indicate how good the explainability of a model is. For The calculation of the score differnt weights can be assigned to the metrics. </p>
<p>metrics where 5 is the best and 1 the worst: </p>
<ul>
<li>Model type: Different between ML model types and assigne a score to each depending on how easily the result is to explain.</li>
<li>Monotonicity: A score from 1 to 5 for indicating how many monotone attributes there are, the higher the fraction the better.</li>
<li>Relevance: A score from 1 to 5 how relevant the attributes are. Ideally there are no irrelevant attributes (the definition of relevance depends on the model type)</li>
<li>Model size: A score from 1 to 5 where bigger more complex models with many attributes have </li>
</ul>
<p>(new metric: uncorrelated dataset?)</p>
<style>
table {
    width:100%;
}
</style>

<table>
<thead>
<tr>
<th>metric</th>
<th align="center">description</th>
<th>unit</th>
<th>weight</th>
</tr>
</thead>
<tbody>
<tr>
<td>model type</td>
<td align="center">some models like linear regression or<br> decision tree are very explainable and<br>models like neural networks less</td>
<td>[1,5]</td>
<td>0.5</td>
</tr>
<tr>
<td>faithfulness<br>relevance</td>
<td align="center">are the features truly relevant<br>or can some be omitted?</td>
<td>[1,5]</td>
<td>0.2</td>
</tr>
<tr>
<td>monotonicity</td>
<td align="center">monotonic attribute functions<br>most important feature for classification</td>
<td>[1,5]</td>
<td>0.2</td>
</tr>
<tr>
<td>model size</td>
<td align="center">more features add to the complexity and may decrease comprehensibility</td>
<td>[1,5]</td>
<td>0.1</td>
</tr>
</tbody>
</table>
              
            </div>
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="../Robustness/" class="btn btn-neutral float-right" title="Robustness">Next <span class="icon icon-circle-arrow-right"></span></a>
      
      
        <a href="../Fairness/" class="btn btn-neutral" title="Fairness"><span class="icon icon-circle-arrow-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <!-- Copyright etc -->
    
  </div>

  Built with <a href="https://www.mkdocs.org/">MkDocs</a> using a <a href="https://github.com/snide/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
</footer>
      
        </div>
      </div>

    </section>

  </div>

  <div class="rst-versions" role="note" aria-label="versions">
    <span class="rst-current-version" data-toggle="rst-current-version">
      
      
        <span><a href="../Fairness/" style="color: #fcfcfc;">&laquo; Previous</a></span>
      
      
        <span style="margin-left: 15px"><a href="../Robustness/" style="color: #fcfcfc">Next &raquo;</a></span>
      
    </span>
</div>
    <script>var base_url = '../..';</script>
    <script src="../../js/theme.js" defer></script>
      <script src="../../search/main.js" defer></script>
    <script defer>
        window.onload = function () {
            SphinxRtdTheme.Navigation.enable(true);
        };
    </script>

</body>
</html>
