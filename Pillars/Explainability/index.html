<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  
  
  <link rel="shortcut icon" href="../../img/favicon.ico">
  <title>Explainability - Trused AI</title>
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Lato:400,700|Roboto+Slab:400,700|Inconsolata:400,700" />

  <link rel="stylesheet" href="../../css/theme.css" />
  <link rel="stylesheet" href="../../css/theme_extra.css" />
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css" />
  
  <script>
    // Current page data
    var mkdocs_page_name = "Explainability";
    var mkdocs_page_input_path = "Pillars/Explainability.md";
    var mkdocs_page_url = null;
  </script>
  
  <script src="../../js/jquery-2.1.1.min.js" defer></script>
  <script src="../../js/modernizr-2.8.3.min.js" defer></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
  <script>hljs.initHighlightingOnLoad();</script> 
  
</head>

<body class="wy-body-for-nav" role="document">

  <div class="wy-grid-for-nav">

    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side stickynav">
    <div class="wy-side-scroll">
      <div class="wy-side-nav-search">
        <a href="../.." class="icon icon-home"> Trused AI</a>
        <div role="search">
  <form id ="rtd-search-form" class="wy-form" action="../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" title="Type search term here" />
  </form>
</div>
      </div>

      <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
                <ul>
                    <li class="toctree-l1"><a class="reference internal" href="../..">Home</a>
                    </li>
                </ul>
                <p class="caption"><span class="caption-text">Pillars</span></p>
                <ul class="current">
                    <li class="toctree-l1"><a class="reference internal" href="../Fairness/">Fairness</a>
                    </li>
                    <li class="toctree-l1 current"><a class="reference internal current" href="./">Explainability</a>
    <ul class="current">
    <li class="toctree-l2"><a class="reference internal" href="#introduction">Introduction</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#key-takaways-questions-and-limitations">Key Takaways, Questions and Limitations</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#summaries-of-paper-website">Summaries of Paper &amp; Website</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="#explainable-ai">Explainable AI</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#explainable-artificial-intelligence-xai">Explainable Artificial Intelligence (XAI)</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#improving-transparency-of-models">Improving transparency of models</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#taxonomy">Taxonomy</a>
    </li>
    </ul>
                    </li>
                    <li class="toctree-l1"><a class="reference internal" href="../Robustness/">Robustness</a>
                    </li>
                    <li class="toctree-l1"><a class="reference internal" href="../Accountability/">Accountability</a>
                    </li>
                </ul>
                <p class="caption"><span class="caption-text">Code</span></p>
                <ul>
                    <li class="toctree-l1"><a class="reference internal" href="../../AI_algo/">trusted AI algorithm</a>
                    </li>
                    <li class="toctree-l1"><a class="reference internal" href="../../code_documentation/">Implementation</a>
                    </li>
                </ul>
                <p class="caption"><span class="caption-text">References</span></p>
                <ul>
                    <li class="toctree-l1"><a class="reference internal" href="../../sources/">sources</a>
                    </li>
                </ul>
      </div>
    </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" role="navigation" aria-label="top navigation">
        <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
        <a href="../..">Trused AI</a>
      </nav>

      
      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    <li><a href="../..">Docs</a> &raquo;</li>
    
      
        
          <li>Pillars &raquo;</li>
        
      
    
    <li>Explainability</li>
    <li class="wy-breadcrumbs-aside">
      
    </li>
  </ul>
  
  <hr/>
</div>
          <div role="main">
            <div class="section">
              
                <h1 id="explainability">Explainability</h1>
<div class="admonition abstract">
<p class="admonition-title">Abstract</p>
<p>Interpretability has become a well-recognized goal for machine learning models. The need for
interpretable models is certain to increase as machine learning pushes further into domains such as
medicine, criminal justice, and business, where such models complement human decision-makers and
decisions can have major consequences on human lives. Transparency is thus required for domain
experts to understand, critique, and trust models, and reasoning is required to explain individual
decisions. <a href="https://drive.google.com/drive/folders/1t74vQL453WeyKLj8fjkIAgBAiHjDUSbz">Sanjeeb et. Al</a></p>
</div>
<h2 id="introduction">Introduction</h2>
<p>"In many applications, trust in an AI system will come from its ability to ‘explain itself.’ Yet, when it comes to understanding and explaining the inner workings of an algorithm, one size does not fit all.Different stakeholders require explanations for different purposes and objectives, and explanations must be tailored to their needs. A physician might respond best to seeing examples of patient data similar to their patient’s. On the other hand, a developer training a neural net will benefit from seeing how information flows through the algorithm. While a regulator will aim to understand the system as a whole and probe into its logic, consumers affected by a specific decision will be interested only in factors impacting their case – for example, in a loan processing application, they will expect an explanation for why the request was denied and want to understand what changes could lead to approval. IBM Research is creating diverse explanations, including training highly optimized directly interpretable models, creating contrastive explanations of black box models, using information flow in a high-performing complex model to train simpler, interpretable classifiers, learning disentangled representations, and visualizing information flows in neural networks." <a href="https://www.research.ibm.com/artificial-intelligence/trusted-ai/#">IBM</a></p>
<h2 id="key-takaways-questions-and-limitations">Key Takaways, Questions and Limitations</h2>
<ul>
<li>
<p><em>Key Takaways</em></p>
<ul>
<li>There is a tradeoff between explainability and accuracy (complex models with many features tend to be more accurate but at the same time less transparent)</li>
<li>The level of explainability is mainly dominated by its model type and the number of features it uses.</li>
<li>The importance of model explainability is highly dependent on the context of the model and its consequences. (Revenue, Rate, Rigour, Regulation, Reputation and Risk)</li>
</ul>
</li>
<li>
<p><em>Limitations</em></p>
<ul>
<li>The scope of explainability is not only the model and data selection / pre processing 
but often has to be embedded in the application as a whole (Build an explianable AI system with an explanation userinterface for example)</li>
<li>Explainability needs accessible. Even models which actions could be displayed need to have a method the extract these information and make it the explantaion explicite.</li>
</ul>
</li>
<li>
<p><em>Open Qustions</em></p>
<ul>
<li>How to treat techniques (from outside: model inducion, from inside: deep explanation) which enhance explainability in models which would be less explainable by nature? </li>
</ul>
</li>
</ul>
<h2 id="summaries-of-paper-website">Summaries of Paper &amp; Website</h2>
<p>See all papers related to explainability from IBM here: <a href="https://www.research.ibm.com/artificial-intelligence/publications/?researcharea=explainability">publications</a></p>
<h3 id="explainable-ai"><a href="https://www.pwc.co.uk/audit-assurance/assets/explainable-ai.pdf">Explainable AI</a></h3>
<p>The paper is business oriented and talks about why explainability is an advatage to have embedded in AI
algorithms and in which usecases explainability should have a high priority. </p>
<ul>
<li>
<p>(p.8) The need for Explainability: Working as intended?, How sensitive is the impact?, Are you comfortable with
the level of control?</p>
</li>
<li>
<p>(p.9) factors to consider: Revenue, Rate, Rigour, Regulation, Reputation and Risk</p>
</li>
<li>
<p>(p.12) Explainable by design: Explainability needs to be considered up front and embedded into the design of
the AI application. It affects the choice ofmachine learning algorithm and mayimpact the way you choose to pre-process data.</p>
</li>
<li>
<p>(p.13) Trade-offs in explainability: Interpretability is a characteristic of a model that is generally considered to come at a cost. As a rule of thumb, the more complex the model, the more accurate it is, but the less interpretable it is. (Decision tree vs. DNN with many layers and features)</p>
</li>
<li>
<p>(p.14) Differnt explaination techniques: Sensitivity analysis,Local Interpretable Model Explanations (LIME), Shapley Additive Explanations (SHAP), Tree interpreters, Neural Network Interpreters</p>
</li>
<li>
<p>(Appendix 2)  Subjective scale of explainability of different classes of algorithms and learning techniques 
(with 1 being the most difficult and 5 being the easiest to explain) see a compriesed version of the table belwo:</p>
</li>
</ul>
<table>
<thead>
<tr>
<th>Learning technique</th>
<th align="center">Scale of explainability (1-5)</th>
</tr>
</thead>
<tbody>
<tr>
<td>Bayesian belief networks (BNNs)</td>
<td align="center">3.5</td>
</tr>
<tr>
<td>Decision trees</td>
<td align="center">4</td>
</tr>
<tr>
<td>Logistic regression</td>
<td align="center">3</td>
</tr>
<tr>
<td>Support vector machines (SVMs)</td>
<td align="center">2</td>
</tr>
<tr>
<td>K-means clustering</td>
<td align="center">3</td>
</tr>
<tr>
<td>Neural networks</td>
<td align="center">1</td>
</tr>
<tr>
<td>Random forest/boosting</td>
<td align="center">3</td>
</tr>
<tr>
<td>Q-learning</td>
<td align="center">2</td>
</tr>
<tr>
<td>Hidden Markov models</td>
<td align="center">3</td>
</tr>
</tbody>
</table>
<p><br></p>
<h3 id="explainable-artificial-intelligence-xai"><a href="https://www.darpa.mil/program/explainable-artificial-intelligence">Explainable Artificial Intelligence (XAI)</a></h3>
<p>XAI is a project from Defense Advanced Research Projects Agency which aims to produce more explainable models, while maintaining a high level of learning performance (prediction accuracy)and enable human users to understand, appropriately trust, and effectively manage the emerging generation of artificially intelligent partners</p>
<p>see their concept here: <a href="https://www.darpa.mil/attachments/XAIIndustryDay_Final.pptx">DARPA slide deck on explainability</a> (two key slides below)</p>
<p>See another slide deck on XAI with more examples here: <a href="https://drive.google.com/drive/folders/1t74vQL453WeyKLj8fjkIAgBAiHjDUSbz">second XAI slide deck</a></p>
<p><img alt="DARPA Concept" src="../images/DARPA_concept.png" /></p>
<p><img alt="DARPA Modles" src="../images/DARPA_explaibale_models.png" /></p>
<h3 id="improving-transparency-of-models"><a href="https://drive.google.com/drive/folders/1t74vQL453WeyKLj8fjkIAgBAiHjDUSbz">Improving transparency of models</a></h3>
<p>Context of AI applications were transparency and explainability is key: 
financial risk assessment (Goyal 2018), medical diagnosis and treatment
planning (Strickland 2019), hiring and promotion decisions
(Alsever 2017), social services eligibility determination
(Fishman, Eggers, and Kishnani 2019), predictive policing
(Ensign et al. 2017), and probation and sentencing recom-
mendations (Larson et al. 2016).</p>
<p>Recent work has outlined the need for increased transparency in AI for data sets(Gebru et al. 2018;
Bender and Friedman 2018; Holland et al. 2018), models (Mitchell et al. 2019),and services (Arnold et al. 2019).</p>
<h2 id="taxonomy">Taxonomy</h2>
<p>Construct a set of measurable metrict which can be used to calculate a score that should indicate how good the explainability of a model is. For The calculation of the score differnt weights can be assigned to the metrics. </p>
<style>
table {
    width:100%;
}
</style>

<table>
<thead>
<tr>
<th>metric</th>
<th align="center">description</th>
<th>unit</th>
<th>weight</th>
</tr>
</thead>
<tbody>
<tr>
<td>model type</td>
<td align="center">some models like linear regression or<br> decision tree are very explainable and<br>models like neural networks less</td>
<td>[1,5]</td>
<td>0.6</td>
</tr>
<tr>
<td>faithfulness<br>relevance</td>
<td align="center">are the features truly relevant<br>or can some be omitted?</td>
<td>[0,1]</td>
<td>0.2</td>
</tr>
<tr>
<td>monotonicity</td>
<td align="center">monotonic attribute functions<br>most important feature for classification</td>
<td>[-1,1]</td>
<td>0.2</td>
</tr>
</tbody>
</table>
              
            </div>
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="../Robustness/" class="btn btn-neutral float-right" title="Robustness">Next <span class="icon icon-circle-arrow-right"></span></a>
      
      
        <a href="../Fairness/" class="btn btn-neutral" title="Fairness"><span class="icon icon-circle-arrow-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <!-- Copyright etc -->
    
  </div>

  Built with <a href="https://www.mkdocs.org/">MkDocs</a> using a <a href="https://github.com/snide/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
</footer>
      
        </div>
      </div>

    </section>

  </div>

  <div class="rst-versions" role="note" aria-label="versions">
    <span class="rst-current-version" data-toggle="rst-current-version">
      
      
        <span><a href="../Fairness/" style="color: #fcfcfc;">&laquo; Previous</a></span>
      
      
        <span style="margin-left: 15px"><a href="../Robustness/" style="color: #fcfcfc">Next &raquo;</a></span>
      
    </span>
</div>
    <script>var base_url = '../..';</script>
    <script src="../../js/theme.js" defer></script>
      <script src="../../search/main.js" defer></script>
    <script defer>
        window.onload = function () {
            SphinxRtdTheme.Navigation.enable(true);
        };
    </script>

</body>
</html>
