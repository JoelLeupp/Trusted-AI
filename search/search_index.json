{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Trusted AI Master Project This site serves as documentation and Overview of the Master Project. Introduction Artificial intelligence (AI) systems are getting more and more relevance as support to human decision-making processes. While AI holds the promise of delivering valuable insights into many application scenarios, the broad adoption of AI systems will rely on the ability to trust their decisions. According to IBM, some key aspects provide information to trust (or not) a decision made by an algorithm. In particular, a decision must: be reliable and fair, be accounted for, not cause harm, not be tampered with, be understandable, and be secure. In this context, the previous aspects have been grouped into the following pillars. \u2022 Fairness. One of the essential aspects to enable trusted AI is to avoid bias across the entire lifecycle of AI applications, as well as across different bias and data types. \u2022 Robustness. AI-based systems may be vulnerable to adversarial attacks. Attackers may poison training data by injecting samples to compromise system decisions eventually. \u2022 Explainability. In many application scenarios, decisions made by algorithms must be explainable and understandable by humans. Various stakeholders require explanations for different purposes, and explanations must be tailored to their needs. \u2022 Accountability. The quality of decisions should be evaluated in terms of accuracy, ability to satisfy users\u2019 preferences, as well as other properties related to the impact of the decision.The previous characteristics are demanded from AI systems. Yet, to achieve trust in AI, more efforts and automatic mechanisms able to calculate and communicate trust levels are required. Project Documentation The whole documentation of the project can be found on our GitHub page mkdocs The documentation is created with mkdocs, which uses simple markdown file and a .yml config file to build and deploy the documentation to GitHub. The relevant files to create the documentation are in the mkdocs folder The file mkdocs.yml is the config file and has to be adapted if the theme should change or the index changes or new markdowns added. The markdown files are in the folder docs and include the whole documentation. installation pip install --upgrade pip pip install mkdocs workflow: At the location of the mkdocs folder open the terminal and enter: mkdocs serve This will build the documentation and host it locally, where the full documentation can be seen. change the config or markdown files. Sinc you are serving the documentation locally you can see the changes in the config or markdown files in real time. You will always see the output and how the documentation will look in the end and therefore minimalize syntax errors in markdown for example. deploy the build on GitHub to the branche gh-pages. For this open another termianl at the localtion of the mkdocs folder and enter: mkdocs gh-deploy","title":"Home"},{"location":"#trusted-ai-master-project","text":"This site serves as documentation and Overview of the Master Project.","title":"Trusted AI Master Project"},{"location":"#introduction","text":"Artificial intelligence (AI) systems are getting more and more relevance as support to human decision-making processes. While AI holds the promise of delivering valuable insights into many application scenarios, the broad adoption of AI systems will rely on the ability to trust their decisions. According to IBM, some key aspects provide information to trust (or not) a decision made by an algorithm. In particular, a decision must: be reliable and fair, be accounted for, not cause harm, not be tampered with, be understandable, and be secure. In this context, the previous aspects have been grouped into the following pillars. \u2022 Fairness. One of the essential aspects to enable trusted AI is to avoid bias across the entire lifecycle of AI applications, as well as across different bias and data types. \u2022 Robustness. AI-based systems may be vulnerable to adversarial attacks. Attackers may poison training data by injecting samples to compromise system decisions eventually. \u2022 Explainability. In many application scenarios, decisions made by algorithms must be explainable and understandable by humans. Various stakeholders require explanations for different purposes, and explanations must be tailored to their needs. \u2022 Accountability. The quality of decisions should be evaluated in terms of accuracy, ability to satisfy users\u2019 preferences, as well as other properties related to the impact of the decision.The previous characteristics are demanded from AI systems. Yet, to achieve trust in AI, more efforts and automatic mechanisms able to calculate and communicate trust levels are required.","title":"Introduction"},{"location":"#project-documentation","text":"The whole documentation of the project can be found on our GitHub page","title":"Project Documentation"},{"location":"#mkdocs","text":"The documentation is created with mkdocs, which uses simple markdown file and a .yml config file to build and deploy the documentation to GitHub. The relevant files to create the documentation are in the mkdocs folder The file mkdocs.yml is the config file and has to be adapted if the theme should change or the index changes or new markdowns added. The markdown files are in the folder docs and include the whole documentation. installation pip install --upgrade pip pip install mkdocs workflow: At the location of the mkdocs folder open the terminal and enter: mkdocs serve This will build the documentation and host it locally, where the full documentation can be seen. change the config or markdown files. Sinc you are serving the documentation locally you can see the changes in the config or markdown files in real time. You will always see the output and how the documentation will look in the end and therefore minimalize syntax errors in markdown for example. deploy the build on GitHub to the branche gh-pages. For this open another termianl at the localtion of the mkdocs folder and enter: mkdocs gh-deploy","title":"mkdocs"},{"location":"AI_algo/","text":"Trusted AI Algorithm Basic Idea Taxonomy Architecture","title":"trusted AI algorithm"},{"location":"AI_algo/#trusted-ai-algorithm","text":"","title":"Trusted AI Algorithm"},{"location":"AI_algo/#basic-idea","text":"","title":"Basic Idea"},{"location":"AI_algo/#taxonomy","text":"","title":"Taxonomy"},{"location":"AI_algo/#architecture","text":"","title":"Architecture"},{"location":"code_documentation/","text":"Implementation of the trusted AI Algorithm General Architecture File Structure Build How it works Evaluation and Results","title":"Implementation"},{"location":"code_documentation/#implementation-of-the-trusted-ai-algorithm","text":"","title":"Implementation of the trusted AI Algorithm"},{"location":"code_documentation/#general-architecture","text":"","title":"General Architecture"},{"location":"code_documentation/#file-structure","text":"","title":"File Structure"},{"location":"code_documentation/#build","text":"","title":"Build"},{"location":"code_documentation/#how-it-works","text":"","title":"How it works"},{"location":"code_documentation/#evaluation-and-results","text":"","title":"Evaluation and Results"},{"location":"sources/","text":"Overview This is a collection of all material that might be interesting or relevant regarding the project. Papers AI fairness 360: Bellamy, R. K., et al. (2018). AI Fairness 360: An Extensible Toolkit for Detecting, Understanding, and Mitigating Unwanted Algorithmic Bias. arXiv, October 2018, arXiv:1810.01943. pdf version YouTube Videos Add here a list of usefull and interesing YT videos Decison making: Can we trust artificial intelligence to make decisions for us? Websites IBM trusted AI main page: AI Research","title":"sources"},{"location":"sources/#overview","text":"This is a collection of all material that might be interesting or relevant regarding the project.","title":"Overview"},{"location":"sources/#papers","text":"AI fairness 360: Bellamy, R. K., et al. (2018). AI Fairness 360: An Extensible Toolkit for Detecting, Understanding, and Mitigating Unwanted Algorithmic Bias. arXiv, October 2018, arXiv:1810.01943. pdf version","title":"Papers"},{"location":"sources/#youtube-videos","text":"Add here a list of usefull and interesing YT videos Decison making: Can we trust artificial intelligence to make decisions for us?","title":"YouTube Videos"},{"location":"sources/#websites","text":"IBM trusted AI main page: AI Research","title":"Websites"},{"location":"Pillars/Accountability/","text":"Accountability Summary Links Taxonomy and Metrics","title":"Accountability"},{"location":"Pillars/Accountability/#accountability","text":"","title":"Accountability"},{"location":"Pillars/Accountability/#summary","text":"","title":"Summary"},{"location":"Pillars/Accountability/#links","text":"","title":"Links"},{"location":"Pillars/Accountability/#taxonomy-and-metrics","text":"","title":"Taxonomy and Metrics"},{"location":"Pillars/Explainability/","text":"Explainability Summary In many applications, trust in an AI system will come from its ability to \u2018explain itself.\u2019 Yet, when it comes to understanding and explaining the inner workings of an algorithm, one size does not fit all. Different stakeholders require explanations for different purposes and objectives, and explanations must be tailored to their needs. A physician might respond best to seeing examples of patient data similar to their patient\u2019s. On the other hand, a developer training a neural net will benefit from seeing how information flows through the algorithm. While a regulator will aim to understand the system as a whole and probe into its logic, consumers affected by a specific decision will be interested only in factors impacting their case \u2013 for example, in a loan processing application, they will expect an explanation for why the request was denied and want to understand what changes could lead to approval. IBM Research is creating diverse explanations, including training highly optimized directly interpretable models, creating contrastive explanations of black box models, using information flow in a high-performing complex model to train simpler, interpretable classifiers, learning disentangled representations, and visualizing information flows in neural networks. sources Websites IBM explainability: explainability main page AI Explainability 360: Toolkit page AI Explainability GitHub repo: AIX360 Papers towards Robust Interpretability paper Generating Contrastive Explanations withMonotonic Attribute Functions paper Videos Taxonomy Construct a set of measurable metrict which can be used to calculate a score that should indicate how good the explainability of a model is. For The calculation of the score differnt weights can be assigned to the metrics. table { width:100%; } metric description unit weight model type some models like linear regression or decision tree are very explainable and models like neural networks rather not boolean) 0.6 faithfulness relevance are the features truly relevant or can some be omitted? [0,1] 0.2 monotonicity monotonic attribute functions most important feature for classification [-1,1] 0.2","title":"Explainability"},{"location":"Pillars/Explainability/#explainability","text":"","title":"Explainability"},{"location":"Pillars/Explainability/#summary","text":"In many applications, trust in an AI system will come from its ability to \u2018explain itself.\u2019 Yet, when it comes to understanding and explaining the inner workings of an algorithm, one size does not fit all. Different stakeholders require explanations for different purposes and objectives, and explanations must be tailored to their needs. A physician might respond best to seeing examples of patient data similar to their patient\u2019s. On the other hand, a developer training a neural net will benefit from seeing how information flows through the algorithm. While a regulator will aim to understand the system as a whole and probe into its logic, consumers affected by a specific decision will be interested only in factors impacting their case \u2013 for example, in a loan processing application, they will expect an explanation for why the request was denied and want to understand what changes could lead to approval. IBM Research is creating diverse explanations, including training highly optimized directly interpretable models, creating contrastive explanations of black box models, using information flow in a high-performing complex model to train simpler, interpretable classifiers, learning disentangled representations, and visualizing information flows in neural networks.","title":"Summary"},{"location":"Pillars/Explainability/#sources","text":"","title":"sources"},{"location":"Pillars/Explainability/#websites","text":"IBM explainability: explainability main page AI Explainability 360: Toolkit page AI Explainability GitHub repo: AIX360","title":"Websites"},{"location":"Pillars/Explainability/#papers","text":"towards Robust Interpretability paper Generating Contrastive Explanations withMonotonic Attribute Functions paper","title":"Papers"},{"location":"Pillars/Explainability/#videos","text":"","title":"Videos"},{"location":"Pillars/Explainability/#taxonomy","text":"Construct a set of measurable metrict which can be used to calculate a score that should indicate how good the explainability of a model is. For The calculation of the score differnt weights can be assigned to the metrics. table { width:100%; } metric description unit weight model type some models like linear regression or decision tree are very explainable and models like neural networks rather not boolean) 0.6 faithfulness relevance are the features truly relevant or can some be omitted? [0,1] 0.2 monotonicity monotonic attribute functions most important feature for classification [-1,1] 0.2","title":"Taxonomy"},{"location":"Pillars/Fairness/","text":"Fairness Summary Some text sanity checks link Links Metrics","title":"Fairness"},{"location":"Pillars/Fairness/#fairness","text":"","title":"Fairness"},{"location":"Pillars/Fairness/#summary","text":"Some text sanity checks link","title":"Summary"},{"location":"Pillars/Fairness/#links","text":"","title":"Links"},{"location":"Pillars/Fairness/#metrics","text":"","title":"Metrics"},{"location":"Pillars/Robustness/","text":"Robustness Summary Links Metrics Empirical Robustness Loss Sensitivity","title":"Robustness"},{"location":"Pillars/Robustness/#robustness","text":"","title":"Robustness"},{"location":"Pillars/Robustness/#summary","text":"","title":"Summary"},{"location":"Pillars/Robustness/#links","text":"","title":"Links"},{"location":"Pillars/Robustness/#metrics","text":"","title":"Metrics"},{"location":"Pillars/Robustness/#empirical-robustness","text":"","title":"Empirical Robustness"},{"location":"Pillars/Robustness/#loss-sensitivity","text":"","title":"Loss Sensitivity"}]}