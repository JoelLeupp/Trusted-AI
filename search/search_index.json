{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"AI_algo/","text":"Trusted AI Algorithm Basic Idea Taxonomy Architecture","title":"trusted AI algorithm"},{"location":"AI_algo/#trusted-ai-algorithm","text":"","title":"Trusted AI Algorithm"},{"location":"AI_algo/#basic-idea","text":"","title":"Basic Idea"},{"location":"AI_algo/#taxonomy","text":"","title":"Taxonomy"},{"location":"AI_algo/#architecture","text":"","title":"Architecture"},{"location":"Other_aspects/","text":"Other Aspects To Consider For Trusted AI Algorithm Privacy Joel said there are metrics to measure privacy. Ethics, Governance and Compliance They are important to consider but no metrics. Accuracy on Train and Test Datasets We have to consider the accuracy as the higher accuracy implies a better/more trustworthy model. IBM considers accuracy as one of the main aspects on its display of Factsheets with Explainability, Fairness and Robustness. However solely showing the test accuracy is not enough. The difference between the train accuracy and the test accuracy displays the quality of the model. If the train accuracy is high but the test accuracy is relatively low we could say that the model is overfitting. Hence we need to consider the difference of accuracies. Data Quality We can try to analyze data quality. We can consider the amount of data used, the proportions and the distributions of train and test datasets. We can check for bias (correlated with fairness).","title":"Additional Aspects"},{"location":"Other_aspects/#other-aspects-to-consider-for-trusted-ai-algorithm","text":"","title":"Other Aspects To Consider For Trusted AI Algorithm"},{"location":"Other_aspects/#privacy","text":"Joel said there are metrics to measure privacy.","title":"Privacy"},{"location":"Other_aspects/#ethics-governance-and-compliance","text":"They are important to consider but no metrics.","title":"Ethics, Governance and Compliance"},{"location":"Other_aspects/#accuracy-on-train-and-test-datasets","text":"We have to consider the accuracy as the higher accuracy implies a better/more trustworthy model. IBM considers accuracy as one of the main aspects on its display of Factsheets with Explainability, Fairness and Robustness. However solely showing the test accuracy is not enough. The difference between the train accuracy and the test accuracy displays the quality of the model. If the train accuracy is high but the test accuracy is relatively low we could say that the model is overfitting. Hence we need to consider the difference of accuracies.","title":"Accuracy on Train and Test Datasets"},{"location":"Other_aspects/#data-quality","text":"We can try to analyze data quality. We can consider the amount of data used, the proportions and the distributions of train and test datasets. We can check for bias (correlated with fairness).","title":"Data Quality"},{"location":"code_documentation/","text":"Implementation of the trusted AI Algorithm General Architecture File Structure Build How it works Evaluation and Results","title":"Implementation"},{"location":"code_documentation/#implementation-of-the-trusted-ai-algorithm","text":"","title":"Implementation of the trusted AI Algorithm"},{"location":"code_documentation/#general-architecture","text":"","title":"General Architecture"},{"location":"code_documentation/#file-structure","text":"","title":"File Structure"},{"location":"code_documentation/#build","text":"","title":"Build"},{"location":"code_documentation/#how-it-works","text":"","title":"How it works"},{"location":"code_documentation/#evaluation-and-results","text":"","title":"Evaluation and Results"},{"location":"documentation/","text":"Trusted AI Master Project This site serves as documentation and Overview of the Master Project. The whole documentation of the project can be found on our GitHub page mkdocs The documentation is created with mkdocs, which uses simple markdown file and a .yml config file to build and deploy the documentation to GitHub. The relevant files to create the documentation are in the mkdocs folder The file mkdocs.yml is the config file and has to be adapted if the theme should change or the index changes or new markdowns added. The markdown files are in the folder docs and include the whole documentation. installation pip install --upgrade pip pip install mkdocs workflow: At the location of the mkdocs folder open the terminal and enter: mkdocs serve This will build the documentation and host it locally, where the full documentation can be seen. change the config or markdown files. Sinc you are serving the documentation locally you can see the changes in the config or markdown files in real time. You will always see the output and how the documentation will look in the end and therefore minimalize syntax errors in markdown for example. deploy the build on GitHub to the branche gh-pages. For this open another termianl at the localtion of the mkdocs folder and enter: mkdocs gh-deploy","title":"Documentation"},{"location":"documentation/#trusted-ai-master-project","text":"This site serves as documentation and Overview of the Master Project. The whole documentation of the project can be found on our GitHub page","title":"Trusted AI Master Project"},{"location":"documentation/#mkdocs","text":"The documentation is created with mkdocs, which uses simple markdown file and a .yml config file to build and deploy the documentation to GitHub. The relevant files to create the documentation are in the mkdocs folder The file mkdocs.yml is the config file and has to be adapted if the theme should change or the index changes or new markdowns added. The markdown files are in the folder docs and include the whole documentation. installation pip install --upgrade pip pip install mkdocs workflow: At the location of the mkdocs folder open the terminal and enter: mkdocs serve This will build the documentation and host it locally, where the full documentation can be seen. change the config or markdown files. Sinc you are serving the documentation locally you can see the changes in the config or markdown files in real time. You will always see the output and how the documentation will look in the end and therefore minimalize syntax errors in markdown for example. deploy the build on GitHub to the branche gh-pages. For this open another termianl at the localtion of the mkdocs folder and enter: mkdocs gh-deploy","title":"mkdocs"},{"location":"introduction/","text":"Introduction Artificial intelligence (AI) systems are getting more and more relevance as support to human decision-making processes. While AI holds the promise of delivering valuable insights into many application scenarios, the broad adoption of AI systems will rely on the ability to trust their decisions. According to IBM, some key aspects provide information to trust (or not) a decision made by an algorithm. In particular, a decision must: be reliable and fair, be accounted for, not cause harm, not be tampered with, be understandable, and be secure. In this context, the previous aspects have been grouped into the following pillars. \u2022 Fairness. One of the essential aspects to enable trusted AI is to avoid bias across the entire lifecycle of AI applications, as well as across different bias and data types. \u2022 Robustness. AI-based systems may be vulnerable to adversarial attacks. Attackers may poison training data by injecting samples to compromise system decisions eventually. \u2022 Explainability. In many application scenarios, decisions made by algorithms must be explainable and understandable by humans. Various stakeholders require explanations for different purposes, and explanations must be tailored to their needs. \u2022 Accountability. The quality of decisions should be evaluated in terms of accuracy, ability to satisfy users\u2019 preferences, as well as other properties related to the impact of the decision.The previous characteristics are demanded from AI systems. Yet, to achieve trust in AI, more efforts and automatic mechanisms able to calculate and communicate trust levels are required. Opportunities The use of machine learning models can provide many advantages to end-users. Higher accuracy, effectiveness Lower cost, higher efficiency Preventing human biases and prejudices Transparency, consistency More equal access to opportunities and resources Challenges and Risks Unfairness, unequal allocation of benefit or harm Opaqueness; inexplicability Accountability; due process Recourse; right to dispute/appeal Stereotyping; denigration; unequal representation Invasion of privacy; surveillance","title":"Introduction"},{"location":"introduction/#introduction","text":"Artificial intelligence (AI) systems are getting more and more relevance as support to human decision-making processes. While AI holds the promise of delivering valuable insights into many application scenarios, the broad adoption of AI systems will rely on the ability to trust their decisions. According to IBM, some key aspects provide information to trust (or not) a decision made by an algorithm. In particular, a decision must: be reliable and fair, be accounted for, not cause harm, not be tampered with, be understandable, and be secure. In this context, the previous aspects have been grouped into the following pillars. \u2022 Fairness. One of the essential aspects to enable trusted AI is to avoid bias across the entire lifecycle of AI applications, as well as across different bias and data types. \u2022 Robustness. AI-based systems may be vulnerable to adversarial attacks. Attackers may poison training data by injecting samples to compromise system decisions eventually. \u2022 Explainability. In many application scenarios, decisions made by algorithms must be explainable and understandable by humans. Various stakeholders require explanations for different purposes, and explanations must be tailored to their needs. \u2022 Accountability. The quality of decisions should be evaluated in terms of accuracy, ability to satisfy users\u2019 preferences, as well as other properties related to the impact of the decision.The previous characteristics are demanded from AI systems. Yet, to achieve trust in AI, more efforts and automatic mechanisms able to calculate and communicate trust levels are required.","title":"Introduction"},{"location":"introduction/#opportunities","text":"The use of machine learning models can provide many advantages to end-users. Higher accuracy, effectiveness Lower cost, higher efficiency Preventing human biases and prejudices Transparency, consistency More equal access to opportunities and resources","title":"Opportunities"},{"location":"introduction/#challenges-and-risks","text":"Unfairness, unequal allocation of benefit or harm Opaqueness; inexplicability Accountability; due process Recourse; right to dispute/appeal Stereotyping; denigration; unequal representation Invasion of privacy; surveillance","title":"Challenges and Risks"},{"location":"sources/","text":"Overview This is a collection of all material that might be interesting or relevant regarding the project. Websites, papers and videos Explainability IBM explainability papers: explainability papers AI Explainability 360: Toolkit page AI Explainability GitHub repo: AIX360 towards Robust Interpretability paper Generating Contrastive Explanations withMonotonic Attribute Functions paper FICO community YouTube Videos Add here a list of usefull and interesing YT videos Decison making: Can we trust artificial intelligence to make decisions for us? Websites IBM trusted AI main page: AI Research","title":"sources"},{"location":"sources/#overview","text":"This is a collection of all material that might be interesting or relevant regarding the project.","title":"Overview"},{"location":"sources/#websites-papers-and-videos","text":"","title":"Websites, papers and videos"},{"location":"sources/#explainability","text":"IBM explainability papers: explainability papers AI Explainability 360: Toolkit page AI Explainability GitHub repo: AIX360 towards Robust Interpretability paper Generating Contrastive Explanations withMonotonic Attribute Functions paper FICO community","title":"Explainability"},{"location":"sources/#youtube-videos","text":"Add here a list of usefull and interesing YT videos Decison making: Can we trust artificial intelligence to make decisions for us?","title":"YouTube Videos"},{"location":"sources/#websites","text":"IBM trusted AI main page: AI Research","title":"Websites"},{"location":"Pillars/Explainability/","text":"Explainability Explainability The level to which a system can provideclarification for the cause of its decisions/outputs. Interpretability has become a well-recognized goal for machine learning models. The need for interpretable models is certain to increase as machine learning pushes further into domains such as medicine, criminal justice, and business, where such models complement human decision-makers and decisions can have major consequences on human lives. Transparency is thus required for domain experts to understand, critique, and trust models, and reasoning is required to explain individual decisions. Sanjeeb et. Al Introduction \"In many applications, trust in an AI system will come from its ability to \u2018explain itself.\u2019 Yet, when it comes to understanding and explaining the inner workings of an algorithm, one size does not fit all.Different stakeholders require explanations for different purposes and objectives, and explanations must be tailored to their needs. A physician might respond best to seeing examples of patient data similar to their patient\u2019s. On the other hand, a developer training a neural net will benefit from seeing how information flows through the algorithm. While a regulator will aim to understand the system as a whole and probe into its logic, consumers affected by a specific decision will be interested only in factors impacting their case \u2013 for example, in a loan processing application, they will expect an explanation for why the request was denied and want to understand what changes could lead to approval. IBM Research is creating diverse explanations, including training highly optimized directly interpretable models, creating contrastive explanations of black box models, using information flow in a high-performing complex model to train simpler, interpretable classifiers, learning disentangled representations, and visualizing information flows in neural networks.\" IBM Key Takaways, Questions and Limitations Key Takaways There is a tradeoff between explainability and accuracy (complex models with many features tend to be more accurate but at the same time less transparent) The level of explainability is mainly dominated by its model type and the number of features it uses. The importance of model explainability is highly dependent on the context of the model and its consequences. (Revenue, Rate, Rigour, Regulation, Reputation and Risk) Almost every model can be implemented in a way that makes it explainable and no models are explainable by themself but need an additional layer for the translation of the results to make it explainable for endusers. Scenarios where explainability is important: Medical diagnosis, loan lending or general assisance in human desicion making. All scenarios are mainly about classification models . Limitations The scope of explainability is not only defined by the model and data selection / pre processing but often has to be embedded in the application as a whole (Build an explianable AI system with an explanation userinterface for example) Explainability needs to be accessible. Even models which would be interpretable need to have a method the extract these information and make the explantaion explicite. Open Qustions How to treat techniques (from outside: model inducion, from inside: deep explanation) which enhance explainability in models which would be less explainable by nature? What should our score on explainability tell? How easily the model could be interpreted and an XAI system could be build with it? Summaries of Paper & Website See all papers related to explainability from IBM here: publications Explainable AI The paper is business oriented and talks about why explainability is an advatage to have embedded in AI algorithms and in which usecases explainability should have a high priority. (p.8) The need for Explainability: Working as intended?, How sensitive is the impact?, Are you comfortable with the level of control? (p.9) factors to consider: Revenue, Rate, Rigour, Regulation, Reputation and Risk (p.12) Explainable by design: Explainability needs to be considered up front and embedded into the design of the AI application. It affects the choice ofmachine learning algorithm and mayimpact the way you choose to pre-process data. (p.13) Trade-offs in explainability: Interpretability is a characteristic of a model that is generally considered to come at a cost. As a rule of thumb, the more complex the model, the more accurate it is, but the less interpretable it is. (Decision tree vs. DNN with many layers and features) (p.14) Differnt explaination techniques: Sensitivity analysis,Local Interpretable Model Explanations (LIME), Shapley Additive Explanations (SHAP), Tree interpreters, Neural Network Interpreters (Appendix 2) Subjective scale of explainability of different classes of algorithms and learning techniques (with 1 being the most difficult and 5 being the easiest to explain) see a compriesed version of the table belwo: Learning technique Scale of explainability (1-5) Bayesian belief networks (BNNs) 3.5 Decision trees 4 Logistic regression 3 Support vector machines (SVMs) 2 K-means clustering 3 Neural networks 1 Random forest/boosting 3 Q-learning 2 Hidden Markov models 3 Explainable Artificial Intelligence (XAI) XAI is a project from Defense Advanced Research Projects Agency which aims to produce more explainable models, while maintaining a high level of learning performance (prediction accuracy)and enable human users to understand, appropriately trust, and effectively manage the emerging generation of artificially intelligent partners see their concept here: DARPA slide deck on explainability (two key slides below) See another slide deck on XAI with more examples here: second XAI slide deck Improving transparency of models Context of AI applications were transparency and explainability is key: financial risk assessment (Goyal 2018), medical diagnosis and treatment planning (Strickland 2019), hiring and promotion decisions (Alsever 2017), social services eligibility determination (Fishman, Eggers, and Kishnani 2019), predictive policing (Ensign et al. 2017), and probation and sentencing recom- mendations (Larson et al. 2016). Recent work has outlined the need for increased transparency in AI for data sets(Gebru et al. 2018; Bender and Friedman 2018; Holland et al. 2018), models (Mitchell et al. 2019),and services (Arnold et al. 2019). Metrics for Explainable AI (XAI) Note Explaibale AI is much more than just having an easily interpretable model but about a whole system that is able to explain itself and interact with the user to create a full unterstanding tailored to the user needs. The papers elaborates metrics for the goodness of explanations, whether users are satisfied by explanations, how well users understand the AI systems, how curiosity motivates the search for explanations, whether the user's trust and reliance on the AI are appropriate. Interpretability to whome? A machinelearning system\u2019s interpretability should be de-fined in relation to a specific agent or task: we should not ask if the system is interpretable, but to whom is it interpretable Explainability in image classification Saliency maps are a popular approach to creating post-hoc explanations of image classifier outputs. These methods produce estimates of the relevance of each pixel to the classification output score, which can be displayed as a saliency map that highlights important pixels. Towards A Rigorous Science of Interpretable Machine Learning The need for interpretability stems from an incompleteness in the problem formalization. Scensario where this can be the case: Scientific Understanding: The human\u2019s goal is to gain knowledge. We do not have a completeway of stating what knowledge is; thus the best we can do is ask for explanations we canconvert into knowledge. Safety: For complex tasks, the end-to-end system is almost never completely testable; onecannot create a complete list of scenarios in which the system may fail. Enumerating allpossible outputs given all possible inputs be computationally or logistically infeasible, and wemay be unable to flag all undesirable outputs. Ethics: The human may want to guard against certain kinds of discrimination, and theirnotion of fairness may be too abstract to be completely encoded into the system. Even if we can encode protections for specific protected classes into the system, there might be biases that we did not consider a priori dimensions of interpretability: Global vs. Local: Global interpretability implies knowing what patterns are present in general, while local interpretability implies knowing the reasons for a specific decision (such as why a particular loan application was rejected).The former may be important for when scientific understanding or bias detection is the goal;the latter when one needs a justification for a specific decision Area, Severity of Incompleteness: What part of the problem formulation is incomplete, andhow incomplete is it? ime Constraints: How long can the user afford to spend to understand the explanation? Adecision that needs to be made at the bedside or during the operation of a plant must beunderstood quickly, while in scientific or anti-discrimination applications, the end-user maybe willing to spend hours trying to fully understand an explanation. User Expertise: How experienced is the user in the task? Comprehensible Classification Models (p.2-5) Decision trees, classification rules, decision tables, nearest neighbors, and Bayesian network classifiers, with respect to their interpretability (p.6) The Drawbacks of Model Size as the Single Measure of Comprehensibility. Big decision trees with easily understandable features can be more comprehensible than small trees with less intuitive features. Models that are too small can also be to simple for users to accept and not lead to less comprehensibility and general trust in the model. (p.7) Monotonicity Constraints in Classification Models. Users are more like to trust and accept classification models when they are built by respecting monotonicity constraints in the application domain. A monotonic relationship between a feature and the class attribute occurs when increasing the value of the feature tends to either monotonically increase or monotonically decrease the probability of an instance\u2019s membership to a class Taxonomy Construct a set of measurable metrict which can be used to calculate a score that should indicate how good the explainability of a model is. For The calculation of the score differnt weights can be assigned to the metrics. metrics where 5 is the best and 1 the worst: Model type: Different between ML model types and assigne a score to each depending on how easily the result is to explain. Monotonicity: A score from 1 to 5 for indicating how many monotone attributes there are, the higher the fraction the better. Relevance: A score from 1 to 5 how relevant the attributes are. Ideally there are no irrelevant attributes (the definition of relevance depends on the model type) Model size: A score from 1 to 5 where bigger more complex models with many attributes have (new metric: uncorrelated dataset?) table { width:100%; } metric description unit weight model type some models like linear regression or decision tree are very explainable and models like neural networks less [1,5] 0.5 faithfulness relevance are the features truly relevant or can some be omitted? [1,5] 0.2 monotonicity monotonic attribute functions most important feature for classification [1,5] 0.2 model size more features add to the complexity and may decrease comprehensibility [1,5] 0.1","title":"Explainability"},{"location":"Pillars/Explainability/#explainability","text":"Explainability The level to which a system can provideclarification for the cause of its decisions/outputs. Interpretability has become a well-recognized goal for machine learning models. The need for interpretable models is certain to increase as machine learning pushes further into domains such as medicine, criminal justice, and business, where such models complement human decision-makers and decisions can have major consequences on human lives. Transparency is thus required for domain experts to understand, critique, and trust models, and reasoning is required to explain individual decisions. Sanjeeb et. Al","title":"Explainability"},{"location":"Pillars/Explainability/#introduction","text":"\"In many applications, trust in an AI system will come from its ability to \u2018explain itself.\u2019 Yet, when it comes to understanding and explaining the inner workings of an algorithm, one size does not fit all.Different stakeholders require explanations for different purposes and objectives, and explanations must be tailored to their needs. A physician might respond best to seeing examples of patient data similar to their patient\u2019s. On the other hand, a developer training a neural net will benefit from seeing how information flows through the algorithm. While a regulator will aim to understand the system as a whole and probe into its logic, consumers affected by a specific decision will be interested only in factors impacting their case \u2013 for example, in a loan processing application, they will expect an explanation for why the request was denied and want to understand what changes could lead to approval. IBM Research is creating diverse explanations, including training highly optimized directly interpretable models, creating contrastive explanations of black box models, using information flow in a high-performing complex model to train simpler, interpretable classifiers, learning disentangled representations, and visualizing information flows in neural networks.\" IBM","title":"Introduction"},{"location":"Pillars/Explainability/#key-takaways-questions-and-limitations","text":"Key Takaways There is a tradeoff between explainability and accuracy (complex models with many features tend to be more accurate but at the same time less transparent) The level of explainability is mainly dominated by its model type and the number of features it uses. The importance of model explainability is highly dependent on the context of the model and its consequences. (Revenue, Rate, Rigour, Regulation, Reputation and Risk) Almost every model can be implemented in a way that makes it explainable and no models are explainable by themself but need an additional layer for the translation of the results to make it explainable for endusers. Scenarios where explainability is important: Medical diagnosis, loan lending or general assisance in human desicion making. All scenarios are mainly about classification models . Limitations The scope of explainability is not only defined by the model and data selection / pre processing but often has to be embedded in the application as a whole (Build an explianable AI system with an explanation userinterface for example) Explainability needs to be accessible. Even models which would be interpretable need to have a method the extract these information and make the explantaion explicite. Open Qustions How to treat techniques (from outside: model inducion, from inside: deep explanation) which enhance explainability in models which would be less explainable by nature? What should our score on explainability tell? How easily the model could be interpreted and an XAI system could be build with it?","title":"Key Takaways, Questions and Limitations"},{"location":"Pillars/Explainability/#summaries-of-paper-website","text":"See all papers related to explainability from IBM here: publications","title":"Summaries of Paper &amp; Website"},{"location":"Pillars/Explainability/#explainable-ai","text":"The paper is business oriented and talks about why explainability is an advatage to have embedded in AI algorithms and in which usecases explainability should have a high priority. (p.8) The need for Explainability: Working as intended?, How sensitive is the impact?, Are you comfortable with the level of control? (p.9) factors to consider: Revenue, Rate, Rigour, Regulation, Reputation and Risk (p.12) Explainable by design: Explainability needs to be considered up front and embedded into the design of the AI application. It affects the choice ofmachine learning algorithm and mayimpact the way you choose to pre-process data. (p.13) Trade-offs in explainability: Interpretability is a characteristic of a model that is generally considered to come at a cost. As a rule of thumb, the more complex the model, the more accurate it is, but the less interpretable it is. (Decision tree vs. DNN with many layers and features) (p.14) Differnt explaination techniques: Sensitivity analysis,Local Interpretable Model Explanations (LIME), Shapley Additive Explanations (SHAP), Tree interpreters, Neural Network Interpreters (Appendix 2) Subjective scale of explainability of different classes of algorithms and learning techniques (with 1 being the most difficult and 5 being the easiest to explain) see a compriesed version of the table belwo: Learning technique Scale of explainability (1-5) Bayesian belief networks (BNNs) 3.5 Decision trees 4 Logistic regression 3 Support vector machines (SVMs) 2 K-means clustering 3 Neural networks 1 Random forest/boosting 3 Q-learning 2 Hidden Markov models 3","title":"Explainable AI"},{"location":"Pillars/Explainability/#explainable-artificial-intelligence-xai","text":"XAI is a project from Defense Advanced Research Projects Agency which aims to produce more explainable models, while maintaining a high level of learning performance (prediction accuracy)and enable human users to understand, appropriately trust, and effectively manage the emerging generation of artificially intelligent partners see their concept here: DARPA slide deck on explainability (two key slides below) See another slide deck on XAI with more examples here: second XAI slide deck","title":"Explainable Artificial Intelligence (XAI)"},{"location":"Pillars/Explainability/#improving-transparency-of-models","text":"Context of AI applications were transparency and explainability is key: financial risk assessment (Goyal 2018), medical diagnosis and treatment planning (Strickland 2019), hiring and promotion decisions (Alsever 2017), social services eligibility determination (Fishman, Eggers, and Kishnani 2019), predictive policing (Ensign et al. 2017), and probation and sentencing recom- mendations (Larson et al. 2016). Recent work has outlined the need for increased transparency in AI for data sets(Gebru et al. 2018; Bender and Friedman 2018; Holland et al. 2018), models (Mitchell et al. 2019),and services (Arnold et al. 2019).","title":"Improving transparency of models"},{"location":"Pillars/Explainability/#metrics-for-explainable-ai-xai","text":"Note Explaibale AI is much more than just having an easily interpretable model but about a whole system that is able to explain itself and interact with the user to create a full unterstanding tailored to the user needs. The papers elaborates metrics for the goodness of explanations, whether users are satisfied by explanations, how well users understand the AI systems, how curiosity motivates the search for explanations, whether the user's trust and reliance on the AI are appropriate.","title":"Metrics for Explainable AI (XAI)"},{"location":"Pillars/Explainability/#interpretability-to-whome","text":"A machinelearning system\u2019s interpretability should be de-fined in relation to a specific agent or task: we should not ask if the system is interpretable, but to whom is it interpretable","title":"Interpretability to whome?"},{"location":"Pillars/Explainability/#explainability-in-image-classification","text":"Saliency maps are a popular approach to creating post-hoc explanations of image classifier outputs. These methods produce estimates of the relevance of each pixel to the classification output score, which can be displayed as a saliency map that highlights important pixels.","title":"Explainability in image classification"},{"location":"Pillars/Explainability/#towards-a-rigorous-science-of-interpretable-machine-learning","text":"The need for interpretability stems from an incompleteness in the problem formalization. Scensario where this can be the case: Scientific Understanding: The human\u2019s goal is to gain knowledge. We do not have a completeway of stating what knowledge is; thus the best we can do is ask for explanations we canconvert into knowledge. Safety: For complex tasks, the end-to-end system is almost never completely testable; onecannot create a complete list of scenarios in which the system may fail. Enumerating allpossible outputs given all possible inputs be computationally or logistically infeasible, and wemay be unable to flag all undesirable outputs. Ethics: The human may want to guard against certain kinds of discrimination, and theirnotion of fairness may be too abstract to be completely encoded into the system. Even if we can encode protections for specific protected classes into the system, there might be biases that we did not consider a priori dimensions of interpretability: Global vs. Local: Global interpretability implies knowing what patterns are present in general, while local interpretability implies knowing the reasons for a specific decision (such as why a particular loan application was rejected).The former may be important for when scientific understanding or bias detection is the goal;the latter when one needs a justification for a specific decision Area, Severity of Incompleteness: What part of the problem formulation is incomplete, andhow incomplete is it? ime Constraints: How long can the user afford to spend to understand the explanation? Adecision that needs to be made at the bedside or during the operation of a plant must beunderstood quickly, while in scientific or anti-discrimination applications, the end-user maybe willing to spend hours trying to fully understand an explanation. User Expertise: How experienced is the user in the task?","title":"Towards A Rigorous Science of Interpretable Machine Learning"},{"location":"Pillars/Explainability/#comprehensible-classification-models","text":"(p.2-5) Decision trees, classification rules, decision tables, nearest neighbors, and Bayesian network classifiers, with respect to their interpretability (p.6) The Drawbacks of Model Size as the Single Measure of Comprehensibility. Big decision trees with easily understandable features can be more comprehensible than small trees with less intuitive features. Models that are too small can also be to simple for users to accept and not lead to less comprehensibility and general trust in the model. (p.7) Monotonicity Constraints in Classification Models. Users are more like to trust and accept classification models when they are built by respecting monotonicity constraints in the application domain. A monotonic relationship between a feature and the class attribute occurs when increasing the value of the feature tends to either monotonically increase or monotonically decrease the probability of an instance\u2019s membership to a class","title":"Comprehensible Classification Models"},{"location":"Pillars/Explainability/#taxonomy","text":"Construct a set of measurable metrict which can be used to calculate a score that should indicate how good the explainability of a model is. For The calculation of the score differnt weights can be assigned to the metrics. metrics where 5 is the best and 1 the worst: Model type: Different between ML model types and assigne a score to each depending on how easily the result is to explain. Monotonicity: A score from 1 to 5 for indicating how many monotone attributes there are, the higher the fraction the better. Relevance: A score from 1 to 5 how relevant the attributes are. Ideally there are no irrelevant attributes (the definition of relevance depends on the model type) Model size: A score from 1 to 5 where bigger more complex models with many attributes have (new metric: uncorrelated dataset?) table { width:100%; } metric description unit weight model type some models like linear regression or decision tree are very explainable and models like neural networks less [1,5] 0.5 faithfulness relevance are the features truly relevant or can some be omitted? [1,5] 0.2 monotonicity monotonic attribute functions most important feature for classification [1,5] 0.2 model size more features add to the complexity and may decrease comprehensibility [1,5] 0.1","title":"Taxonomy"},{"location":"Pillars/Robustness/","text":"Robustness Introduction Machine Learning models might not be robust against adversarial perturbations. With visually imperceptible adversarial perturbations, attackers may cause an input to be misclassified. These attacks can be generated in both the black-box setting, where the parameters of the model are unknown to the attacker, and the white-box settings, where attacker have the all necessary information about the model. To trust AI we need to be sure that it is robust against adversarial attacks. Key Takeaways, Questions and Limitations Key Takeaways Robustness is a mathematical term and context independent. Mostly measured by a specific adversarial attack's success rate on the model. Stability and robustness are related to models confidence score. Limitations Robustness can be defined as vulnerability to adversarial examples. An adversarial example is an instance with small, intentional feature perturbations that cause a machine learning model to make a false prediction. Hence robustness is defined on models where the \"False Prediction\" is clearly defined and where true and false predictions are sufficiently different. For example we can talk about robustness of classifiers. However it is quite difficult to talk about robustness of regression models. Robustness only makes sense if the input to the model is sufficiently large. When the input is adversarially perturbed the difference should be unnoticiable to the human eye but still be large enough to make the input misclassified. This is only possible when the input is high dimensional so that the differences can be hidden. Thats why they usually use Convolutional Neural Networks as an example. The topic is well explored on neural network classifiers. For the other models I was able to find a paper proposing an attack working for neural networks, logistic regression models and SVM's. With the help of the same paper, I learned that 2 other attacks can be modified to work for logistic regression, SVM and neural networks. Sources Websites IBM Robustness: Robustness main page Adversarial Robustness 360: Toolbox Adversarial Robustness Toolbox Github repo: ART v1.5 CLEVER Github repo: Robustness Score Papers and Summaries Year Author Title Content 2019 Maria-Irina Nicolae et al. Adversarial Robustness Toolbox v1.0.0 Adversarial Robustness Toolbox is an open-source Python library. It offers adversarial attack/defense implementations, runtime detection methods, poisoning detection and robustness metrics. 2018 Tsui-Wei Weng et al. Evaluating The Robustness of Neural Networks: An Extreme Value Theory Approach They propose a robustness metric called CLEVER (Cross Lipschitz Extreme Value for nEtwork Robustness). 2018 Huan Zhang et al. Efficient Neural Network Robustness Certification with General Activation Functions CROWN, a framework to certify robustness of neural networks for given input data points. Decision of linear and quadratic functions are made according to the activation function. They provide calculation details of tanh, sigmoid, arctan and RELU and claim that the calculations can be generalized. 2019 Hongge Chen et al. Robustness Verification of Tree-based Models Robustness verification of decision tree ensembles involves finding the exact minimal adversarial perturbation. It can be done with maximum clique searching algorithms. 2019 Elham Tabassi et al A Taxonomy and Terminology of Adversarial Machine Learning They present a taxonomy of concepts and define terminology in the field of Adversarial ML. It is arranged in a conceptual hierarchy that includes key types of attacks, defenses, and consequences. 2014 Ian Goodfellow et al. Explaining and Harnessing Adversarial Examples They come up with a fast method to generate adversarial examples. They show that \"a simple linear model can have adversarial examples if its input has sufficient dimensionality.\" Their attack works for logistic regression, SVM's and neural networks. 2017 Nicholas Carlini et al Towards Evaluating the Robustness of Neural Networks They introduce a new white-box targeted attack algorithm tailored to three distance metrics. 2017 Alexey Kurakin et al Adversarial Examples In The Physical World They prove that systems operating in the physical world, using signals from cameras and other sensors as input can be a target for adversarial attacks. While doing so they introduce a new attack called \u201cBasic Iterative Method\u201d. 2016 Seyed-Mohsen Moosavi-Dezfooli et al. Deepfool: A Simple and Accurate Method to Fool Deep neural networks They claim that they present an accurate method for computing and comparing the robustness of different classifiers. They do it using the DeepFool algorithm with creates adversarial samples. 2019 Arjun Nitin Bhagoji et al Analyzing Federated Learning through an Adversarial Lens They explore the threat of poisoning attacks on federated learning. They explore several strategies to carry out this attack and try to increase attack stealth. 2017 Matthias Hein et al Formal Guarantees on the Robustness of a Classifier against Adversarial Manipulation They propose the Cross-Lipschitz regularization functional. This form of regularization in neural networks improves the robustness of the classifier with no loss in performance. 2020 Chuanbiao Song et al Robust Local Features For Improving The Generalization of Adversarial Training They investigate the relationship between the generalization of adversarial training and the robust local features, try to make models learn robust local features from adversarial training. Metrics The main idea of the robustness metrics is to calculate the minimal perturbation that is required to get an input misclassified. In other words to calculate the average sensitivity of the model\u2019s loss function with respect to changes in the inputs. Empirical Robustness Assesses the robustness of a given classifier with respect to a specific attack and test data set. It is equivalent to the average minimal perturbation that the attacker needs to introduce for a successful attack. To be able to calculate empirical robustness we need to know the type and the parameters of the attack. Model Uncertainty & Confidence Score The model uncertainty can be estimated by computing the variance of M independently trained models predicted probability of a data point. For the same input the all of the model's predict probabilities for classes. If they returns similar probabilities, the model architecture is said to be stable and robust. The experiment results show that the Model Uncertainty and Confidence Scores are inversely related where the Confidence Score is defined as average prediction probability of the group of models. For one model we can define Confidence Score as the ratio of probabilities of the predicted class and the second close class. Larger ratio can imply a more robust model. Loss Sensitivity Local loss sensitivity quantify the smoothness of a model by estimating its Lipschitz continuity constant. Lipschitz constant measures the largest variation of the output of the model under a small change in its input. The smaller the value, the smoother the function. CLEVER Score For a given input CLEVER estimates the minimal perturbation that is required to change the classification. The derivation is based on a Lipschitz constant. The CLEVER algorithm uses an estimate based on extreme value theory. The CLEVER score can be calculated for both untargeted and targeted attacks. A higher CLEVER score indicates better robustness. Loss Sensitivity VS CLEVER Score CLEVER Score and Loss Sensitivity both uses the same idea of estimating the smoothness by Lipschitz constant. They both are attack-independent. But they have a couple of differences. Loss sensitivity uses Local Lipschitz constant estimation meanwhile CLEVER uses Cross Lipschitz constant estimation. Even though they try to estimate similar constants, Cross Lipschitz constant calculation includes an additional difference operation. Hence a model with a high CLEVER score would have a low loss sensitivity. For Loss sensitivity; the smaller the value, the smoother the function. For CLEVER score; the greater the value, the more robust the model. As CLEVER uses a sampling based method, the scores may vary slightly for different runs while Loss Sensitivity calculation is determined. Clique Method Robustness Verification for Decision Tree Ensembles For decision-tree ensemble models like Gradient Boosted Decision Trees, Random Forest, or Extra Trees robustness can be verified based on clique method. For our purposes this measure might be too specific. CROWN framework for certifying neural networks CROWN is a framework for efficiently computing a certified lower bound of minimum adversarial distortion given any input data point for neural networks. To calculate the lower bound one must know about the activation function used in the Neural Network. Then check where the activation function is concave or convex and choose lower/upper bound functions accordingly. Not a fully designed metric, just gives an upper bound with respect to the given data point. Attacks As most of the papers use success rate of attacks as a robustness metric we can use state-of-the-art attacks in our algorithm. Carlini Wagner attack It is a white-box targeted attack algorithm tailored to three distance metrics (l 2 , l 0 , l infinity ). It is referred as a state-of-the-art attack. It tries to optimize a minimization problem using gradient descent. Quite powerful, however it is often much slower than others. Even though it is developed for Neural Networks, it can be modified to be effective on Logistic Regression models and Support Vector Classifiers. Fast Gradient Attack It is a white-box attack and has targeted/untargeted versions. Straightforward and practical. It is effective on Logistic Regression models, Support Vector Classifiers and Neural Networks. DeepFool An efficient attack for deep neural networks. It is white-box and untargeted. For a given input, it finds the nearest decision boundary in l 2 norm. It can be modified to be effective on Logistic Regression models and Support Vector Classifiers. Taxonomy model type metric description unit All models that we have access to the prediction probabilities Confidence Score Measures the stability and robustness of the model. Larger confidence better robustness. [0 1] Decision Tree Clique Method Robustness Verification Gives a lower bound on robustness for decision tree ensembles. Larger value better robustness. [0 inf] Neural Network Loss Sensitivity Quantify the smoothness of a model. Smaller value better robustness. [0 inf] Neural Network CLEVER Score Estimates the minimal perturbation that is required to change the classification. Higher value better robustness. [0 inf] Neural Network, Logistic Regression, SVM Empirical robustness (CW attack) Success rate of the CW attack. Smaller value better robustness. [0 1] Neural Network, Logistic Regression, SVM Empirical robustness (Fast Gradient Method) Success rate of the Fast Gradient Attack. Smaller value better robustness. [0 1] Neural Network, Logistic Regression, SVM Empirical robustness (DeepFool) Success rate of the DeepFool attack. Smaller value better robustness. [0 1]","title":"Robustness"},{"location":"Pillars/Robustness/#robustness","text":"","title":"Robustness"},{"location":"Pillars/Robustness/#introduction","text":"Machine Learning models might not be robust against adversarial perturbations. With visually imperceptible adversarial perturbations, attackers may cause an input to be misclassified. These attacks can be generated in both the black-box setting, where the parameters of the model are unknown to the attacker, and the white-box settings, where attacker have the all necessary information about the model. To trust AI we need to be sure that it is robust against adversarial attacks.","title":"Introduction"},{"location":"Pillars/Robustness/#key-takeaways-questions-and-limitations","text":"Key Takeaways Robustness is a mathematical term and context independent. Mostly measured by a specific adversarial attack's success rate on the model. Stability and robustness are related to models confidence score. Limitations Robustness can be defined as vulnerability to adversarial examples. An adversarial example is an instance with small, intentional feature perturbations that cause a machine learning model to make a false prediction. Hence robustness is defined on models where the \"False Prediction\" is clearly defined and where true and false predictions are sufficiently different. For example we can talk about robustness of classifiers. However it is quite difficult to talk about robustness of regression models. Robustness only makes sense if the input to the model is sufficiently large. When the input is adversarially perturbed the difference should be unnoticiable to the human eye but still be large enough to make the input misclassified. This is only possible when the input is high dimensional so that the differences can be hidden. Thats why they usually use Convolutional Neural Networks as an example. The topic is well explored on neural network classifiers. For the other models I was able to find a paper proposing an attack working for neural networks, logistic regression models and SVM's. With the help of the same paper, I learned that 2 other attacks can be modified to work for logistic regression, SVM and neural networks.","title":"Key Takeaways, Questions and Limitations"},{"location":"Pillars/Robustness/#sources","text":"","title":"Sources"},{"location":"Pillars/Robustness/#websites","text":"IBM Robustness: Robustness main page Adversarial Robustness 360: Toolbox Adversarial Robustness Toolbox Github repo: ART v1.5 CLEVER Github repo: Robustness Score","title":"Websites"},{"location":"Pillars/Robustness/#papers-and-summaries","text":"Year Author Title Content 2019 Maria-Irina Nicolae et al. Adversarial Robustness Toolbox v1.0.0 Adversarial Robustness Toolbox is an open-source Python library. It offers adversarial attack/defense implementations, runtime detection methods, poisoning detection and robustness metrics. 2018 Tsui-Wei Weng et al. Evaluating The Robustness of Neural Networks: An Extreme Value Theory Approach They propose a robustness metric called CLEVER (Cross Lipschitz Extreme Value for nEtwork Robustness). 2018 Huan Zhang et al. Efficient Neural Network Robustness Certification with General Activation Functions CROWN, a framework to certify robustness of neural networks for given input data points. Decision of linear and quadratic functions are made according to the activation function. They provide calculation details of tanh, sigmoid, arctan and RELU and claim that the calculations can be generalized. 2019 Hongge Chen et al. Robustness Verification of Tree-based Models Robustness verification of decision tree ensembles involves finding the exact minimal adversarial perturbation. It can be done with maximum clique searching algorithms. 2019 Elham Tabassi et al A Taxonomy and Terminology of Adversarial Machine Learning They present a taxonomy of concepts and define terminology in the field of Adversarial ML. It is arranged in a conceptual hierarchy that includes key types of attacks, defenses, and consequences. 2014 Ian Goodfellow et al. Explaining and Harnessing Adversarial Examples They come up with a fast method to generate adversarial examples. They show that \"a simple linear model can have adversarial examples if its input has sufficient dimensionality.\" Their attack works for logistic regression, SVM's and neural networks. 2017 Nicholas Carlini et al Towards Evaluating the Robustness of Neural Networks They introduce a new white-box targeted attack algorithm tailored to three distance metrics. 2017 Alexey Kurakin et al Adversarial Examples In The Physical World They prove that systems operating in the physical world, using signals from cameras and other sensors as input can be a target for adversarial attacks. While doing so they introduce a new attack called \u201cBasic Iterative Method\u201d. 2016 Seyed-Mohsen Moosavi-Dezfooli et al. Deepfool: A Simple and Accurate Method to Fool Deep neural networks They claim that they present an accurate method for computing and comparing the robustness of different classifiers. They do it using the DeepFool algorithm with creates adversarial samples. 2019 Arjun Nitin Bhagoji et al Analyzing Federated Learning through an Adversarial Lens They explore the threat of poisoning attacks on federated learning. They explore several strategies to carry out this attack and try to increase attack stealth. 2017 Matthias Hein et al Formal Guarantees on the Robustness of a Classifier against Adversarial Manipulation They propose the Cross-Lipschitz regularization functional. This form of regularization in neural networks improves the robustness of the classifier with no loss in performance. 2020 Chuanbiao Song et al Robust Local Features For Improving The Generalization of Adversarial Training They investigate the relationship between the generalization of adversarial training and the robust local features, try to make models learn robust local features from adversarial training.","title":"Papers and Summaries"},{"location":"Pillars/Robustness/#metrics","text":"The main idea of the robustness metrics is to calculate the minimal perturbation that is required to get an input misclassified. In other words to calculate the average sensitivity of the model\u2019s loss function with respect to changes in the inputs.","title":"Metrics"},{"location":"Pillars/Robustness/#empirical-robustness","text":"Assesses the robustness of a given classifier with respect to a specific attack and test data set. It is equivalent to the average minimal perturbation that the attacker needs to introduce for a successful attack. To be able to calculate empirical robustness we need to know the type and the parameters of the attack.","title":"Empirical Robustness"},{"location":"Pillars/Robustness/#model-uncertainty-confidence-score","text":"The model uncertainty can be estimated by computing the variance of M independently trained models predicted probability of a data point. For the same input the all of the model's predict probabilities for classes. If they returns similar probabilities, the model architecture is said to be stable and robust. The experiment results show that the Model Uncertainty and Confidence Scores are inversely related where the Confidence Score is defined as average prediction probability of the group of models. For one model we can define Confidence Score as the ratio of probabilities of the predicted class and the second close class. Larger ratio can imply a more robust model.","title":"Model Uncertainty &amp; Confidence Score"},{"location":"Pillars/Robustness/#loss-sensitivity","text":"Local loss sensitivity quantify the smoothness of a model by estimating its Lipschitz continuity constant. Lipschitz constant measures the largest variation of the output of the model under a small change in its input. The smaller the value, the smoother the function.","title":"Loss Sensitivity"},{"location":"Pillars/Robustness/#clever-score","text":"For a given input CLEVER estimates the minimal perturbation that is required to change the classification. The derivation is based on a Lipschitz constant. The CLEVER algorithm uses an estimate based on extreme value theory. The CLEVER score can be calculated for both untargeted and targeted attacks. A higher CLEVER score indicates better robustness.","title":"CLEVER Score"},{"location":"Pillars/Robustness/#loss-sensitivity-vs-clever-score","text":"CLEVER Score and Loss Sensitivity both uses the same idea of estimating the smoothness by Lipschitz constant. They both are attack-independent. But they have a couple of differences. Loss sensitivity uses Local Lipschitz constant estimation meanwhile CLEVER uses Cross Lipschitz constant estimation. Even though they try to estimate similar constants, Cross Lipschitz constant calculation includes an additional difference operation. Hence a model with a high CLEVER score would have a low loss sensitivity. For Loss sensitivity; the smaller the value, the smoother the function. For CLEVER score; the greater the value, the more robust the model. As CLEVER uses a sampling based method, the scores may vary slightly for different runs while Loss Sensitivity calculation is determined.","title":"Loss Sensitivity VS CLEVER Score"},{"location":"Pillars/Robustness/#clique-method-robustness-verification-for-decision-tree-ensembles","text":"For decision-tree ensemble models like Gradient Boosted Decision Trees, Random Forest, or Extra Trees robustness can be verified based on clique method. For our purposes this measure might be too specific.","title":"Clique Method Robustness Verification for Decision Tree Ensembles"},{"location":"Pillars/Robustness/#crown-framework-for-certifying-neural-networks","text":"CROWN is a framework for efficiently computing a certified lower bound of minimum adversarial distortion given any input data point for neural networks. To calculate the lower bound one must know about the activation function used in the Neural Network. Then check where the activation function is concave or convex and choose lower/upper bound functions accordingly. Not a fully designed metric, just gives an upper bound with respect to the given data point.","title":"CROWN framework for certifying neural networks"},{"location":"Pillars/Robustness/#attacks","text":"As most of the papers use success rate of attacks as a robustness metric we can use state-of-the-art attacks in our algorithm.","title":"Attacks"},{"location":"Pillars/Robustness/#carlini-wagner-attack","text":"It is a white-box targeted attack algorithm tailored to three distance metrics (l 2 , l 0 , l infinity ). It is referred as a state-of-the-art attack. It tries to optimize a minimization problem using gradient descent. Quite powerful, however it is often much slower than others. Even though it is developed for Neural Networks, it can be modified to be effective on Logistic Regression models and Support Vector Classifiers.","title":"Carlini Wagner attack"},{"location":"Pillars/Robustness/#fast-gradient-attack","text":"It is a white-box attack and has targeted/untargeted versions. Straightforward and practical. It is effective on Logistic Regression models, Support Vector Classifiers and Neural Networks.","title":"Fast Gradient Attack"},{"location":"Pillars/Robustness/#deepfool","text":"An efficient attack for deep neural networks. It is white-box and untargeted. For a given input, it finds the nearest decision boundary in l 2 norm. It can be modified to be effective on Logistic Regression models and Support Vector Classifiers.","title":"DeepFool"},{"location":"Pillars/Robustness/#taxonomy","text":"model type metric description unit All models that we have access to the prediction probabilities Confidence Score Measures the stability and robustness of the model. Larger confidence better robustness. [0 1] Decision Tree Clique Method Robustness Verification Gives a lower bound on robustness for decision tree ensembles. Larger value better robustness. [0 inf] Neural Network Loss Sensitivity Quantify the smoothness of a model. Smaller value better robustness. [0 inf] Neural Network CLEVER Score Estimates the minimal perturbation that is required to change the classification. Higher value better robustness. [0 inf] Neural Network, Logistic Regression, SVM Empirical robustness (CW attack) Success rate of the CW attack. Smaller value better robustness. [0 1] Neural Network, Logistic Regression, SVM Empirical robustness (Fast Gradient Method) Success rate of the Fast Gradient Attack. Smaller value better robustness. [0 1] Neural Network, Logistic Regression, SVM Empirical robustness (DeepFool) Success rate of the DeepFool attack. Smaller value better robustness. [0 1]","title":"Taxonomy"},{"location":"Pillars/Accountability/","text":"Accountability and Transparency While there has been a push to make developers more cognizant of the possible repercussions of their algorithms, others point out that public agencies and companies reliant on AI also need to be accountable. \u201cThere is this emphasis on designers understanding a system. But it\u2019s also about the people administering and implementing the system,\u201d says Jason Schultz, a professor of law at New York University who works with the AI Now Institute on legal and policy issues. \"That\u2019s where the rubber meets the road in accountability. A government agency using AI has the most responsibility and they need to understand it, too. If you can\u2019t understand the technology, you shouldn\u2019t be able to use it.\u201d To that end, AI Now is promoting the use of \u201calgorithmic impact assessments,\u201d which would require public agencies to disclose the systems they\u2019re using, and allow outside researchers to analyze them for potential problems. When it comes to police departments, some legal experts think it\u2019s also important for them to clearly spell out how they\u2019re using technology and be willing to share that with the local community. Which metrics should be used in order to communicate the quality of a model to a Mangement requires interpretability to gain comfort and build confidence that they should deploy the system Greater visibility uncovers unknown vulnerabilities and flaws. How many people would trust an AI algorithm giving a diagnosis rather than a doctor without having some form of clarity over how the algorithm came up with the conclusion? Although the AI diagnosis may be more accurate, a lack of explainability may lead to a lack of trust. Why does this enable governance? Increased transparency provides information for AI consumers to better understand how an AI model or service was created and build in order to determine if the model is appropriate for their specific situation or need. Safety, fairness, explainability and robustness are characteristics we demand from an AI system. Yet, to achieve trust in AI, engineering these traits into a final solution will not be enough; it must be accompanied with the ability to measure and communicate the performance levels of a system on each of these dimensions. By demonstrating how to effectively capture events and performance metrics of an AI system on hyperledger fabric, and introduce the concept of factsheets for AI services, which outlines how these metrics can be communicated to end-users as a way of informing their understanding of how the service works, evaluating its functionality, and comprehending its strengths and limitations. Many decision making processes can be suported by machine learning models. For example if someone should go to jail or should be released until their trial. Or if an employee will perform well and therefore should be hired or not? The involvment of the machine can reduce the humane workload, ncrease accuracy and enlarge fairness. Every person involved in the creation of AI at any step is accountable for considering the system\u2019s impact in the world, as are the companies invested in its development. Transparency => Information that lets you know what your system is doing. If you are more transparent as a company and openly communicate why certain design decisions were taken end users better understand what has been done. Recommended Actions Make company policies clear and accessible to design and development teams from day one so that no one is confused about issues of responsibility or accountability. As an AI designer or developer, it is your responsibility to know. Understand where the responsibility of the company/software ends. You may not have control over how data or a tool will be used by a user, client, or other external source. Keep detailed records of your design processes and decision making. Determine a strategy for keeping records during the design and development process to encourage best practices and encourage iteration. Adhere to your company\u2019s business conduct guidelines. Also, understand national and international laws, regulations, and guidelines that your AI may have to work within. You can find other related resources in the IEEE Ethically Aligned Design Document [1]. Fact Sheet Components Model Name Overview Purpose Intended Domain Training Data Input Output Metrics Bias Robustness Domain Shift Test Data Optimal Conditions Poor Conditions Explanation Contact Information Summary Summary papers Arnold et al. 2019: Arnold, M.; Bellamy, R. K. E.; Hind, M.; Houde, S.; Mehta, S.; Mojsilovi \u0301c, A.; Nair, R.; Natesan- Ramamurthy, K.; Olteanu, A.; Piorkowski, D.; Reimer, D.; Richards, J.; Tsay, J.; and Varshney, K. R. 2019. FactSheets: Increasing trust in AI services through suppliers declarations of conformity. IBM Journal of Research & Development 63(4/5). A FactSheet, as proposed by (Arnold et al. 2019), is a col- lection of relevant information about an AI model or ser- vice that is created during the machine learning life cycle. It includes information from the business owner (e.g., in- tended use and business justification), from the data gather- ing/feature selection/data cleaning phase (e.g., data sets, fea- tures used or created, cleaning operations), from the model training phase (e.g., bias, robustness, and explainability in- formation), and from the model validation and deployment phase (e.g., key performance indicators). A FactSheet is as- sociated with a model (or service) and is meant to be write once, i.e., an update to a model would trigger a new Fact- Sheet for the updated model. FactSheets can be consumed by any role in the ML life cycle to confirm process gov- ernance adherence or model performance, or by the ulti- mate users of a model to provide increased transparency. Links [1] ... IBM 2019 - Accountability - https://www.ibm.com/design/ai/ethics/accountability/ Taxonomy and Metrics metric target description unit weight Train-Test-Split Data Is the split into training and testing data appropriate [0,1] 0.2 Level of Human Involvement Model Is the AI to be embedded in a human decision-making process, is it making decisions on its own, or is it a hybrid? {1/3, 2/3, 3/3} 0.2 Fact Sheet Availability Does the model come with a FactSheet? {0, 1} 0.2","title":"Accountability"},{"location":"Pillars/Accountability/#accountability-and-transparency","text":"While there has been a push to make developers more cognizant of the possible repercussions of their algorithms, others point out that public agencies and companies reliant on AI also need to be accountable. \u201cThere is this emphasis on designers understanding a system. But it\u2019s also about the people administering and implementing the system,\u201d says Jason Schultz, a professor of law at New York University who works with the AI Now Institute on legal and policy issues. \"That\u2019s where the rubber meets the road in accountability. A government agency using AI has the most responsibility and they need to understand it, too. If you can\u2019t understand the technology, you shouldn\u2019t be able to use it.\u201d To that end, AI Now is promoting the use of \u201calgorithmic impact assessments,\u201d which would require public agencies to disclose the systems they\u2019re using, and allow outside researchers to analyze them for potential problems. When it comes to police departments, some legal experts think it\u2019s also important for them to clearly spell out how they\u2019re using technology and be willing to share that with the local community. Which metrics should be used in order to communicate the quality of a model to a Mangement requires interpretability to gain comfort and build confidence that they should deploy the system Greater visibility uncovers unknown vulnerabilities and flaws. How many people would trust an AI algorithm giving a diagnosis rather than a doctor without having some form of clarity over how the algorithm came up with the conclusion? Although the AI diagnosis may be more accurate, a lack of explainability may lead to a lack of trust. Why does this enable governance? Increased transparency provides information for AI consumers to better understand how an AI model or service was created and build in order to determine if the model is appropriate for their specific situation or need. Safety, fairness, explainability and robustness are characteristics we demand from an AI system. Yet, to achieve trust in AI, engineering these traits into a final solution will not be enough; it must be accompanied with the ability to measure and communicate the performance levels of a system on each of these dimensions. By demonstrating how to effectively capture events and performance metrics of an AI system on hyperledger fabric, and introduce the concept of factsheets for AI services, which outlines how these metrics can be communicated to end-users as a way of informing their understanding of how the service works, evaluating its functionality, and comprehending its strengths and limitations. Many decision making processes can be suported by machine learning models. For example if someone should go to jail or should be released until their trial. Or if an employee will perform well and therefore should be hired or not? The involvment of the machine can reduce the humane workload, ncrease accuracy and enlarge fairness. Every person involved in the creation of AI at any step is accountable for considering the system\u2019s impact in the world, as are the companies invested in its development. Transparency => Information that lets you know what your system is doing. If you are more transparent as a company and openly communicate why certain design decisions were taken end users better understand what has been done.","title":"Accountability and Transparency"},{"location":"Pillars/Accountability/#recommended-actions","text":"Make company policies clear and accessible to design and development teams from day one so that no one is confused about issues of responsibility or accountability. As an AI designer or developer, it is your responsibility to know. Understand where the responsibility of the company/software ends. You may not have control over how data or a tool will be used by a user, client, or other external source. Keep detailed records of your design processes and decision making. Determine a strategy for keeping records during the design and development process to encourage best practices and encourage iteration. Adhere to your company\u2019s business conduct guidelines. Also, understand national and international laws, regulations, and guidelines that your AI may have to work within. You can find other related resources in the IEEE Ethically Aligned Design Document [1].","title":"Recommended Actions"},{"location":"Pillars/Accountability/#fact-sheet-components","text":"Model Name Overview Purpose Intended Domain Training Data Input Output Metrics Bias Robustness Domain Shift Test Data Optimal Conditions Poor Conditions Explanation Contact Information","title":"Fact Sheet Components"},{"location":"Pillars/Accountability/#summary","text":"","title":"Summary"},{"location":"Pillars/Accountability/#summary-papers","text":"Arnold et al. 2019: Arnold, M.; Bellamy, R. K. E.; Hind, M.; Houde, S.; Mehta, S.; Mojsilovi \u0301c, A.; Nair, R.; Natesan- Ramamurthy, K.; Olteanu, A.; Piorkowski, D.; Reimer, D.; Richards, J.; Tsay, J.; and Varshney, K. R. 2019. FactSheets: Increasing trust in AI services through suppliers declarations of conformity. IBM Journal of Research & Development 63(4/5). A FactSheet, as proposed by (Arnold et al. 2019), is a col- lection of relevant information about an AI model or ser- vice that is created during the machine learning life cycle. It includes information from the business owner (e.g., in- tended use and business justification), from the data gather- ing/feature selection/data cleaning phase (e.g., data sets, fea- tures used or created, cleaning operations), from the model training phase (e.g., bias, robustness, and explainability in- formation), and from the model validation and deployment phase (e.g., key performance indicators). A FactSheet is as- sociated with a model (or service) and is meant to be write once, i.e., an update to a model would trigger a new Fact- Sheet for the updated model. FactSheets can be consumed by any role in the ML life cycle to confirm process gov- ernance adherence or model performance, or by the ulti- mate users of a model to provide increased transparency.","title":"Summary papers"},{"location":"Pillars/Accountability/#links","text":"[1] ... IBM 2019 - Accountability - https://www.ibm.com/design/ai/ethics/accountability/","title":"Links"},{"location":"Pillars/Accountability/#taxonomy-and-metrics","text":"metric target description unit weight Train-Test-Split Data Is the split into training and testing data appropriate [0,1] 0.2 Level of Human Involvement Model Is the AI to be embedded in a human decision-making process, is it making decisions on its own, or is it a hybrid? {1/3, 2/3, 3/3} 0.2 Fact Sheet Availability Does the model come with a FactSheet? {0, 1} 0.2","title":"Taxonomy and Metrics"},{"location":"Pillars/Fairness/","text":"Fairness Introduction An increasing number of information systems take decisions based on statistical inference rules, acquired through machine learning techniques. Especially in decision-critical contexts such as predictive policing, lending decisions in credit scoring or triage and allocation of health care resources discrimination is to be avoided. In order to avoid encoding discrimination in automated decisions, multiple fairness aspects needs to be accounted for. Legally recognized 'protected classes' Depending on the context the stakeholders need to decide during the model creation process which systemic difference between groups should be available to the model and which should be excluded. These sensitive features should either not be used or very carefully, since they are protected by law. Race (Civil Rights Act of 1964) Color (Civil Rights Act of 1964) Sex (Equal Pay Act of 1963; Civil Rights Act of 1964) Religion (Civil Rights Act of 1964) National Origin (Civil Rights Act of 1964) Citizenship (Immigration Reform and Control Act) Age (Age Discrimination in Employment Act of 1967) Pregnancy (Pregnancy Discrimination Act) Family Status (Civil Rights Act of 1968) Disability Status (Rehabilitation Act of 1973; Americans with Disabilities Act of 1990) Veteran Status (Vietnam Era Veterans Readjustment Assistance Act of 1974) Genetic Information (Genetic Information Nondiscrimination Act) Sexual orientation (in some jurisdications) Application Examples Medical Testing and Diagnosis Making decisions about a patient\u2019s treatment may rely on tests providing probability estimates for different diseases and conditions. Here too we can ask whether such decision-making is being applied uniformly across different groups of patients. Hiring Lending School Admission Criminal Justice Counterexamples Examples of Algorithmic Unfairness COMPAS risk tool Assess a defendant\u2019s probability of recidivism. Want to keep Tool\u2019s errors were asymmetric: African-American defendants were more likely to be incorrectly labeled as higher-risk than they actually were, while white defendants were more likely to be incorrectly labeled as lower-risk than they actually were. White defendants who went on to commit future crimes were assigned risk scores corresponding to lower probability estimates in aggregate [5]. Apple Credit Card Apple Card's Credit Scoring Algorithm was investigated after discriminating against women. Beauty AI Contest The first international beauty contest decided by an algorithm has sparked controversy after the results revealed that the algorithm discriminated against people of color [1]. Google Advertising Discrimination A team of researchers form Carnegie Mellon university build a tool called AdFisher to automatically test the Google Adwords system. They found out that women are much less likely to be shown ads for high paid positions [3]. University Admission Universities are starting to use machine learning tools in order to predict which students are going to perform well. This can potentially lead to a discrimination based on the students family background [4]. Predictive Policing Violent crime is like a communicable disease, that it tends to break out in geographic clusters. PredPol identifies areas in a neighborhood where serious crimes are more likely to occur during a particular period. The company claims its research has found the software to be twice as accurate as human analysts when it comes to predicting where crimes will happen [6]. Amazon's AI Recruiting Engine Amazons recruiting engine discriminated against women. Since most software developers at Amazon are male the system thought they are prefered over women. Sources of Bias The term bias to refer to computer systems that systematically and unfairly discriminate against certain individuals or groups of individuals in favor of others. A system discriminates unfairly if it denies an opportunity or a good or if it assigns an undesirable outcome to an individual or group of individuals on grounds that are unreasonable or inappropriate. Consider, for example, an automated credit advisor that assists in the decision of whether or not to extend credit to a particular applicant. If the advisor denies credit to individuals with consistently poor payment records we do not judge the system to be biased because it is reasonable and appropriate for a credit company to want to avoid extending credit privileges to people who consistently do not pay their bills. In contrast, a credit advisor that systematically assigns poor credit ratings to individuals with ethnic surnames discriminates on grounds that are not relevant to credit assessments and, hence, discriminates unfairly. Bias is systematic discrimination with an unfair outcome [8]. Three categories of bias in computer systems Preexisting Has its roots in social institutions, practices, and attitudes. Can also reflect the personal biases of individuals who have significant input into the design of the system. Can enter a system either through the explicit and conscious efforts of individuals or institutions, or implicitly and unconsciously, even in spite of the best of intentions. Technical Arises from technical constraints or considerations. E.g bias that originates from the use of an algorithm that fails to treat all groups fairly. Emergent Arises in a context of use. Typically emerges some time after a design is completed, as a result of changing societal knowledge, population, or cultural values If the system is complex, and most are, biases can remain hidden in the code. The distinction between a rationally based discrimination and bias is always easy to draw Common types of bias in machine learning Automation bias When a human decision maker favors recommendations made by an automated decision-making system over information made without automation, even when the automated decision-making system makes errors. Confirmation bias The tendency to search for, interpret, favor, and recall information in a way that confirms one's preexisting beliefs or hypotheses. Machine learning developers may inadvertently collect or label data in ways that influence an outcome supporting their existing beliefs Experimenter\u2019s bias Is a form of confirmation bias in which an experimenter continues training models until a preexisting hypothesis is confirmed. Group attribution bias Assuming that what is true for an individual is also true for everyone in that group. Implicit bias Automatically making an association or assumption based on one\u2019s mental models and memories In-group bias Showing partiality to one's own group or own characteristics. If testers or raters consist of the machine learning developer's friends, family, or colleagues, then in-group bias may invalidate product testing or the dataset Out-group homogeneity bias Showing partiality to one's own group or own characteristics. If testers or raters consist of the machine learning developer's friends, family, or colleagues, then in-group bias may invalidate product testing or the dataset. Coverage bias The population represented in the dataset does not match the population that the machine learning model is making predictions about. Non-response bias Also called participation bias. Users from certain groups opt-out of surveys at different rates than users from other groups. Reporting bias The fact that the frequency with which people write about actions, outcomes, or properties is not a reflection of their real-world frequencies or the degree to which a property is characteristic of a class of individuals. Reporting bias can influence the composition of data that machine learning systems learn from. For example, in books, the word laughed is more prevalent than breathed. A machine learning model that estimates the relative frequency of laughing and breathing from a book corpus would probably determine that laughing is more common than breathing. Sampling bias Data is not collected randomly from the target group. Selection bias Errors in conclusions drawn from sampled data due to a selection process that generates systematic differences between samples observed in the data and those not observed. Sources of Unfairness Data Unfairness ... from Task Definition and Data Collection. Algorithmic Unfairness ... from Model Specification and Model Training. Impact Unfairness ... from Model Testing and Deployment in Real-World. Algorithmic Unfairness f \u2217 ,the underlying model (y i =f \u2217 (x i )+\u03b5 i ). h \u2217 \u2208 H, the best available hypothesis. h=argmin h\u2032\u2208H L(D,h\u2032), the best model on finite sample. For the sake of concreteness, s \u2208 {0, 1} is assumed. Unfairness = E[(h(x)\u2212y) 2 |s=0] \u2212 E[(h(x)\u2212y) 2 |s=1] Fairness Metrics within the ML Pipeline 1. Business Understanding Ask relevant questions and define objectives for the problem that needs to be tackled. Counter Examples Inferring criminality using face images is unethical. Identifying gay faces is an unenthical use case. 2. Data Mining Gather and scrape the data necessary for the project. Mismatch between training and deployment populations (e.g Different population or drifting populations) Under/over-representation Less data from the minority (e.g., accents in speech recognition) Feedback loops (e.g observe if \u201cpaid back the loan\u201d only if loan granted) 3. Data Cleaning Fix the inconsistencies within the data and handle the missing values. Class imbalance: The number of observations for each class deviates substantially from the proportions in the underlying population. Over- and Undersampling Unfairness from Underrepresentation 4. Data Exploration Form hypotheses about your defined problem by visually analyzing the data. 5. Feature Engineering Select important features and construct more meaningful ones using the raw data that you have. 6. Predictive Modeling Train machine learning models, evaluate their performance, and use them to make predictions. 7. Data Visualization Communicate the findings with key stakeholders using plots and interactive visualizations. 8. Deployment Unintended use or adversarial feedback (e.g Tay.ai) Gaming the system. Users can try to use the rules and procedures meant to protect the system, in order to instead, manipulate the system. Statistical Measures Of Fairness The closer these values are to one the better the classifier. Positive Predicted Value (PPV) How many of the samples that were classified as pos samples are actually pos samples. Also called precision. PPV = P(actual=+|prediction=+) = TP / (TP + FP) False Discovery Rate (FDR) The fractions of samples that got classified as pos samples but however actually are negative samples. FDR = P(actual=-|prediction=+) = FP / (TP + FP) Negative Predicted Value (NPV) NPV = P(actual=-|prediction=+) = TN / (TN + FN) False Omission Rate (FOR) FOR = P(actual=+|prediction=-) = FN / (TN + FN) True Positive Rate (TPR) Number of positive samples that got successfully identified as positives. Also called sensitivity or recall. TPR = P(prediction=+|actual=+) = TP / (TP + FN) False Negative Rate (FNR) Number of positive samples that accidentally got identified as negatives. FNR = P(prediction=-|actual=+) = FN / (TP + FN) True Negative Rate (TNR) TNR = P(prediction=-|actual=-) = TN / (TN + FP) False Positive Rate (FPR) FPR = P(prediction=+|actual=-) = FP / (TN + FP) Overview (1) and (2) are widely used ideas inspired by anti-discrimnation legislation. There are group notions of fairness (e.g statistical parity, equality of accuracy, equality of false positive/false negative rates ) and individual notions (e.g treating similar individuals similar ). There is no universally accepted definition of fairness. The existing notions consider how errors are distributed across different groups. Biased Data During the data collection bias can be introduced. Selection bias for example could result in the data not being representative of the underlying population. Or depending on how certain questions are framed the results of a survey could also be different. However it is really difficult to automatically detect bias in data since you need to compare the data to something in order to identify deviations. Sample Bias: When the sample process generating the data is not uniform across protected groups. Label Bias: We define \u2018label bias\u2019 as the case when there is a causal link between a protected attribute and the class label assigned to an individual which is not warranted by ground truth. Consider a dataset composed of elementary school students, with a dependent variable that indicates whether the student misbehaves. Studies have shown that Black and Latinx children are more likely than White children to receive suspensions or expulsions for similar problem behavior, so if our dataset\u2019s dependent variable were based on suspensions it would contain label bias. Simpson\u2019s paradox Cases where different levels of data aggregation produce different fairness conclusions. An analysis of graduate admissions data from Berkeley offers one such case [1], in which the aggregate data show an admissions bias against women, but when the data are disaggregated to the department level the bias is reversed. This difficulty stems from the causal influence of the protected class (in this case gender) on the presence of an individual in the set of applicants to each department: women preferentially apply to departments with lower acceptance rates. Class Imbalance Imagine a dataset with 99% samples with label one and 1% samples with label zero. A classifier returning always one as the predicted label would still receive an accuracy of 99%. Individual Fairness Treating people as individuals, regardless of their group membership. Similarity is defined with respect to the task at hand. Individual notions, while intuitively appealing, are hard to formalize (what is the right notion of benefit or distance?) and are computationally challenging. Formalizing Individual Fairness d(x i , x j ): a metric defining distance between two individuals. D: a measure of distance between distributions. A randomized classifier h : X \u2192 \u2206(Y) satisfies the (D, d)-Lipschitz property if \u2200x i , x j : D(\u2206 h (x i ), \u2206 h (x j )) \u2264 L d(x i , x j ) for a real number L > 0. This means that the differences in prediction between individuals need to be based on a well reasoned distance in the feature space with regard to the application context. How should we pick d, D and L? Does not treat dissimilar individuals differently. Unfairness as inequality of received benefit Define the benefit obtained by each individual. \\(\\forall\\ individual\\ i: b_i = \\hat{y}_i - y_i + 1\\) Plug the different benefits into the inequality equation e.g Gini index. \\(G(b) = \\frac{\\Sigma_{i=1}^{n}\\Sigma_{j=1}^{n}|b_i-b_j|}{2n \\Sigma_{i=1}^{n}b_i}\\) Group Fairness 1. Disparate Treatment A practice that intentionally disadvantages/discriminates a group based on a protected feature ( e.g the pay difference between men and women at the same position, ). The treatment or process should not depend on a sensitive feature encoding group membership. How to check for Disparate Treatment Depending on the context certain attributes are considered to be protected. For hiring decisions in Germany for example the exmployer is not allowed to use certain information e.g (pregnancy status, wish for a child, relationship status). The user of our tool could be presented with all attributes the model is taking into consideration. He could then select every attribute he would consider in this context. On the other hand side it would be possible to define a default list of protected attributes (gender, religion, race) and let their use negatively influence the fairness score. 2. Disparate Impact Is what occurs when an organization\u2019s actions, policies, or some other aspect of their processes inadvertently result in unintentional discrimination against people who are in a protected class. Even though the policy, action, or item in question would otherwise appear to be neutral. What matters is the outcome, not the intent. Equality in outcomes across groups based on protected features has to be assured. How to check for Disparate Impact 3. Statistical Parity In some cases statistical parity is a central goal (and in some it is legally mandated). Statistical parity, ensures that the overall proportion of members in a protected group receiving positive (negative) classification are identical to the proportion of the population as a whole. \u201cA selection rate for any race, sex, or ethnic group which is less than four-fifths (or 80%) of the rate for the group with the highest rate will generally be regarded by the Federal enforcement agencies as evidence of [discrimination].\u201d - Equal Employment Opportunity Commission Certain definitions of fairness, and thus certain fairness metrics, are demonstrably inappropriate in particular circumstances. For example, if there is a legitimate reason for a difference in the rate of positive labels between members of different protected classes (e.g. incidence of breast cancer by gender) then statistical parity between model results for the protected classes would be an in- appropriate measure A classifier h satisfies demographic parity under a distribution over (X, A, Y ) if its prediction \\(h(X)\\) is statistically independent of the protected attribute A\u2014that is, if \\(P[h(X) = \\hat{y} | A=a]=P[h(X)=\\hat{y}]\\ \\forall a,\\hat{y}\\) . Because \\(\\hat{y}\u2208{0,1}\\) , this is equivalent to \\(E[h(X) | A = a] = E[h(X)]\\) for all a. Equal selection rates across different groups P[y_pred = 1|s 1 ] = P[y_pred = 1|s 2 ] Equality of the prediction accuracy across groups E[l(y_pred, y)|s 1 ] = E[l(y_pred, y)|s 2 ] 4. Envy Free Fairness In an envy free assignment every individual does not want to receive the anothers' assignment. E.g cake cutting where the slices have equal size. The concept of envy free fairness is closesly linked to pareto optimality . 5. Disparate Mistreatment Classifier might make decisions for people belonging to different social groups with different misclassification rates. When the ground truth for historical decisions is available, disproportionately beneficial outcomes for certain sensitive attribute value groups can be justified and explained by means of the ground truth. Therefore, disparate impact would not be a suitable notion of unfairness in such scenarios. Disparate Mistreatment arises when the false positive rates (FPR) and false negative rates (FNR) are substantially different between groups. Avoiding Disparate Mistreatment requires to establish Group Error Parity For this the prediction error (FPR, FNR) should be similar across groups and therefore independant of protected feature E s=0 [f(x,s), y] = E s=1 [f(x,s), y] 6. Equalized Odds Demographic parity rules out perfect classifiers whenever Y is correlated with A. A classifier h satisfies equalized odds under a distribution over \\((X,A,Y)\\) if its prediction \\(h(X)\\) is conditionally independent of the protected attribute A given the label Y \u2014that is, if \\(P[h(X)=\\hat{y}|A=a,Y =y] = P[h(X)=\\hat{y}|Y=y] \\forall a, y, and\\ \\hat{y}\\) . Because \\(\\hat{y} \u2208 {0, 1}\\) , this is equivalent to \\(E[h(X)|A=a,Y =y]=E[h(X)|Y=y]\\ \\forall a,y\\) . Fairness Mechanisms Fairness constraints and mechanisms have to be weighted against a loss in accuracy. Pre-processing Pre-processing the data. In-processing \\(min_{h' \\in H} L(h', D)\\) \\(s.t\\ F(h', D)\\) Choose the model h' that minimizes the prediction loss while respecting the fairness constraints. Post-processing Post-processing the classifier\u2019s predictions. Process Let the user select the variables that he considers to be protected from the training dataset. Some metrics need to be scored by the user. For example the proper class balance and for example the question if the problem or question the model is trying to solve or predict is very hard to be assessed automatically. Therefore the user scores that fairness on a level from [0,1] or [0,100]. Compute metrics based on the protected attributes for example group fairness, disparate treatment score, disparate impact score etc. Based on the taxonomy an overall fairness score can then be created. Taxonomy Stage Metric Target Description Unit Weight 1. Business Understanding Question Fairness Problem Context Is the question we are trying to answer fair in itself? It would be considered fair to recommend a preferred treatment to a patient, but the application of machine learning for racial profiling would be considered unfair. [0,1] 1/n Context Criticality Problem Context How important is fairness in the context the model operates in? In a legal context fairness is very important while it is less important for marketing purposes. [0,1] 1/n 2. Data Mining Biased Data Dataset Does the data possibly contain a bias which was introduced during the data collection? (e.g selection bias) {0,1} 1/n 3. Data Cleaning Class Balance/ Imbalance Dataset To what degree does the sample (training dataset) represent the expected class distribution of the real underlying population? [0,1] 1/n 4. Data Exploration - - - - - 5. Feature Engineering - - - - - 6. Predictive Modeling Equally Distributed Precision Model What fraction of predictions as a positive class were actually positive. Precision: TP/(TP+FP) [0,1] 1/n Equally Distributed Recall Model What fraction of all positive samples were correctly predicted as positive by the classifier. Recall: TP/(TP+FN). [0,1] 1/n Equally Distributed Specificity Model What fraction of all negative samples are correctly predicted as negative by the classifier. Specificity: TN/(TN+FP). [0,1] 1/n Equally Distributed F1 Score Model F1 = 2*(precision * recall)/(precision + recall) [0,1] 1/n Disparate Treatment Model Depending on the context certain features (gender, religion, race) are considered to be protected. Is at least one protected feature used during the training process for the model's prediction? [0,1] 1/n Disparate Impact Model A practice that intentionally disadvantages/discriminates a group based on a protected feature (e.g the pay difference between men and women at the same position, ). The treatment or process should not depend on a sensitive feature encoding group membership. [0,1] 1/n Disparate Mistreatment Model Check if the prediction error (FPR, FNR) is similar across groups and therefore independant of protected features E s=0 [f(x,s), y] = E s=1 [f(x,s), y] [0,1] 1/n Statistical Parity Model Statistical parity, ensures that the overall proportion of members in a protected group receiving positive (negative) classification are identical to the proportion of the population as a whole. [0,1] 1/n 7. Data Visualization - - - - Sources Papers Year Author Title Context Content Link 2012 Martin Glaser \"Racial profiling is a discriminatory practice that undermines fundamental civil rights while failing to promote law enforcement goals\" Racial Profiling Racial profiling is of unjust nature - 2015 Amit Datta \"Automated Experiments on Ad Privacy Settings\" Advertising - Setting the gender to famel results in getting fewer instances of an ad related to high paying jobs than setting it to male. - The amoral status of an algorithm does not negate its effects on society. - Manifold interactions can lead to discrimination, whereby not one party can be blamed solely. - 2016 John Kleinberg \"Inherent Trade-Offs in the Fair Determination of Risk Scores\" Law Healthcare - Three fairness notions are developed and it is shown that under non trivial conditions it is impossible to satisfy all three simultaneously. - 2016 Muhammad Zafar \"Fairness Beyond Disparate Treatment & Disparate Impact: Learning Classification without Disparate Mistreatment\" - 2019 Ziad Obermeyer Dissecting racial bias in an algorithm used to manage the health of populations Healthcare - Show that a widely used algorithm, typical of this industry-wide approach and affecting millions of patients, exhibits significant racial bias. - At a given risk score, Black patients are considerably sicker than White patients, as evidenced by signs of uncontrolled illnesses. - 2018 Alekh Agarwal A Reductions Approach to Fair Classification Fairness Mechanisms Reduce fair classification to a sequence of cost-sensitive classification problems. [9] Websites [1]- Sam Levin - A beauty contest was judged by AI and the robots didn't like dark skin (2019) [2] - Robert Hart - If you\u2019re not a white male, artificial intelligence\u2019s use in healthcare could be dangerous (2017) [3] - Samuel Gibbs - Women less likely to be shown ads for high-paid jobs on Google, study shows (2015) [4] - Neal Ungeleider - Colleges Are Using Big Data To Predict Which Students Will Do Well\u2013Before They Accept Them [5] - Julia Angwin - There\u2019s software used across the country to predict future criminals. And it\u2019s biased against blacks. [6] - Randy Rieland - Artificial Intelligence Is Now Used to Predict Crime. But Is It Biased? [7] - Jeffrey Dastin - Amazon scraps secret AI recruiting tool that showed bias against women [8] - Batya Friedman - Bias in Computer Systems","title":"Fairness"},{"location":"Pillars/Fairness/#fairness","text":"","title":"Fairness"},{"location":"Pillars/Fairness/#introduction","text":"An increasing number of information systems take decisions based on statistical inference rules, acquired through machine learning techniques. Especially in decision-critical contexts such as predictive policing, lending decisions in credit scoring or triage and allocation of health care resources discrimination is to be avoided. In order to avoid encoding discrimination in automated decisions, multiple fairness aspects needs to be accounted for.","title":"Introduction"},{"location":"Pillars/Fairness/#legally-recognized-protected-classes","text":"Depending on the context the stakeholders need to decide during the model creation process which systemic difference between groups should be available to the model and which should be excluded. These sensitive features should either not be used or very carefully, since they are protected by law. Race (Civil Rights Act of 1964) Color (Civil Rights Act of 1964) Sex (Equal Pay Act of 1963; Civil Rights Act of 1964) Religion (Civil Rights Act of 1964) National Origin (Civil Rights Act of 1964) Citizenship (Immigration Reform and Control Act) Age (Age Discrimination in Employment Act of 1967) Pregnancy (Pregnancy Discrimination Act) Family Status (Civil Rights Act of 1968) Disability Status (Rehabilitation Act of 1973; Americans with Disabilities Act of 1990) Veteran Status (Vietnam Era Veterans Readjustment Assistance Act of 1974) Genetic Information (Genetic Information Nondiscrimination Act) Sexual orientation (in some jurisdications)","title":"Legally recognized 'protected classes'"},{"location":"Pillars/Fairness/#application-examples","text":"Medical Testing and Diagnosis Making decisions about a patient\u2019s treatment may rely on tests providing probability estimates for different diseases and conditions. Here too we can ask whether such decision-making is being applied uniformly across different groups of patients. Hiring Lending School Admission Criminal Justice","title":"Application Examples"},{"location":"Pillars/Fairness/#counterexamples","text":"Examples of Algorithmic Unfairness COMPAS risk tool Assess a defendant\u2019s probability of recidivism. Want to keep Tool\u2019s errors were asymmetric: African-American defendants were more likely to be incorrectly labeled as higher-risk than they actually were, while white defendants were more likely to be incorrectly labeled as lower-risk than they actually were. White defendants who went on to commit future crimes were assigned risk scores corresponding to lower probability estimates in aggregate [5]. Apple Credit Card Apple Card's Credit Scoring Algorithm was investigated after discriminating against women. Beauty AI Contest The first international beauty contest decided by an algorithm has sparked controversy after the results revealed that the algorithm discriminated against people of color [1]. Google Advertising Discrimination A team of researchers form Carnegie Mellon university build a tool called AdFisher to automatically test the Google Adwords system. They found out that women are much less likely to be shown ads for high paid positions [3]. University Admission Universities are starting to use machine learning tools in order to predict which students are going to perform well. This can potentially lead to a discrimination based on the students family background [4]. Predictive Policing Violent crime is like a communicable disease, that it tends to break out in geographic clusters. PredPol identifies areas in a neighborhood where serious crimes are more likely to occur during a particular period. The company claims its research has found the software to be twice as accurate as human analysts when it comes to predicting where crimes will happen [6]. Amazon's AI Recruiting Engine Amazons recruiting engine discriminated against women. Since most software developers at Amazon are male the system thought they are prefered over women.","title":"Counterexamples"},{"location":"Pillars/Fairness/#sources-of-bias","text":"The term bias to refer to computer systems that systematically and unfairly discriminate against certain individuals or groups of individuals in favor of others. A system discriminates unfairly if it denies an opportunity or a good or if it assigns an undesirable outcome to an individual or group of individuals on grounds that are unreasonable or inappropriate. Consider, for example, an automated credit advisor that assists in the decision of whether or not to extend credit to a particular applicant. If the advisor denies credit to individuals with consistently poor payment records we do not judge the system to be biased because it is reasonable and appropriate for a credit company to want to avoid extending credit privileges to people who consistently do not pay their bills. In contrast, a credit advisor that systematically assigns poor credit ratings to individuals with ethnic surnames discriminates on grounds that are not relevant to credit assessments and, hence, discriminates unfairly. Bias is systematic discrimination with an unfair outcome [8]. Three categories of bias in computer systems Preexisting Has its roots in social institutions, practices, and attitudes. Can also reflect the personal biases of individuals who have significant input into the design of the system. Can enter a system either through the explicit and conscious efforts of individuals or institutions, or implicitly and unconsciously, even in spite of the best of intentions. Technical Arises from technical constraints or considerations. E.g bias that originates from the use of an algorithm that fails to treat all groups fairly. Emergent Arises in a context of use. Typically emerges some time after a design is completed, as a result of changing societal knowledge, population, or cultural values If the system is complex, and most are, biases can remain hidden in the code. The distinction between a rationally based discrimination and bias is always easy to draw Common types of bias in machine learning Automation bias When a human decision maker favors recommendations made by an automated decision-making system over information made without automation, even when the automated decision-making system makes errors. Confirmation bias The tendency to search for, interpret, favor, and recall information in a way that confirms one's preexisting beliefs or hypotheses. Machine learning developers may inadvertently collect or label data in ways that influence an outcome supporting their existing beliefs Experimenter\u2019s bias Is a form of confirmation bias in which an experimenter continues training models until a preexisting hypothesis is confirmed. Group attribution bias Assuming that what is true for an individual is also true for everyone in that group. Implicit bias Automatically making an association or assumption based on one\u2019s mental models and memories In-group bias Showing partiality to one's own group or own characteristics. If testers or raters consist of the machine learning developer's friends, family, or colleagues, then in-group bias may invalidate product testing or the dataset Out-group homogeneity bias Showing partiality to one's own group or own characteristics. If testers or raters consist of the machine learning developer's friends, family, or colleagues, then in-group bias may invalidate product testing or the dataset. Coverage bias The population represented in the dataset does not match the population that the machine learning model is making predictions about. Non-response bias Also called participation bias. Users from certain groups opt-out of surveys at different rates than users from other groups. Reporting bias The fact that the frequency with which people write about actions, outcomes, or properties is not a reflection of their real-world frequencies or the degree to which a property is characteristic of a class of individuals. Reporting bias can influence the composition of data that machine learning systems learn from. For example, in books, the word laughed is more prevalent than breathed. A machine learning model that estimates the relative frequency of laughing and breathing from a book corpus would probably determine that laughing is more common than breathing. Sampling bias Data is not collected randomly from the target group. Selection bias Errors in conclusions drawn from sampled data due to a selection process that generates systematic differences between samples observed in the data and those not observed.","title":"Sources of Bias"},{"location":"Pillars/Fairness/#sources-of-unfairness","text":"Data Unfairness ... from Task Definition and Data Collection. Algorithmic Unfairness ... from Model Specification and Model Training. Impact Unfairness ... from Model Testing and Deployment in Real-World.","title":"Sources of Unfairness"},{"location":"Pillars/Fairness/#algorithmic-unfairness","text":"f \u2217 ,the underlying model (y i =f \u2217 (x i )+\u03b5 i ). h \u2217 \u2208 H, the best available hypothesis. h=argmin h\u2032\u2208H L(D,h\u2032), the best model on finite sample. For the sake of concreteness, s \u2208 {0, 1} is assumed. Unfairness = E[(h(x)\u2212y) 2 |s=0] \u2212 E[(h(x)\u2212y) 2 |s=1]","title":"Algorithmic Unfairness"},{"location":"Pillars/Fairness/#fairness-metrics","text":"","title":"Fairness Metrics"},{"location":"Pillars/Fairness/#within-the-ml-pipeline","text":"","title":"within the ML Pipeline"},{"location":"Pillars/Fairness/#1-business-understanding","text":"Ask relevant questions and define objectives for the problem that needs to be tackled. Counter Examples Inferring criminality using face images is unethical. Identifying gay faces is an unenthical use case.","title":"1. Business Understanding"},{"location":"Pillars/Fairness/#2-data-mining","text":"Gather and scrape the data necessary for the project. Mismatch between training and deployment populations (e.g Different population or drifting populations) Under/over-representation Less data from the minority (e.g., accents in speech recognition) Feedback loops (e.g observe if \u201cpaid back the loan\u201d only if loan granted)","title":"2. Data Mining"},{"location":"Pillars/Fairness/#3-data-cleaning","text":"Fix the inconsistencies within the data and handle the missing values. Class imbalance: The number of observations for each class deviates substantially from the proportions in the underlying population. Over- and Undersampling Unfairness from Underrepresentation","title":"3. Data Cleaning"},{"location":"Pillars/Fairness/#4-data-exploration","text":"Form hypotheses about your defined problem by visually analyzing the data.","title":"4. Data Exploration"},{"location":"Pillars/Fairness/#5-feature-engineering","text":"Select important features and construct more meaningful ones using the raw data that you have.","title":"5. Feature Engineering"},{"location":"Pillars/Fairness/#6-predictive-modeling","text":"Train machine learning models, evaluate their performance, and use them to make predictions.","title":"6. Predictive Modeling"},{"location":"Pillars/Fairness/#7-data-visualization","text":"Communicate the findings with key stakeholders using plots and interactive visualizations.","title":"7. Data Visualization"},{"location":"Pillars/Fairness/#8-deployment","text":"Unintended use or adversarial feedback (e.g Tay.ai) Gaming the system. Users can try to use the rules and procedures meant to protect the system, in order to instead, manipulate the system.","title":"8. Deployment"},{"location":"Pillars/Fairness/#statistical-measures-of-fairness","text":"The closer these values are to one the better the classifier. Positive Predicted Value (PPV) How many of the samples that were classified as pos samples are actually pos samples. Also called precision. PPV = P(actual=+|prediction=+) = TP / (TP + FP) False Discovery Rate (FDR) The fractions of samples that got classified as pos samples but however actually are negative samples. FDR = P(actual=-|prediction=+) = FP / (TP + FP) Negative Predicted Value (NPV) NPV = P(actual=-|prediction=+) = TN / (TN + FN) False Omission Rate (FOR) FOR = P(actual=+|prediction=-) = FN / (TN + FN) True Positive Rate (TPR) Number of positive samples that got successfully identified as positives. Also called sensitivity or recall. TPR = P(prediction=+|actual=+) = TP / (TP + FN) False Negative Rate (FNR) Number of positive samples that accidentally got identified as negatives. FNR = P(prediction=-|actual=+) = FN / (TP + FN) True Negative Rate (TNR) TNR = P(prediction=-|actual=-) = TN / (TN + FP) False Positive Rate (FPR) FPR = P(prediction=+|actual=-) = FP / (TN + FP)","title":"Statistical Measures Of Fairness"},{"location":"Pillars/Fairness/#overview","text":"(1) and (2) are widely used ideas inspired by anti-discrimnation legislation. There are group notions of fairness (e.g statistical parity, equality of accuracy, equality of false positive/false negative rates ) and individual notions (e.g treating similar individuals similar ). There is no universally accepted definition of fairness. The existing notions consider how errors are distributed across different groups.","title":"Overview"},{"location":"Pillars/Fairness/#biased-data","text":"During the data collection bias can be introduced. Selection bias for example could result in the data not being representative of the underlying population. Or depending on how certain questions are framed the results of a survey could also be different. However it is really difficult to automatically detect bias in data since you need to compare the data to something in order to identify deviations. Sample Bias: When the sample process generating the data is not uniform across protected groups. Label Bias: We define \u2018label bias\u2019 as the case when there is a causal link between a protected attribute and the class label assigned to an individual which is not warranted by ground truth. Consider a dataset composed of elementary school students, with a dependent variable that indicates whether the student misbehaves. Studies have shown that Black and Latinx children are more likely than White children to receive suspensions or expulsions for similar problem behavior, so if our dataset\u2019s dependent variable were based on suspensions it would contain label bias.","title":"Biased Data"},{"location":"Pillars/Fairness/#simpsons-paradox","text":"Cases where different levels of data aggregation produce different fairness conclusions. An analysis of graduate admissions data from Berkeley offers one such case [1], in which the aggregate data show an admissions bias against women, but when the data are disaggregated to the department level the bias is reversed. This difficulty stems from the causal influence of the protected class (in this case gender) on the presence of an individual in the set of applicants to each department: women preferentially apply to departments with lower acceptance rates.","title":"Simpson\u2019s paradox"},{"location":"Pillars/Fairness/#class-imbalance","text":"Imagine a dataset with 99% samples with label one and 1% samples with label zero. A classifier returning always one as the predicted label would still receive an accuracy of 99%.","title":"Class Imbalance"},{"location":"Pillars/Fairness/#individual-fairness","text":"Treating people as individuals, regardless of their group membership. Similarity is defined with respect to the task at hand. Individual notions, while intuitively appealing, are hard to formalize (what is the right notion of benefit or distance?) and are computationally challenging. Formalizing Individual Fairness d(x i , x j ): a metric defining distance between two individuals. D: a measure of distance between distributions. A randomized classifier h : X \u2192 \u2206(Y) satisfies the (D, d)-Lipschitz property if \u2200x i , x j : D(\u2206 h (x i ), \u2206 h (x j )) \u2264 L d(x i , x j ) for a real number L > 0. This means that the differences in prediction between individuals need to be based on a well reasoned distance in the feature space with regard to the application context. How should we pick d, D and L? Does not treat dissimilar individuals differently. Unfairness as inequality of received benefit Define the benefit obtained by each individual. \\(\\forall\\ individual\\ i: b_i = \\hat{y}_i - y_i + 1\\) Plug the different benefits into the inequality equation e.g Gini index. \\(G(b) = \\frac{\\Sigma_{i=1}^{n}\\Sigma_{j=1}^{n}|b_i-b_j|}{2n \\Sigma_{i=1}^{n}b_i}\\)","title":"Individual Fairness"},{"location":"Pillars/Fairness/#group-fairness","text":"","title":"Group Fairness"},{"location":"Pillars/Fairness/#1-disparate-treatment","text":"A practice that intentionally disadvantages/discriminates a group based on a protected feature ( e.g the pay difference between men and women at the same position, ). The treatment or process should not depend on a sensitive feature encoding group membership. How to check for Disparate Treatment Depending on the context certain attributes are considered to be protected. For hiring decisions in Germany for example the exmployer is not allowed to use certain information e.g (pregnancy status, wish for a child, relationship status). The user of our tool could be presented with all attributes the model is taking into consideration. He could then select every attribute he would consider in this context. On the other hand side it would be possible to define a default list of protected attributes (gender, religion, race) and let their use negatively influence the fairness score.","title":"1. Disparate Treatment"},{"location":"Pillars/Fairness/#2-disparate-impact","text":"Is what occurs when an organization\u2019s actions, policies, or some other aspect of their processes inadvertently result in unintentional discrimination against people who are in a protected class. Even though the policy, action, or item in question would otherwise appear to be neutral. What matters is the outcome, not the intent. Equality in outcomes across groups based on protected features has to be assured. How to check for Disparate Impact","title":"2. Disparate Impact"},{"location":"Pillars/Fairness/#3-statistical-parity","text":"In some cases statistical parity is a central goal (and in some it is legally mandated). Statistical parity, ensures that the overall proportion of members in a protected group receiving positive (negative) classification are identical to the proportion of the population as a whole. \u201cA selection rate for any race, sex, or ethnic group which is less than four-fifths (or 80%) of the rate for the group with the highest rate will generally be regarded by the Federal enforcement agencies as evidence of [discrimination].\u201d - Equal Employment Opportunity Commission Certain definitions of fairness, and thus certain fairness metrics, are demonstrably inappropriate in particular circumstances. For example, if there is a legitimate reason for a difference in the rate of positive labels between members of different protected classes (e.g. incidence of breast cancer by gender) then statistical parity between model results for the protected classes would be an in- appropriate measure A classifier h satisfies demographic parity under a distribution over (X, A, Y ) if its prediction \\(h(X)\\) is statistically independent of the protected attribute A\u2014that is, if \\(P[h(X) = \\hat{y} | A=a]=P[h(X)=\\hat{y}]\\ \\forall a,\\hat{y}\\) . Because \\(\\hat{y}\u2208{0,1}\\) , this is equivalent to \\(E[h(X) | A = a] = E[h(X)]\\) for all a. Equal selection rates across different groups P[y_pred = 1|s 1 ] = P[y_pred = 1|s 2 ] Equality of the prediction accuracy across groups E[l(y_pred, y)|s 1 ] = E[l(y_pred, y)|s 2 ]","title":"3. Statistical Parity"},{"location":"Pillars/Fairness/#4-envy-free-fairness","text":"In an envy free assignment every individual does not want to receive the anothers' assignment. E.g cake cutting where the slices have equal size. The concept of envy free fairness is closesly linked to pareto optimality .","title":"4. Envy Free Fairness"},{"location":"Pillars/Fairness/#5-disparate-mistreatment","text":"Classifier might make decisions for people belonging to different social groups with different misclassification rates. When the ground truth for historical decisions is available, disproportionately beneficial outcomes for certain sensitive attribute value groups can be justified and explained by means of the ground truth. Therefore, disparate impact would not be a suitable notion of unfairness in such scenarios. Disparate Mistreatment arises when the false positive rates (FPR) and false negative rates (FNR) are substantially different between groups. Avoiding Disparate Mistreatment requires to establish Group Error Parity For this the prediction error (FPR, FNR) should be similar across groups and therefore independant of protected feature E s=0 [f(x,s), y] = E s=1 [f(x,s), y]","title":"5. Disparate Mistreatment"},{"location":"Pillars/Fairness/#6-equalized-odds","text":"Demographic parity rules out perfect classifiers whenever Y is correlated with A. A classifier h satisfies equalized odds under a distribution over \\((X,A,Y)\\) if its prediction \\(h(X)\\) is conditionally independent of the protected attribute A given the label Y \u2014that is, if \\(P[h(X)=\\hat{y}|A=a,Y =y] = P[h(X)=\\hat{y}|Y=y] \\forall a, y, and\\ \\hat{y}\\) . Because \\(\\hat{y} \u2208 {0, 1}\\) , this is equivalent to \\(E[h(X)|A=a,Y =y]=E[h(X)|Y=y]\\ \\forall a,y\\) .","title":"6. Equalized Odds"},{"location":"Pillars/Fairness/#fairness-mechanisms","text":"Fairness constraints and mechanisms have to be weighted against a loss in accuracy.","title":"Fairness Mechanisms"},{"location":"Pillars/Fairness/#pre-processing","text":"Pre-processing the data.","title":"Pre-processing"},{"location":"Pillars/Fairness/#in-processing","text":"\\(min_{h' \\in H} L(h', D)\\) \\(s.t\\ F(h', D)\\) Choose the model h' that minimizes the prediction loss while respecting the fairness constraints.","title":"In-processing"},{"location":"Pillars/Fairness/#post-processing","text":"Post-processing the classifier\u2019s predictions.","title":"Post-processing"},{"location":"Pillars/Fairness/#process","text":"Let the user select the variables that he considers to be protected from the training dataset. Some metrics need to be scored by the user. For example the proper class balance and for example the question if the problem or question the model is trying to solve or predict is very hard to be assessed automatically. Therefore the user scores that fairness on a level from [0,1] or [0,100]. Compute metrics based on the protected attributes for example group fairness, disparate treatment score, disparate impact score etc. Based on the taxonomy an overall fairness score can then be created.","title":"Process"},{"location":"Pillars/Fairness/#taxonomy","text":"Stage Metric Target Description Unit Weight 1. Business Understanding Question Fairness Problem Context Is the question we are trying to answer fair in itself? It would be considered fair to recommend a preferred treatment to a patient, but the application of machine learning for racial profiling would be considered unfair. [0,1] 1/n Context Criticality Problem Context How important is fairness in the context the model operates in? In a legal context fairness is very important while it is less important for marketing purposes. [0,1] 1/n 2. Data Mining Biased Data Dataset Does the data possibly contain a bias which was introduced during the data collection? (e.g selection bias) {0,1} 1/n 3. Data Cleaning Class Balance/ Imbalance Dataset To what degree does the sample (training dataset) represent the expected class distribution of the real underlying population? [0,1] 1/n 4. Data Exploration - - - - - 5. Feature Engineering - - - - - 6. Predictive Modeling Equally Distributed Precision Model What fraction of predictions as a positive class were actually positive. Precision: TP/(TP+FP) [0,1] 1/n Equally Distributed Recall Model What fraction of all positive samples were correctly predicted as positive by the classifier. Recall: TP/(TP+FN). [0,1] 1/n Equally Distributed Specificity Model What fraction of all negative samples are correctly predicted as negative by the classifier. Specificity: TN/(TN+FP). [0,1] 1/n Equally Distributed F1 Score Model F1 = 2*(precision * recall)/(precision + recall) [0,1] 1/n Disparate Treatment Model Depending on the context certain features (gender, religion, race) are considered to be protected. Is at least one protected feature used during the training process for the model's prediction? [0,1] 1/n Disparate Impact Model A practice that intentionally disadvantages/discriminates a group based on a protected feature (e.g the pay difference between men and women at the same position, ). The treatment or process should not depend on a sensitive feature encoding group membership. [0,1] 1/n Disparate Mistreatment Model Check if the prediction error (FPR, FNR) is similar across groups and therefore independant of protected features E s=0 [f(x,s), y] = E s=1 [f(x,s), y] [0,1] 1/n Statistical Parity Model Statistical parity, ensures that the overall proportion of members in a protected group receiving positive (negative) classification are identical to the proportion of the population as a whole. [0,1] 1/n 7. Data Visualization - - - -","title":"Taxonomy"},{"location":"Pillars/Fairness/#sources","text":"","title":"Sources"},{"location":"Pillars/Fairness/#papers","text":"Year Author Title Context Content Link 2012 Martin Glaser \"Racial profiling is a discriminatory practice that undermines fundamental civil rights while failing to promote law enforcement goals\" Racial Profiling Racial profiling is of unjust nature - 2015 Amit Datta \"Automated Experiments on Ad Privacy Settings\" Advertising - Setting the gender to famel results in getting fewer instances of an ad related to high paying jobs than setting it to male. - The amoral status of an algorithm does not negate its effects on society. - Manifold interactions can lead to discrimination, whereby not one party can be blamed solely. - 2016 John Kleinberg \"Inherent Trade-Offs in the Fair Determination of Risk Scores\" Law Healthcare - Three fairness notions are developed and it is shown that under non trivial conditions it is impossible to satisfy all three simultaneously. - 2016 Muhammad Zafar \"Fairness Beyond Disparate Treatment & Disparate Impact: Learning Classification without Disparate Mistreatment\" - 2019 Ziad Obermeyer Dissecting racial bias in an algorithm used to manage the health of populations Healthcare - Show that a widely used algorithm, typical of this industry-wide approach and affecting millions of patients, exhibits significant racial bias. - At a given risk score, Black patients are considerably sicker than White patients, as evidenced by signs of uncontrolled illnesses. - 2018 Alekh Agarwal A Reductions Approach to Fair Classification Fairness Mechanisms Reduce fair classification to a sequence of cost-sensitive classification problems. [9]","title":"Papers"},{"location":"Pillars/Fairness/#websites","text":"[1]- Sam Levin - A beauty contest was judged by AI and the robots didn't like dark skin (2019) [2] - Robert Hart - If you\u2019re not a white male, artificial intelligence\u2019s use in healthcare could be dangerous (2017) [3] - Samuel Gibbs - Women less likely to be shown ads for high-paid jobs on Google, study shows (2015) [4] - Neal Ungeleider - Colleges Are Using Big Data To Predict Which Students Will Do Well\u2013Before They Accept Them [5] - Julia Angwin - There\u2019s software used across the country to predict future criminals. And it\u2019s biased against blacks. [6] - Randy Rieland - Artificial Intelligence Is Now Used to Predict Crime. But Is It Biased? [7] - Jeffrey Dastin - Amazon scraps secret AI recruiting tool that showed bias against women [8] - Batya Friedman - Bias in Computer Systems","title":"Websites"},{"location":"RelatedResearch/","text":"Dr. Alberto Huertas Research on IT-Security The data was collected using the Linux perf tool. Id Feature Description 1. 'time' - 2. 'timestamp' Timestamp when the event took place. 3. 'seconds' - 4. 'connectivity' Does this mean the network connection was up during that period? 5. 'alarmtimer:alarmtimer_fired' An alarm was fired? 6. 'alarmtimer:alarmtimer_start' A timer was started? 7. 'block:block_bio_backmerge' Disk event. Did a bio_backmerge event happen? 8. 'block:block_bio_remap' An operation for a logical device has been mapped to a raw block device. 9. 'block:block_dirty_buffer' Software defined kernel event. Signals that the buffer has been written to. 10. 'block:block_getrq' Tracepoint event 11. 'block:block_touch_buffer' Gets triggered on touch_buffer() event 12. 'block:block_unplug' - 13. 'cachefiles:cachefiles_create' New cachefile was created 14. 'cachefiles:cachefiles_lookup' - 15. 'cachefiles:cachefiles_mark_active' - 16. 'clk:clk_set_rate' - 17. 'cpu-migrations' - 18. 'cs' - 19. 'dma_fence:dma_fence_init' - 20. 'fib:fib_table_lookup' - 21. 'filemap:mm_filemap_add_to_page_cache' - 22. 'gpio:gpio_value' - 23. 'ipi:ipi_raise' - 24. 'irq:irq_handler_entry' - 25. 'irq:softirq_entry' - 26. 'jbd2:jbd2_handle_start' - 27. 'jbd2:jbd2_start_commit' - 28. 'kmem:kfree' - 29. 'kmem:kmalloc' - 30. 'kmem:kmem_cache_alloc' - 31. 'kmem:kmem_cache_free' - 32. 'kmem:mm_page_alloc' - 33. 'kmem:mm_page_alloc_zone_locked' - 34 'kmem:mm_page_free' - 35. 'kmem:mm_page_pcpu_drain' - 36. 'mmc:mmc_request_start' - 37. 'net:net_dev_queue' - 38. 'net:net_dev_xmit' - 39. 'net:netif_rx' - 40. 'page-faults' - 41. 'pagemap:mm_lru_insertion' - 42. 'preemptirq:irq_enable' - 43. 'qdisc:qdisc_dequeue' - 44. 'qdisc:qdisc_dequeue.1' - 45. 'random:get_random_bytes' - 46. 'random:mix_pool_bytes_nolock' - 47. 'random:urandom_read' - 48. 'raw_syscalls:sys_enter' - 49. 'raw_syscalls:sys_exit' - 50. 'rpm:rpm_resume' - 51. 'rpm:rpm_suspend' - 52. 'sched:sched_process_exec' - 53. 'sched:sched_process_free' - 54. 'sched:sched_process_wait' - 55. 'sched:sched_switch' - 56. 'sched:sched_wakeup' - 57. 'signal:signal_deliver' - 58. 'signal:signal_generate' - 59. 'skb:consume_skb' - 60. 'skb:consume_skb.1' - 61. 'skb:kfree_skb' - 62. 'skb:kfree_skb.1' - 63. 'skb:skb_copy_datagram_iovec' - 64. 'sock:inet_sock_set_state' - 65. 'task:task_newtask' - 66. 'tcp:tcp_destroy_sock' - 67. 'tcp:tcp_probe' - 68. 'timer:hrtimer_start' - 69. 'timer:timer_start' - 70. 'udp:udp_fail_queue_rcv_skb' - 71. 'workqueue:workqueue_activate_work' - 72. 'writeback:global_dirty_state' - 73. 'writeback:sb_clear_inode_writeback' - 74. 'writeback:wbc_writepage' - 75. 'writeback:writeback_dirty_inode' - 76. 'writeback:writeback_dirty_inode_enqueue' - 77. 'writeback:writeback_dirty_page' - 78. 'writeback:writeback_mark_inode_dirty' - 79. 'writeback:writeback_pages_written' - 80. 'writeback:writeback_single_inode' - 81. 'writeback:writeback_write_inode' - 82. 'writeback:writeback_written' -","title":"Index"},{"location":"RelatedResearch/#dr-alberto-huertas-research-on-it-security","text":"The data was collected using the Linux perf tool. Id Feature Description 1. 'time' - 2. 'timestamp' Timestamp when the event took place. 3. 'seconds' - 4. 'connectivity' Does this mean the network connection was up during that period? 5. 'alarmtimer:alarmtimer_fired' An alarm was fired? 6. 'alarmtimer:alarmtimer_start' A timer was started? 7. 'block:block_bio_backmerge' Disk event. Did a bio_backmerge event happen? 8. 'block:block_bio_remap' An operation for a logical device has been mapped to a raw block device. 9. 'block:block_dirty_buffer' Software defined kernel event. Signals that the buffer has been written to. 10. 'block:block_getrq' Tracepoint event 11. 'block:block_touch_buffer' Gets triggered on touch_buffer() event 12. 'block:block_unplug' - 13. 'cachefiles:cachefiles_create' New cachefile was created 14. 'cachefiles:cachefiles_lookup' - 15. 'cachefiles:cachefiles_mark_active' - 16. 'clk:clk_set_rate' - 17. 'cpu-migrations' - 18. 'cs' - 19. 'dma_fence:dma_fence_init' - 20. 'fib:fib_table_lookup' - 21. 'filemap:mm_filemap_add_to_page_cache' - 22. 'gpio:gpio_value' - 23. 'ipi:ipi_raise' - 24. 'irq:irq_handler_entry' - 25. 'irq:softirq_entry' - 26. 'jbd2:jbd2_handle_start' - 27. 'jbd2:jbd2_start_commit' - 28. 'kmem:kfree' - 29. 'kmem:kmalloc' - 30. 'kmem:kmem_cache_alloc' - 31. 'kmem:kmem_cache_free' - 32. 'kmem:mm_page_alloc' - 33. 'kmem:mm_page_alloc_zone_locked' - 34 'kmem:mm_page_free' - 35. 'kmem:mm_page_pcpu_drain' - 36. 'mmc:mmc_request_start' - 37. 'net:net_dev_queue' - 38. 'net:net_dev_xmit' - 39. 'net:netif_rx' - 40. 'page-faults' - 41. 'pagemap:mm_lru_insertion' - 42. 'preemptirq:irq_enable' - 43. 'qdisc:qdisc_dequeue' - 44. 'qdisc:qdisc_dequeue.1' - 45. 'random:get_random_bytes' - 46. 'random:mix_pool_bytes_nolock' - 47. 'random:urandom_read' - 48. 'raw_syscalls:sys_enter' - 49. 'raw_syscalls:sys_exit' - 50. 'rpm:rpm_resume' - 51. 'rpm:rpm_suspend' - 52. 'sched:sched_process_exec' - 53. 'sched:sched_process_free' - 54. 'sched:sched_process_wait' - 55. 'sched:sched_switch' - 56. 'sched:sched_wakeup' - 57. 'signal:signal_deliver' - 58. 'signal:signal_generate' - 59. 'skb:consume_skb' - 60. 'skb:consume_skb.1' - 61. 'skb:kfree_skb' - 62. 'skb:kfree_skb.1' - 63. 'skb:skb_copy_datagram_iovec' - 64. 'sock:inet_sock_set_state' - 65. 'task:task_newtask' - 66. 'tcp:tcp_destroy_sock' - 67. 'tcp:tcp_probe' - 68. 'timer:hrtimer_start' - 69. 'timer:timer_start' - 70. 'udp:udp_fail_queue_rcv_skb' - 71. 'workqueue:workqueue_activate_work' - 72. 'writeback:global_dirty_state' - 73. 'writeback:sb_clear_inode_writeback' - 74. 'writeback:wbc_writepage' - 75. 'writeback:writeback_dirty_inode' - 76. 'writeback:writeback_dirty_inode_enqueue' - 77. 'writeback:writeback_dirty_page' - 78. 'writeback:writeback_mark_inode_dirty' - 79. 'writeback:writeback_pages_written' - 80. 'writeback:writeback_single_inode' - 81. 'writeback:writeback_write_inode' - 82. 'writeback:writeback_written' -","title":"Dr. Alberto Huertas Research on IT-Security"},{"location":"Tools/IBM-FactSheets/","text":"IBM AI FactSheets 360 Introduction The goal of the FactSheet project is to foster trust in AI by increasing transparency an increased understanding of how AI was created and deployed and enabling governance the ability to control how AI is created and deployed. Increased transparency provides information for AI consumers to better understand how the AI model a program component that is generated by learning patterns in training data to make predictions on new data, such as a loan application. or service an executable program, deployed behind an API, that allows it to respond to program requests from other programs or services was created. This allows a consumer of the model to determine if it is appropriate for their situation. What is a FactSheet? Is a collection of relevant information about the creation and deployment of an AI model or service. Such facts can be the purpose, criticality, performance, measured dataset and model characteristics (e.g accuracy). Every stakeholder might contribute facts about the model (data collector, model creator, model validator). The fact sheet documents the construction process of the model similar to a transcript of records providing insights about a student. Modeled after a supplier\u2019s declaration of conformity. Since they are tailored to a particular Ai service or model they might vary in content. The format can be differently for different audiences (e.g Full, tabular, slides) Contains information about how the model was created, tested, trained, deployed and evaluated. What legislation (e.g GDPR) or company rules (e.g what features can not be used) need to be accounted for. Why do we need FactSheets? Many stakeholders are involved in the model creation process (e.g In the Structure of a FactSheet The structure is customizable for varying audiences (internal, external stakeholders). Fact Sheet Examples can be found here (https://aifs360.mybluemix.net/examples) Taxonomy Stage Metric Description Unit Weight - - - - - References [1] - Matthew Arnold - \"FactSheets: Increasing Trust in AI Services through Supplier's Declarations of Conformity\"","title":"IBM FactSheets 360"},{"location":"Tools/IBM-FactSheets/#ibm-ai-factsheets-360","text":"","title":"IBM AI FactSheets 360"},{"location":"Tools/IBM-FactSheets/#introduction","text":"The goal of the FactSheet project is to foster trust in AI by increasing transparency an increased understanding of how AI was created and deployed and enabling governance the ability to control how AI is created and deployed. Increased transparency provides information for AI consumers to better understand how the AI model a program component that is generated by learning patterns in training data to make predictions on new data, such as a loan application. or service an executable program, deployed behind an API, that allows it to respond to program requests from other programs or services was created. This allows a consumer of the model to determine if it is appropriate for their situation.","title":"Introduction"},{"location":"Tools/IBM-FactSheets/#what-is-a-factsheet","text":"Is a collection of relevant information about the creation and deployment of an AI model or service. Such facts can be the purpose, criticality, performance, measured dataset and model characteristics (e.g accuracy). Every stakeholder might contribute facts about the model (data collector, model creator, model validator). The fact sheet documents the construction process of the model similar to a transcript of records providing insights about a student. Modeled after a supplier\u2019s declaration of conformity. Since they are tailored to a particular Ai service or model they might vary in content. The format can be differently for different audiences (e.g Full, tabular, slides) Contains information about how the model was created, tested, trained, deployed and evaluated. What legislation (e.g GDPR) or company rules (e.g what features can not be used) need to be accounted for.","title":"What is a FactSheet?"},{"location":"Tools/IBM-FactSheets/#why-do-we-need-factsheets","text":"Many stakeholders are involved in the model creation process (e.g In the","title":"Why do we need FactSheets?"},{"location":"Tools/IBM-FactSheets/#structure-of-a-factsheet","text":"The structure is customizable for varying audiences (internal, external stakeholders). Fact Sheet Examples can be found here (https://aifs360.mybluemix.net/examples)","title":"Structure of a FactSheet"},{"location":"Tools/IBM-FactSheets/#taxonomy","text":"Stage Metric Description Unit Weight - - - - -","title":"Taxonomy"},{"location":"Tools/IBM-FactSheets/#references","text":"[1] - Matthew Arnold - \"FactSheets: Increasing Trust in AI Services through Supplier's Declarations of Conformity\"","title":"References"}]}