{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Trusted AI Master Project This site serves as documentation and Overview of the Master Project. Introduction Artificial intelligence (AI) systems are getting more and more relevance as support to human decision-making processes. While AI holds the promise of delivering valuable insights into many application scenarios, the broad adoption of AI systems will rely on the ability to trust their decisions. According to IBM, some key aspects provide information to trust (or not) a decision made by an algorithm. In particular, a decision must: be reliable and fair, be accounted for, not cause harm, not be tampered with, be understandable, and be secure. In this context, the previous aspects have been grouped into the following pillars. \u2022 Fairness. One of the essential aspects to enable trusted AI is to avoid bias across the entire lifecycle of AI applications, as well as across different bias and data types. \u2022 Robustness. AI-based systems may be vulnerable to adversarial attacks. Attackers may poison training data by injecting samples to compromise system decisions eventually. \u2022 Explainability. In many application scenarios, decisions made by algorithms must be explainable and understandable by humans. Various stakeholders require explanations for different purposes, and explanations must be tailored to their needs. \u2022 Accountability. The quality of decisions should be evaluated in terms of accuracy, ability to satisfy users\u2019 preferences, as well as other properties related to the impact of the decision.The previous characteristics are demanded from AI systems. Yet, to achieve trust in AI, more efforts and automatic mechanisms able to calculate and communicate trust levels are required. Project Documentation The whole documentation of the project can be found on our GitHub page mkdocs The documentation is created with mkdocs, which uses simple markdown file and a .yml config file to build and deploy the documentation to GitHub. The relevant files to create the documentation are in the mkdocs folder The file mkdocs.yml is the config file and has to be adapted if the theme should change or the index changes or new markdowns added. The markdown files are in the folder docs and include the whole documentation. installation pip install --upgrade pip pip install mkdocs workflow: At the location of the mkdocs folder open the terminal and enter: mkdocs serve This will build the documentation and host it locally, where the full documentation can be seen. change the config or markdown files. Sinc you are serving the documentation locally you can see the changes in the config or markdown files in real time. You will always see the output and how the documentation will look in the end and therefore minimalize syntax errors in markdown for example. deploy the build on GitHub to the branche gh-pages. For this open another termianl at the localtion of the mkdocs folder and enter: mkdocs gh-deploy","title":"Home"},{"location":"#trusted-ai-master-project","text":"This site serves as documentation and Overview of the Master Project.","title":"Trusted AI Master Project"},{"location":"#introduction","text":"Artificial intelligence (AI) systems are getting more and more relevance as support to human decision-making processes. While AI holds the promise of delivering valuable insights into many application scenarios, the broad adoption of AI systems will rely on the ability to trust their decisions. According to IBM, some key aspects provide information to trust (or not) a decision made by an algorithm. In particular, a decision must: be reliable and fair, be accounted for, not cause harm, not be tampered with, be understandable, and be secure. In this context, the previous aspects have been grouped into the following pillars. \u2022 Fairness. One of the essential aspects to enable trusted AI is to avoid bias across the entire lifecycle of AI applications, as well as across different bias and data types. \u2022 Robustness. AI-based systems may be vulnerable to adversarial attacks. Attackers may poison training data by injecting samples to compromise system decisions eventually. \u2022 Explainability. In many application scenarios, decisions made by algorithms must be explainable and understandable by humans. Various stakeholders require explanations for different purposes, and explanations must be tailored to their needs. \u2022 Accountability. The quality of decisions should be evaluated in terms of accuracy, ability to satisfy users\u2019 preferences, as well as other properties related to the impact of the decision.The previous characteristics are demanded from AI systems. Yet, to achieve trust in AI, more efforts and automatic mechanisms able to calculate and communicate trust levels are required.","title":"Introduction"},{"location":"#project-documentation","text":"The whole documentation of the project can be found on our GitHub page","title":"Project Documentation"},{"location":"#mkdocs","text":"The documentation is created with mkdocs, which uses simple markdown file and a .yml config file to build and deploy the documentation to GitHub. The relevant files to create the documentation are in the mkdocs folder The file mkdocs.yml is the config file and has to be adapted if the theme should change or the index changes or new markdowns added. The markdown files are in the folder docs and include the whole documentation. installation pip install --upgrade pip pip install mkdocs workflow: At the location of the mkdocs folder open the terminal and enter: mkdocs serve This will build the documentation and host it locally, where the full documentation can be seen. change the config or markdown files. Sinc you are serving the documentation locally you can see the changes in the config or markdown files in real time. You will always see the output and how the documentation will look in the end and therefore minimalize syntax errors in markdown for example. deploy the build on GitHub to the branche gh-pages. For this open another termianl at the localtion of the mkdocs folder and enter: mkdocs gh-deploy","title":"mkdocs"},{"location":"AI_algo/","text":"Trusted AI Algorithm Basic Idea Taxonomy Architecture","title":"trusted AI algorithm"},{"location":"AI_algo/#trusted-ai-algorithm","text":"","title":"Trusted AI Algorithm"},{"location":"AI_algo/#basic-idea","text":"","title":"Basic Idea"},{"location":"AI_algo/#taxonomy","text":"","title":"Taxonomy"},{"location":"AI_algo/#architecture","text":"","title":"Architecture"},{"location":"code_documentation/","text":"Implementation of the trusted AI Algorithm General Architecture File Structure Build How it works Evaluation and Results","title":"Implementation"},{"location":"code_documentation/#implementation-of-the-trusted-ai-algorithm","text":"","title":"Implementation of the trusted AI Algorithm"},{"location":"code_documentation/#general-architecture","text":"","title":"General Architecture"},{"location":"code_documentation/#file-structure","text":"","title":"File Structure"},{"location":"code_documentation/#build","text":"","title":"Build"},{"location":"code_documentation/#how-it-works","text":"","title":"How it works"},{"location":"code_documentation/#evaluation-and-results","text":"","title":"Evaluation and Results"},{"location":"sources/","text":"Overview This is a collection of all material that might be interesting or relevant regarding the project. Websites, papers and videos Explainability IBM explainability papers: explainability papers AI Explainability 360: Toolkit page AI Explainability GitHub repo: AIX360 towards Robust Interpretability paper Generating Contrastive Explanations withMonotonic Attribute Functions paper FICO community YouTube Videos Add here a list of usefull and interesing YT videos Decison making: Can we trust artificial intelligence to make decisions for us? Websites IBM trusted AI main page: AI Research","title":"sources"},{"location":"sources/#overview","text":"This is a collection of all material that might be interesting or relevant regarding the project.","title":"Overview"},{"location":"sources/#websites-papers-and-videos","text":"","title":"Websites, papers and videos"},{"location":"sources/#explainability","text":"IBM explainability papers: explainability papers AI Explainability 360: Toolkit page AI Explainability GitHub repo: AIX360 towards Robust Interpretability paper Generating Contrastive Explanations withMonotonic Attribute Functions paper FICO community","title":"Explainability"},{"location":"sources/#youtube-videos","text":"Add here a list of usefull and interesing YT videos Decison making: Can we trust artificial intelligence to make decisions for us?","title":"YouTube Videos"},{"location":"sources/#websites","text":"IBM trusted AI main page: AI Research","title":"Websites"},{"location":"Pillars/Accountability/","text":"Accountability and Transparency Summary Summary papers Arnold et al. 2019: Arnold, M.; Bellamy, R. K. E.; Hind, M.; Houde, S.; Mehta, S.; Mojsilovi \u0301c, A.; Nair, R.; Natesan- Ramamurthy, K.; Olteanu, A.; Piorkowski, D.; Reimer, D.; Richards, J.; Tsay, J.; and Varshney, K. R. 2019. FactSheets: Increasing trust in AI services through suppliers declarations of conformity. IBM Journal of Research & Development 63(4/5). A FactSheet, as proposed by (Arnold et al. 2019), is a col- lection of relevant information about an AI model or ser- vice that is created during the machine learning life cycle. It includes information from the business owner (e.g., in- tended use and business justification), from the data gather- ing/feature selection/data cleaning phase (e.g., data sets, fea- tures used or created, cleaning operations), from the model training phase (e.g., bias, robustness, and explainability in- formation), and from the model validation and deployment phase (e.g., key performance indicators). A FactSheet is as- sociated with a model (or service) and is meant to be write once, i.e., an update to a model would trigger a new Fact- Sheet for the updated model. FactSheets can be consumed by any role in the ML life cycle to confirm process gov- ernance adherence or model performance, or by the ulti- mate users of a model to provide increased transparency. Links Taxonomy and Metrics","title":"Accountability"},{"location":"Pillars/Accountability/#accountability-and-transparency","text":"","title":"Accountability and Transparency"},{"location":"Pillars/Accountability/#summary","text":"","title":"Summary"},{"location":"Pillars/Accountability/#summary-papers","text":"Arnold et al. 2019: Arnold, M.; Bellamy, R. K. E.; Hind, M.; Houde, S.; Mehta, S.; Mojsilovi \u0301c, A.; Nair, R.; Natesan- Ramamurthy, K.; Olteanu, A.; Piorkowski, D.; Reimer, D.; Richards, J.; Tsay, J.; and Varshney, K. R. 2019. FactSheets: Increasing trust in AI services through suppliers declarations of conformity. IBM Journal of Research & Development 63(4/5). A FactSheet, as proposed by (Arnold et al. 2019), is a col- lection of relevant information about an AI model or ser- vice that is created during the machine learning life cycle. It includes information from the business owner (e.g., in- tended use and business justification), from the data gather- ing/feature selection/data cleaning phase (e.g., data sets, fea- tures used or created, cleaning operations), from the model training phase (e.g., bias, robustness, and explainability in- formation), and from the model validation and deployment phase (e.g., key performance indicators). A FactSheet is as- sociated with a model (or service) and is meant to be write once, i.e., an update to a model would trigger a new Fact- Sheet for the updated model. FactSheets can be consumed by any role in the ML life cycle to confirm process gov- ernance adherence or model performance, or by the ulti- mate users of a model to provide increased transparency.","title":"Summary papers"},{"location":"Pillars/Accountability/#links","text":"","title":"Links"},{"location":"Pillars/Accountability/#taxonomy-and-metrics","text":"","title":"Taxonomy and Metrics"},{"location":"Pillars/Explainability/","text":"Explainability Explainability The level to which a system can provideclarification for the cause of its decisions/outputs. Interpretability has become a well-recognized goal for machine learning models. The need for interpretable models is certain to increase as machine learning pushes further into domains such as medicine, criminal justice, and business, where such models complement human decision-makers and decisions can have major consequences on human lives. Transparency is thus required for domain experts to understand, critique, and trust models, and reasoning is required to explain individual decisions. Sanjeeb et. Al Introduction \"In many applications, trust in an AI system will come from its ability to \u2018explain itself.\u2019 Yet, when it comes to understanding and explaining the inner workings of an algorithm, one size does not fit all.Different stakeholders require explanations for different purposes and objectives, and explanations must be tailored to their needs. A physician might respond best to seeing examples of patient data similar to their patient\u2019s. On the other hand, a developer training a neural net will benefit from seeing how information flows through the algorithm. While a regulator will aim to understand the system as a whole and probe into its logic, consumers affected by a specific decision will be interested only in factors impacting their case \u2013 for example, in a loan processing application, they will expect an explanation for why the request was denied and want to understand what changes could lead to approval. IBM Research is creating diverse explanations, including training highly optimized directly interpretable models, creating contrastive explanations of black box models, using information flow in a high-performing complex model to train simpler, interpretable classifiers, learning disentangled representations, and visualizing information flows in neural networks.\" IBM Key Takaways, Questions and Limitations Key Takaways There is a tradeoff between explainability and accuracy (complex models with many features tend to be more accurate but at the same time less transparent) The level of explainability is mainly dominated by its model type and the number of features it uses. The importance of model explainability is highly dependent on the context of the model and its consequences. (Revenue, Rate, Rigour, Regulation, Reputation and Risk) Almost every model can be implemented in a way that makes it explainable and no models are explainable by themself but need an additional layer for the translation of the results to make it explainable for endusers. Scenarios where explainability is important: Medical diagnosis, loan lending or general assisance in human desicion making. All scenarios are mainly about classification models . Limitations The scope of explainability is not only defined by the model and data selection / pre processing but often has to be embedded in the application as a whole (Build an explianable AI system with an explanation userinterface for example) Explainability needs to be accessible. Even models which would be interpretable need to have a method the extract these information and make the explantaion explicite. Open Qustions How to treat techniques (from outside: model inducion, from inside: deep explanation) which enhance explainability in models which would be less explainable by nature? What should our score on explainability tell? How easily the model could be interpreted and an XAI system could be build with it? Summaries of Paper & Website See all papers related to explainability from IBM here: publications Explainable AI The paper is business oriented and talks about why explainability is an advatage to have embedded in AI algorithms and in which usecases explainability should have a high priority. (p.8) The need for Explainability: Working as intended?, How sensitive is the impact?, Are you comfortable with the level of control? (p.9) factors to consider: Revenue, Rate, Rigour, Regulation, Reputation and Risk (p.12) Explainable by design: Explainability needs to be considered up front and embedded into the design of the AI application. It affects the choice ofmachine learning algorithm and mayimpact the way you choose to pre-process data. (p.13) Trade-offs in explainability: Interpretability is a characteristic of a model that is generally considered to come at a cost. As a rule of thumb, the more complex the model, the more accurate it is, but the less interpretable it is. (Decision tree vs. DNN with many layers and features) (p.14) Differnt explaination techniques: Sensitivity analysis,Local Interpretable Model Explanations (LIME), Shapley Additive Explanations (SHAP), Tree interpreters, Neural Network Interpreters (Appendix 2) Subjective scale of explainability of different classes of algorithms and learning techniques (with 1 being the most difficult and 5 being the easiest to explain) see a compriesed version of the table belwo: Learning technique Scale of explainability (1-5) Bayesian belief networks (BNNs) 3.5 Decision trees 4 Logistic regression 3 Support vector machines (SVMs) 2 K-means clustering 3 Neural networks 1 Random forest/boosting 3 Q-learning 2 Hidden Markov models 3 Explainable Artificial Intelligence (XAI) XAI is a project from Defense Advanced Research Projects Agency which aims to produce more explainable models, while maintaining a high level of learning performance (prediction accuracy)and enable human users to understand, appropriately trust, and effectively manage the emerging generation of artificially intelligent partners see their concept here: DARPA slide deck on explainability (two key slides below) See another slide deck on XAI with more examples here: second XAI slide deck Improving transparency of models Context of AI applications were transparency and explainability is key: financial risk assessment (Goyal 2018), medical diagnosis and treatment planning (Strickland 2019), hiring and promotion decisions (Alsever 2017), social services eligibility determination (Fishman, Eggers, and Kishnani 2019), predictive policing (Ensign et al. 2017), and probation and sentencing recom- mendations (Larson et al. 2016). Recent work has outlined the need for increased transparency in AI for data sets(Gebru et al. 2018; Bender and Friedman 2018; Holland et al. 2018), models (Mitchell et al. 2019),and services (Arnold et al. 2019). Metrics for Explainable AI (XAI) Note Explaibale AI is much more than just having an easily interpretable model but about a whole system that is able to explain itself and interact with the user to create a full unterstanding tailored to the user needs. The papers elaborates metrics for the goodness of explanations, whether users are satisfied by explanations, how well users understand the AI systems, how curiosity motivates the search for explanations, whether the user's trust and reliance on the AI are appropriate. Interpretability to whome? A machinelearning system\u2019s interpretability should be de-fined in relation to a specific agent or task: we should not ask if the system is interpretable, but to whom is it interpretable Explainability in image classification Saliency maps are a popular approach to creating post-hoc explanations of image classifier outputs. These methods produce estimates of the relevance of each pixel to the classification output score, which can be displayed as a saliency map that highlights important pixels. Towards A Rigorous Science of Interpretable Machine Learning The need for interpretability stems from an incompleteness in the problem formalization. Scensario where this can be the case: Scientific Understanding: The human\u2019s goal is to gain knowledge. We do not have a completeway of stating what knowledge is; thus the best we can do is ask for explanations we canconvert into knowledge. Safety: For complex tasks, the end-to-end system is almost never completely testable; onecannot create a complete list of scenarios in which the system may fail. Enumerating allpossible outputs given all possible inputs be computationally or logistically infeasible, and wemay be unable to flag all undesirable outputs. Ethics: The human may want to guard against certain kinds of discrimination, and theirnotion of fairness may be too abstract to be completely encoded into the system. Even if we can encode protections for specific protected classes into the system, there might be biases that we did not consider a priori dimensions of interpretability: Global vs. Local: Global interpretability implies knowing what patterns are present in general, while local interpretability implies knowing the reasons for a specific decision (such as why a particular loan application was rejected).The former may be important for when scientific understanding or bias detection is the goal;the latter when one needs a justification for a specific decision Area, Severity of Incompleteness: What part of the problem formulation is incomplete, andhow incomplete is it? ime Constraints: How long can the user afford to spend to understand the explanation? Adecision that needs to be made at the bedside or during the operation of a plant must beunderstood quickly, while in scientific or anti-discrimination applications, the end-user maybe willing to spend hours trying to fully understand an explanation. User Expertise: How experienced is the user in the task? Comprehensible Classification Models (p.2-5) Decision trees, classification rules, decision tables, nearest neighbors, and Bayesian network classifiers, with respect to their interpretability (p.6) The Drawbacks of Model Size as the Single Measure of Comprehensibility. Big decision trees with easily understandable features can be more comprehensible than small trees with less intuitive features. Models that are too small can also be to simple for users to accept and not lead to less comprehensibility and general trust in the model. (p.7) Monotonicity Constraints in Classification Models. Users are more like to trust and accept classification models when they are built by respecting monotonicity constraints in the application domain. A monotonic relationship between a feature and the class attribute occurs when increasing the value of the feature tends to either monotonically increase or monotonically decrease the probability of an instance\u2019s membership to a class Taxonomy Construct a set of measurable metrict which can be used to calculate a score that should indicate how good the explainability of a model is. For The calculation of the score differnt weights can be assigned to the metrics. metrics where 5 is the best and 1 the worst: Model type: Different between ML model types and assigne a score to each depending on how easily the result is to explain. Monotonicity: A score from 1 to 5 for indicating how many monotone attributes there are, the higher the fraction the better. Relevance: A score from 1 to 5 how relevant the attributes are. Ideally there are no irrelevant attributes (the definition of relevance depends on the model type) Model size: A score from 1 to 5 where bigger more complex models with many attributes have (new metric: uncorrelated dataset?) table { width:100%; } metric description unit weight model type some models like linear regression or decision tree are very explainable and models like neural networks less [1,5] 0.5 faithfulness relevance are the features truly relevant or can some be omitted? [1,5] 0.2 monotonicity monotonic attribute functions most important feature for classification [1,5] 0.2 model size more features add to the complexity and may decrease comprehensibility [1,5] 0.1","title":"Explainability"},{"location":"Pillars/Explainability/#explainability","text":"Explainability The level to which a system can provideclarification for the cause of its decisions/outputs. Interpretability has become a well-recognized goal for machine learning models. The need for interpretable models is certain to increase as machine learning pushes further into domains such as medicine, criminal justice, and business, where such models complement human decision-makers and decisions can have major consequences on human lives. Transparency is thus required for domain experts to understand, critique, and trust models, and reasoning is required to explain individual decisions. Sanjeeb et. Al","title":"Explainability"},{"location":"Pillars/Explainability/#introduction","text":"\"In many applications, trust in an AI system will come from its ability to \u2018explain itself.\u2019 Yet, when it comes to understanding and explaining the inner workings of an algorithm, one size does not fit all.Different stakeholders require explanations for different purposes and objectives, and explanations must be tailored to their needs. A physician might respond best to seeing examples of patient data similar to their patient\u2019s. On the other hand, a developer training a neural net will benefit from seeing how information flows through the algorithm. While a regulator will aim to understand the system as a whole and probe into its logic, consumers affected by a specific decision will be interested only in factors impacting their case \u2013 for example, in a loan processing application, they will expect an explanation for why the request was denied and want to understand what changes could lead to approval. IBM Research is creating diverse explanations, including training highly optimized directly interpretable models, creating contrastive explanations of black box models, using information flow in a high-performing complex model to train simpler, interpretable classifiers, learning disentangled representations, and visualizing information flows in neural networks.\" IBM","title":"Introduction"},{"location":"Pillars/Explainability/#key-takaways-questions-and-limitations","text":"Key Takaways There is a tradeoff between explainability and accuracy (complex models with many features tend to be more accurate but at the same time less transparent) The level of explainability is mainly dominated by its model type and the number of features it uses. The importance of model explainability is highly dependent on the context of the model and its consequences. (Revenue, Rate, Rigour, Regulation, Reputation and Risk) Almost every model can be implemented in a way that makes it explainable and no models are explainable by themself but need an additional layer for the translation of the results to make it explainable for endusers. Scenarios where explainability is important: Medical diagnosis, loan lending or general assisance in human desicion making. All scenarios are mainly about classification models . Limitations The scope of explainability is not only defined by the model and data selection / pre processing but often has to be embedded in the application as a whole (Build an explianable AI system with an explanation userinterface for example) Explainability needs to be accessible. Even models which would be interpretable need to have a method the extract these information and make the explantaion explicite. Open Qustions How to treat techniques (from outside: model inducion, from inside: deep explanation) which enhance explainability in models which would be less explainable by nature? What should our score on explainability tell? How easily the model could be interpreted and an XAI system could be build with it?","title":"Key Takaways, Questions and Limitations"},{"location":"Pillars/Explainability/#summaries-of-paper-website","text":"See all papers related to explainability from IBM here: publications","title":"Summaries of Paper &amp; Website"},{"location":"Pillars/Explainability/#explainable-ai","text":"The paper is business oriented and talks about why explainability is an advatage to have embedded in AI algorithms and in which usecases explainability should have a high priority. (p.8) The need for Explainability: Working as intended?, How sensitive is the impact?, Are you comfortable with the level of control? (p.9) factors to consider: Revenue, Rate, Rigour, Regulation, Reputation and Risk (p.12) Explainable by design: Explainability needs to be considered up front and embedded into the design of the AI application. It affects the choice ofmachine learning algorithm and mayimpact the way you choose to pre-process data. (p.13) Trade-offs in explainability: Interpretability is a characteristic of a model that is generally considered to come at a cost. As a rule of thumb, the more complex the model, the more accurate it is, but the less interpretable it is. (Decision tree vs. DNN with many layers and features) (p.14) Differnt explaination techniques: Sensitivity analysis,Local Interpretable Model Explanations (LIME), Shapley Additive Explanations (SHAP), Tree interpreters, Neural Network Interpreters (Appendix 2) Subjective scale of explainability of different classes of algorithms and learning techniques (with 1 being the most difficult and 5 being the easiest to explain) see a compriesed version of the table belwo: Learning technique Scale of explainability (1-5) Bayesian belief networks (BNNs) 3.5 Decision trees 4 Logistic regression 3 Support vector machines (SVMs) 2 K-means clustering 3 Neural networks 1 Random forest/boosting 3 Q-learning 2 Hidden Markov models 3","title":"Explainable AI"},{"location":"Pillars/Explainability/#explainable-artificial-intelligence-xai","text":"XAI is a project from Defense Advanced Research Projects Agency which aims to produce more explainable models, while maintaining a high level of learning performance (prediction accuracy)and enable human users to understand, appropriately trust, and effectively manage the emerging generation of artificially intelligent partners see their concept here: DARPA slide deck on explainability (two key slides below) See another slide deck on XAI with more examples here: second XAI slide deck","title":"Explainable Artificial Intelligence (XAI)"},{"location":"Pillars/Explainability/#improving-transparency-of-models","text":"Context of AI applications were transparency and explainability is key: financial risk assessment (Goyal 2018), medical diagnosis and treatment planning (Strickland 2019), hiring and promotion decisions (Alsever 2017), social services eligibility determination (Fishman, Eggers, and Kishnani 2019), predictive policing (Ensign et al. 2017), and probation and sentencing recom- mendations (Larson et al. 2016). Recent work has outlined the need for increased transparency in AI for data sets(Gebru et al. 2018; Bender and Friedman 2018; Holland et al. 2018), models (Mitchell et al. 2019),and services (Arnold et al. 2019).","title":"Improving transparency of models"},{"location":"Pillars/Explainability/#metrics-for-explainable-ai-xai","text":"Note Explaibale AI is much more than just having an easily interpretable model but about a whole system that is able to explain itself and interact with the user to create a full unterstanding tailored to the user needs. The papers elaborates metrics for the goodness of explanations, whether users are satisfied by explanations, how well users understand the AI systems, how curiosity motivates the search for explanations, whether the user's trust and reliance on the AI are appropriate.","title":"Metrics for Explainable AI (XAI)"},{"location":"Pillars/Explainability/#interpretability-to-whome","text":"A machinelearning system\u2019s interpretability should be de-fined in relation to a specific agent or task: we should not ask if the system is interpretable, but to whom is it interpretable","title":"Interpretability to whome?"},{"location":"Pillars/Explainability/#explainability-in-image-classification","text":"Saliency maps are a popular approach to creating post-hoc explanations of image classifier outputs. These methods produce estimates of the relevance of each pixel to the classification output score, which can be displayed as a saliency map that highlights important pixels.","title":"Explainability in image classification"},{"location":"Pillars/Explainability/#towards-a-rigorous-science-of-interpretable-machine-learning","text":"The need for interpretability stems from an incompleteness in the problem formalization. Scensario where this can be the case: Scientific Understanding: The human\u2019s goal is to gain knowledge. We do not have a completeway of stating what knowledge is; thus the best we can do is ask for explanations we canconvert into knowledge. Safety: For complex tasks, the end-to-end system is almost never completely testable; onecannot create a complete list of scenarios in which the system may fail. Enumerating allpossible outputs given all possible inputs be computationally or logistically infeasible, and wemay be unable to flag all undesirable outputs. Ethics: The human may want to guard against certain kinds of discrimination, and theirnotion of fairness may be too abstract to be completely encoded into the system. Even if we can encode protections for specific protected classes into the system, there might be biases that we did not consider a priori dimensions of interpretability: Global vs. Local: Global interpretability implies knowing what patterns are present in general, while local interpretability implies knowing the reasons for a specific decision (such as why a particular loan application was rejected).The former may be important for when scientific understanding or bias detection is the goal;the latter when one needs a justification for a specific decision Area, Severity of Incompleteness: What part of the problem formulation is incomplete, andhow incomplete is it? ime Constraints: How long can the user afford to spend to understand the explanation? Adecision that needs to be made at the bedside or during the operation of a plant must beunderstood quickly, while in scientific or anti-discrimination applications, the end-user maybe willing to spend hours trying to fully understand an explanation. User Expertise: How experienced is the user in the task?","title":"Towards A Rigorous Science of Interpretable Machine Learning"},{"location":"Pillars/Explainability/#comprehensible-classification-models","text":"(p.2-5) Decision trees, classification rules, decision tables, nearest neighbors, and Bayesian network classifiers, with respect to their interpretability (p.6) The Drawbacks of Model Size as the Single Measure of Comprehensibility. Big decision trees with easily understandable features can be more comprehensible than small trees with less intuitive features. Models that are too small can also be to simple for users to accept and not lead to less comprehensibility and general trust in the model. (p.7) Monotonicity Constraints in Classification Models. Users are more like to trust and accept classification models when they are built by respecting monotonicity constraints in the application domain. A monotonic relationship between a feature and the class attribute occurs when increasing the value of the feature tends to either monotonically increase or monotonically decrease the probability of an instance\u2019s membership to a class","title":"Comprehensible Classification Models"},{"location":"Pillars/Explainability/#taxonomy","text":"Construct a set of measurable metrict which can be used to calculate a score that should indicate how good the explainability of a model is. For The calculation of the score differnt weights can be assigned to the metrics. metrics where 5 is the best and 1 the worst: Model type: Different between ML model types and assigne a score to each depending on how easily the result is to explain. Monotonicity: A score from 1 to 5 for indicating how many monotone attributes there are, the higher the fraction the better. Relevance: A score from 1 to 5 how relevant the attributes are. Ideally there are no irrelevant attributes (the definition of relevance depends on the model type) Model size: A score from 1 to 5 where bigger more complex models with many attributes have (new metric: uncorrelated dataset?) table { width:100%; } metric description unit weight model type some models like linear regression or decision tree are very explainable and models like neural networks less [1,5] 0.5 faithfulness relevance are the features truly relevant or can some be omitted? [1,5] 0.2 monotonicity monotonic attribute functions most important feature for classification [1,5] 0.2 model size more features add to the complexity and may decrease comprehensibility [1,5] 0.1","title":"Taxonomy"},{"location":"Pillars/Robustness/","text":"Robustness Introduction Machine Learning models might not be robust against adversarial perturbations. With visually imperceptible adversarial perturbations, attackers may cause an input to be misclassified. These attacks can be generated in both the black-box setting, where the parameters of the model are unknown to the attacker, and the white-box settings, where attacker have the all necessary information about the model. To trust AI we need to be sure that it is robust against adversarial attacks. Key Takeaways, Questions and Limitations Key Takeaways Robustness is a mathematical term and context independent. Limitations By definition we can only talk about robustness of classifiers. Robustness only makes sense if the input to the model is sufficiently large. When the input is adversarially perturbed the difference should be unnoticiable to the human eye but still be large enough to make the input misclassified. This is only possible when the input is high dimensional so that the differences can be hidden. Thats why they usually use Convolutional Neural Networks as an example. Open Questions How should our algorithm act on regression models? The topic is well explored on neural network classifiers but there are almost zero sources on different models. How should our algorithm evaluate different classifier models? Sources Websites IBM Robustness: Robustness main page Adversarial Robustness 360: Toolbox Adversarial Robustness Toolbox Github repo: ART v1.5 CLEVER Github repo: Robustness Score Papers Adversarial Robustness Toolbox v1.0.0 Evaluating The Robustness of Neural Networks: An Extreme Value Theory Approach Efficient Neural Network Robustness Certification with General Activation Functions Robust Local Features For Improving The Generalization of Adversarial Training Analyzing Federated Learning through an Adversarial Lens Formal Guarantees on the Robustness of a Classifier against Adversarial Manipulation Metrics The main idea of the robustness metrics is to calculate the minimal perturbation that is required to get an input misclassified. In other words to calculate the average sensitivity of the model\u2019s loss function with respect to changes in the inputs. Empirical Robustness Assesses the robustness of a given classifier with respect to a specific attack and test data set. It is equivalent to the average minimal perturbation that the attacker needs to introduce for a successful attack. To be able to calculate empirical robustness we need to know the type and the parameters of the attack. Loss Sensitivity Local loss sensitivity quantify the smoothness of a model by estimating its Lipschitz continuity constant. Lipschitz constant measures the largest variation of the output of the model under a small change in its input. The smaller the value, the smoother the function. CLEVER Score For a given input CLEVER estimates the minimal perturbation that is required to change the classification. The derivation is based on a Lipschitz constant. The CLEVER algorithm uses an estimate based on extreme value theory. The CLEVER score can be calculated for both untargeted and targeted attacks. A higher CLEVER score indicates better robustness. Loss Sensitivity VS CLEVER Score CLEVER Score and Loss Sensitivity both uses the same idea of estimating the smoothness by Lipschitz constant. They both are attack-independent. But they have a couple of differences. Loss sensitivity uses Local Lipschitz constant estimation meanwhile CLEVER uses Cross Lipschitz constant estimation. Even though they try to estimate similar constants, Cross Lipschitz constant calculation includes an additional difference operation. Hence a model with a high CLEVER score would have a low loss sensitivity. For Loss sensitivity; the smaller the value, the smoother the function. For CLEVER score; the greater the value, the more robust the model. As CLEVER uses a sampling based method, the scores may vary slightly for different runs while Loss Sensitivity calculation is determined. Clique Method Robustness Verification for Decision Tree Ensembles For decision-tree ensemble models like Gradient Boosted Decision Trees, Random Forest, or Extra Trees robustness can be verified based on clique method. For our purposes this measure might be too specific. CROWN framework for certifying neural networks CROWN is a framework for efficiently computing a certified lower bound of minimum adversarial distortion given any input data point for neural networks. To calculate the lower bound one must know about the activation function used in the Neural Network. Then check where the activation function is concave or convex and choose lower/upper bound functions accordingly. Not a fully designed metric, just gives an upper bound with respect to the given data point. Attacks As most of the papers use success rate of attacks as a robustness metric we can use state-of-the-art attacks in our algorithm. Carlini Wagner attack It is a white-box targeted attack algorithm tailored to three distance metrics (l 2 , l 0 , l infinity ). It is referred as a state-of-the-art attack. It tries to optimize a minimization problem using gradient descent. Quite powerful, however it is often much slower than others. Basic Iterative Method It is an extension of the fast gradient attack algorithm. It is a black-box attack and has targeted/untargeted versions. Straightforward and practical. DeepFool An efficient attack for deep neural networks. It is black-box and untargeted. For a given input, it finds the nearest decision boundary in l 2 norm. Taxonomy model type metric description unit weight Decision Tree Clique Method Robustness Verification Gives a lower bound on robustness for decision tree ensembles. Larger value better robustness. [0 inf] 100% Neural Network Loss Sensitivity Quantify the smoothness of a model. Smaller value better robustness. [0 inf] 20% Neural Network CLEVER Score Estimates the minimal perturbation that is required to change the classification. Higher value better robustness. [0 inf] 20% Neural Network Empirical robustness (CW attack) Success rate of the CW attack. Smaller value better robustness. [0 1] 20% Neural Network Empirical robustness (Basic Iterative Method) Success rate of the Basic Iterative Method. Smaller value better robustness. [0 1] 20% Neural Network Empirical robustness (DeepFool) Success rate of the DeepFool attack. Smaller value better robustness. [0 1] 20%","title":"Robustness"},{"location":"Pillars/Robustness/#robustness","text":"","title":"Robustness"},{"location":"Pillars/Robustness/#introduction","text":"Machine Learning models might not be robust against adversarial perturbations. With visually imperceptible adversarial perturbations, attackers may cause an input to be misclassified. These attacks can be generated in both the black-box setting, where the parameters of the model are unknown to the attacker, and the white-box settings, where attacker have the all necessary information about the model. To trust AI we need to be sure that it is robust against adversarial attacks.","title":"Introduction"},{"location":"Pillars/Robustness/#key-takeaways-questions-and-limitations","text":"Key Takeaways Robustness is a mathematical term and context independent. Limitations By definition we can only talk about robustness of classifiers. Robustness only makes sense if the input to the model is sufficiently large. When the input is adversarially perturbed the difference should be unnoticiable to the human eye but still be large enough to make the input misclassified. This is only possible when the input is high dimensional so that the differences can be hidden. Thats why they usually use Convolutional Neural Networks as an example. Open Questions How should our algorithm act on regression models? The topic is well explored on neural network classifiers but there are almost zero sources on different models. How should our algorithm evaluate different classifier models?","title":"Key Takeaways, Questions and Limitations"},{"location":"Pillars/Robustness/#sources","text":"","title":"Sources"},{"location":"Pillars/Robustness/#websites","text":"IBM Robustness: Robustness main page Adversarial Robustness 360: Toolbox Adversarial Robustness Toolbox Github repo: ART v1.5 CLEVER Github repo: Robustness Score","title":"Websites"},{"location":"Pillars/Robustness/#papers","text":"Adversarial Robustness Toolbox v1.0.0 Evaluating The Robustness of Neural Networks: An Extreme Value Theory Approach Efficient Neural Network Robustness Certification with General Activation Functions Robust Local Features For Improving The Generalization of Adversarial Training Analyzing Federated Learning through an Adversarial Lens Formal Guarantees on the Robustness of a Classifier against Adversarial Manipulation","title":"Papers"},{"location":"Pillars/Robustness/#metrics","text":"The main idea of the robustness metrics is to calculate the minimal perturbation that is required to get an input misclassified. In other words to calculate the average sensitivity of the model\u2019s loss function with respect to changes in the inputs.","title":"Metrics"},{"location":"Pillars/Robustness/#empirical-robustness","text":"Assesses the robustness of a given classifier with respect to a specific attack and test data set. It is equivalent to the average minimal perturbation that the attacker needs to introduce for a successful attack. To be able to calculate empirical robustness we need to know the type and the parameters of the attack.","title":"Empirical Robustness"},{"location":"Pillars/Robustness/#loss-sensitivity","text":"Local loss sensitivity quantify the smoothness of a model by estimating its Lipschitz continuity constant. Lipschitz constant measures the largest variation of the output of the model under a small change in its input. The smaller the value, the smoother the function.","title":"Loss Sensitivity"},{"location":"Pillars/Robustness/#clever-score","text":"For a given input CLEVER estimates the minimal perturbation that is required to change the classification. The derivation is based on a Lipschitz constant. The CLEVER algorithm uses an estimate based on extreme value theory. The CLEVER score can be calculated for both untargeted and targeted attacks. A higher CLEVER score indicates better robustness.","title":"CLEVER Score"},{"location":"Pillars/Robustness/#loss-sensitivity-vs-clever-score","text":"CLEVER Score and Loss Sensitivity both uses the same idea of estimating the smoothness by Lipschitz constant. They both are attack-independent. But they have a couple of differences. Loss sensitivity uses Local Lipschitz constant estimation meanwhile CLEVER uses Cross Lipschitz constant estimation. Even though they try to estimate similar constants, Cross Lipschitz constant calculation includes an additional difference operation. Hence a model with a high CLEVER score would have a low loss sensitivity. For Loss sensitivity; the smaller the value, the smoother the function. For CLEVER score; the greater the value, the more robust the model. As CLEVER uses a sampling based method, the scores may vary slightly for different runs while Loss Sensitivity calculation is determined.","title":"Loss Sensitivity VS CLEVER Score"},{"location":"Pillars/Robustness/#clique-method-robustness-verification-for-decision-tree-ensembles","text":"For decision-tree ensemble models like Gradient Boosted Decision Trees, Random Forest, or Extra Trees robustness can be verified based on clique method. For our purposes this measure might be too specific.","title":"Clique Method Robustness Verification for Decision Tree Ensembles"},{"location":"Pillars/Robustness/#crown-framework-for-certifying-neural-networks","text":"CROWN is a framework for efficiently computing a certified lower bound of minimum adversarial distortion given any input data point for neural networks. To calculate the lower bound one must know about the activation function used in the Neural Network. Then check where the activation function is concave or convex and choose lower/upper bound functions accordingly. Not a fully designed metric, just gives an upper bound with respect to the given data point.","title":"CROWN framework for certifying neural networks"},{"location":"Pillars/Robustness/#attacks","text":"As most of the papers use success rate of attacks as a robustness metric we can use state-of-the-art attacks in our algorithm.","title":"Attacks"},{"location":"Pillars/Robustness/#carlini-wagner-attack","text":"It is a white-box targeted attack algorithm tailored to three distance metrics (l 2 , l 0 , l infinity ). It is referred as a state-of-the-art attack. It tries to optimize a minimization problem using gradient descent. Quite powerful, however it is often much slower than others.","title":"Carlini Wagner attack"},{"location":"Pillars/Robustness/#basic-iterative-method","text":"It is an extension of the fast gradient attack algorithm. It is a black-box attack and has targeted/untargeted versions. Straightforward and practical.","title":"Basic Iterative Method"},{"location":"Pillars/Robustness/#deepfool","text":"An efficient attack for deep neural networks. It is black-box and untargeted. For a given input, it finds the nearest decision boundary in l 2 norm.","title":"DeepFool"},{"location":"Pillars/Robustness/#taxonomy","text":"model type metric description unit weight Decision Tree Clique Method Robustness Verification Gives a lower bound on robustness for decision tree ensembles. Larger value better robustness. [0 inf] 100% Neural Network Loss Sensitivity Quantify the smoothness of a model. Smaller value better robustness. [0 inf] 20% Neural Network CLEVER Score Estimates the minimal perturbation that is required to change the classification. Higher value better robustness. [0 inf] 20% Neural Network Empirical robustness (CW attack) Success rate of the CW attack. Smaller value better robustness. [0 1] 20% Neural Network Empirical robustness (Basic Iterative Method) Success rate of the Basic Iterative Method. Smaller value better robustness. [0 1] 20% Neural Network Empirical robustness (DeepFool) Success rate of the DeepFool attack. Smaller value better robustness. [0 1] 20%","title":"Taxonomy"},{"location":"Pillars/Fairness/","text":"Fairness Introduction An increasing number of information systems take decisions based on statistical inference rules, acquired through machine learning techniques. Especially in decision-critical contexts such as predictive policing, lending decisions in credit scoring or triage and allocation of health care resources discrimination is to be avoided. In order to avoid encoding discrimination in automated decisions, multiple fairness aspects needs to be accounted for. Definition Legally recognized 'protected classes' Depending on the context the stakeholders need to decide during the model creation process which systemic difference between groups should be available to the model and which should be excluded. These sensitive features should either not be used or very carefully, since they are protected by law. Race (Civil Rights Act of 1964) Color (Civil Rights Act of 1964) Sex (Equal Pay Act of 1963; Civil Rights Act of 1964) Religion (Civil Rights Act of 1964) National Origin (Civil Rights Act of 1964) Citizenship (Immigration Reform and Control Act) Age (Age Discrimination in Employment Act of 1967) Pregnancy (Pregnancy Discrimination Act) Family Status (Civil Rights Act of 1968) Disability Status (Rehabilitation Act of 1973; Americans with Disabilities Act of 1990) Veteran Status (Vietnam Era Veterans Readjustment Assistance Act of 1974) Genetic Information (Genetic Information Nondiscrimination Act) Sexual orientation (in some jurisdications) Application Examples Medical Testing and Diagnosis Making decisions about a patient\u2019s treatment may rely on tests providing probability estimates for different diseases and conditions. Here too we can ask whether such decision-making is being applied uniformly across different groups of patients. Hiring Lending School Admission Criminal Justice Counterexamples Examples of Algorithmic Unfairness 1. COMPAS risk tool * Assess a defendant\u2019s probability of recidivism. * Tool\u2019s errors were asymmetric: African-American defendants were more likely to be incorrectly labeled as higher-risk than they actually were, while white defendants were more likely to be incorrectly labeled as lower-risk than they actually were. * White defendants who went on to commit future crimes were assigned risk scores corresponding to lower probability estimates in aggregate. 2. Apple Credit Card * Apple Card's Credit Scoring Algorithm was investigated after discriminating against women. 3. Apple Credit Card Fairness Metrics within the ML Pipeline 1. Business Understanding Ask relevant questions and define objectives for the problem that needs to be tackled. 2. Data Mining Gather and scrape the data necessary for the project. 3. Data Cleaning Fix the inconsistencies within the data and handle the missing values. Class imbalance: The number of observations for each class deviates substantially from the proportions Over- and Undersampling Unfairness from Underrepresentation 4. Data Exploration Form hypotheses about your defined problem by visually analyzing the data. 5. Feature Engineering Select important features and construct more meaningful ones using the raw data that you have. 6. Predictive Modeling Train machine learning models, evaluate their performance, and use them to make predictions. 7. Data Visualization Communicate the findings with key stakeholders using plots and interactive visualizations. Fairness Metrics (1) and (2) are widely used ideas inspired by anti-discrimnation legislation 1. Disparate Treatment A practice that intentionally disadvantages/discriminates a group based on a protected feature ( e.g the pay difference between men and women at the same position, ). The treatment or process should not depend on a sensitive feature encoding group membership. How to check for Disparate Treatment Depending on the context certain attributes are considered to be protected. For hiring decisions in Germany for example the exmployer is not allowed to use certain information e.g (pregnancy status, wish for a child, relationship status). The user of our tool could be presented with all attributes the model is taking into consideration. He could then select every attribute he would consider in this context. On the other hand side it would be possible to define a default list of protected attributes (gender, religion, race) and let their use negatively influence the fairness score. 2. Disparate Impact Is what occurs when an organization\u2019s actions, policies, or some other aspect of their processes inadvertently result in unintentional discrimination against people who are in a protected class. Even though the policy, action, or item in question would otherwise appear to be neutral. What matters is the outcome, not the intent. Equality in outcomes across groups based on protected features has to be assured. How to check for Disparate Impact 3. Statistical Parity In some cases statistical parity is a central goal (and in some it is legally mandated). Statistical parity, ensures that the overall proportion of members in a protected group receiving positive (negative) classification are identical to the proportion of the population as a whole. 4. Envy Free Fairness In an envy free assignment every individual does not want to receive the anothers' assignment. E.g cake cutting where the slices have equal size. 4. Disparate Mistreatment Classifier might make decisions for people belonging to different social groups with different misclassification rates. When the ground truth for historical decisions is available, disproportionately beneficial outcomes for certain sensitive attribute value groups can be justified and explained by means of the ground truth. Therefore, disparate impact would not be a suitable notion of unfairness in such scenarios. Disparate Mistreatment arises when the false positive rates (FPR) and false negative rates (FNR) are substantially different between groups. Avoiding Disparate Mistreatment requires to establish Group Error Parity For this the prediction error (FPR, FNR) should be similar across groups and therefore independant of protected feature E s=0 [f(x,s), y] = E s=1 [f(x,s), y] Process Let the user select the variables that he considers to be protected from the training dataset. Some metrics need to be scored by the user. For example the proper class balance and for example the question if the problem or question the model is trying to solve or predict is very hard to be assessed automatically. Therefore the user scores that fairness on a level from [0,1] or [0,100]. Compute metrics based on the protected attributes for example group fairness, disparate treatment score, disparate impact score etc. Based on the taxonomy an overall fairness score can then be created. Summaries Year Author Title Context Content 2012 Martin Glaser \"Racial profiling is a discriminatory practice that undermines fundamental civil rights while failing to promote law enforcement goals\" Racial Profiling Racial profiling is of unjust nature 2015 Amit Datta \"Automated Experiments on Ad Privacy Settings\" Advertising - Setting the gender to famel results in getting fewer instances of an ad related to high paying jobs than setting it to male. - The amoral status of an algorithm does not negate its effects on society. - Manifold interactions can lead to discrimination, whereby not one party can be blamed solely. 2016 John Kleinberg \"Inherent Trade-Offs in the Fair Determination of Risk Scores\" Law Healthcare - Three fairness notions are developed and it is shown that under non trivial conditions it is impossible to satisfy all three simultaneously. 2016 Muhammad Zafar \"Fairness Beyond Disparate Treatment & Disparate Impact: Learning Classification without Disparate Mistreatment\" 2019 Ziad Obermeyer Dissecting racial bias in an algorithm used to manage the health of populations Healthcare - Show that a widely used algorithm, typical of this industry-wide approach and affecting millions of patients, exhibits significant racial bias. - At a given risk score, Black patients are considerably sicker than White patients, as evidenced by signs of uncontrolled illnesses. Taxonomy Stage Metric Description Unit Weight 1. Business Understanding Question Fairness Is the question we are trying to answer fair in itself? It would be considered fair to recommend a preferred treatment to a patient, but the application of machine learning for racial profiling would be considered unfair. [0,1] 1/n Context Criticality How important is fairness in the context the model operates in? In a legal context fairness is very important while it is less important for marketing purposes. [0,1] 1/n 2. Data Mining Biased Data Does the data possibly contain a bias which was introduced during the data collection? (e.g selection bias) {0,1} 1/n 3. Data Cleaning Class Balance/ Imbalance To what degree does the sample (training dataset) represent the expected class distribution of the real underlying population? [0,1] 1/n 4. Data Exploration - - - - 5. Feature Engineering - - - - 6. Predictive Modeling Disparate Treatment Depending on the context certain features (gender, religion, race) are considered to be protected. Does the model use a at least one protected feature for its prediction? [0,1] 1/n Disparate Impact A practice that intentionally disadvantages/discriminates a group based on a protected feature (e.g the pay difference between men and women at the same position, ). The treatment or process should not depend on a sensitive feature encoding group membership. [0,1] 1/n Disparate Mistreatment Check if the prediction error (FPR, FNR) is similar across groups and therefore independant of protected features E s=0 [f(x,s), y] = E s=1 [f(x,s), y] [0,1] 1/n Statistical Parity Statistical parity, ensures that the overall proportion of members in a protected group receiving positive (negative) classification are identical to the proportion of the population as a whole. [0,1] 1/n 7. Data Visualization - - - - References","title":"Fairness"},{"location":"Pillars/Fairness/#fairness","text":"","title":"Fairness"},{"location":"Pillars/Fairness/#introduction","text":"An increasing number of information systems take decisions based on statistical inference rules, acquired through machine learning techniques. Especially in decision-critical contexts such as predictive policing, lending decisions in credit scoring or triage and allocation of health care resources discrimination is to be avoided. In order to avoid encoding discrimination in automated decisions, multiple fairness aspects needs to be accounted for.","title":"Introduction"},{"location":"Pillars/Fairness/#definition","text":"","title":"Definition"},{"location":"Pillars/Fairness/#legally-recognized-protected-classes","text":"Depending on the context the stakeholders need to decide during the model creation process which systemic difference between groups should be available to the model and which should be excluded. These sensitive features should either not be used or very carefully, since they are protected by law. Race (Civil Rights Act of 1964) Color (Civil Rights Act of 1964) Sex (Equal Pay Act of 1963; Civil Rights Act of 1964) Religion (Civil Rights Act of 1964) National Origin (Civil Rights Act of 1964) Citizenship (Immigration Reform and Control Act) Age (Age Discrimination in Employment Act of 1967) Pregnancy (Pregnancy Discrimination Act) Family Status (Civil Rights Act of 1968) Disability Status (Rehabilitation Act of 1973; Americans with Disabilities Act of 1990) Veteran Status (Vietnam Era Veterans Readjustment Assistance Act of 1974) Genetic Information (Genetic Information Nondiscrimination Act) Sexual orientation (in some jurisdications)","title":"Legally recognized 'protected classes'"},{"location":"Pillars/Fairness/#application-examples","text":"Medical Testing and Diagnosis Making decisions about a patient\u2019s treatment may rely on tests providing probability estimates for different diseases and conditions. Here too we can ask whether such decision-making is being applied uniformly across different groups of patients. Hiring Lending School Admission Criminal Justice","title":"Application Examples"},{"location":"Pillars/Fairness/#counterexamples","text":"Examples of Algorithmic Unfairness 1. COMPAS risk tool * Assess a defendant\u2019s probability of recidivism. * Tool\u2019s errors were asymmetric: African-American defendants were more likely to be incorrectly labeled as higher-risk than they actually were, while white defendants were more likely to be incorrectly labeled as lower-risk than they actually were. * White defendants who went on to commit future crimes were assigned risk scores corresponding to lower probability estimates in aggregate. 2. Apple Credit Card * Apple Card's Credit Scoring Algorithm was investigated after discriminating against women. 3. Apple Credit Card","title":"Counterexamples"},{"location":"Pillars/Fairness/#fairness-metrics-within-the-ml-pipeline","text":"","title":"Fairness Metrics within the ML Pipeline"},{"location":"Pillars/Fairness/#1-business-understanding","text":"Ask relevant questions and define objectives for the problem that needs to be tackled.","title":"1. Business Understanding"},{"location":"Pillars/Fairness/#2-data-mining","text":"Gather and scrape the data necessary for the project.","title":"2. Data Mining"},{"location":"Pillars/Fairness/#3-data-cleaning","text":"Fix the inconsistencies within the data and handle the missing values. Class imbalance: The number of observations for each class deviates substantially from the proportions Over- and Undersampling Unfairness from Underrepresentation","title":"3. Data Cleaning"},{"location":"Pillars/Fairness/#4-data-exploration","text":"Form hypotheses about your defined problem by visually analyzing the data.","title":"4. Data Exploration"},{"location":"Pillars/Fairness/#5-feature-engineering","text":"Select important features and construct more meaningful ones using the raw data that you have.","title":"5. Feature Engineering"},{"location":"Pillars/Fairness/#6-predictive-modeling","text":"Train machine learning models, evaluate their performance, and use them to make predictions.","title":"6. Predictive Modeling"},{"location":"Pillars/Fairness/#7-data-visualization","text":"Communicate the findings with key stakeholders using plots and interactive visualizations.","title":"7. Data Visualization"},{"location":"Pillars/Fairness/#fairness-metrics","text":"(1) and (2) are widely used ideas inspired by anti-discrimnation legislation","title":"Fairness Metrics"},{"location":"Pillars/Fairness/#1-disparate-treatment","text":"A practice that intentionally disadvantages/discriminates a group based on a protected feature ( e.g the pay difference between men and women at the same position, ). The treatment or process should not depend on a sensitive feature encoding group membership. How to check for Disparate Treatment Depending on the context certain attributes are considered to be protected. For hiring decisions in Germany for example the exmployer is not allowed to use certain information e.g (pregnancy status, wish for a child, relationship status). The user of our tool could be presented with all attributes the model is taking into consideration. He could then select every attribute he would consider in this context. On the other hand side it would be possible to define a default list of protected attributes (gender, religion, race) and let their use negatively influence the fairness score.","title":"1. Disparate Treatment"},{"location":"Pillars/Fairness/#2-disparate-impact","text":"Is what occurs when an organization\u2019s actions, policies, or some other aspect of their processes inadvertently result in unintentional discrimination against people who are in a protected class. Even though the policy, action, or item in question would otherwise appear to be neutral. What matters is the outcome, not the intent. Equality in outcomes across groups based on protected features has to be assured. How to check for Disparate Impact","title":"2. Disparate Impact"},{"location":"Pillars/Fairness/#3-statistical-parity","text":"In some cases statistical parity is a central goal (and in some it is legally mandated). Statistical parity, ensures that the overall proportion of members in a protected group receiving positive (negative) classification are identical to the proportion of the population as a whole.","title":"3. Statistical Parity"},{"location":"Pillars/Fairness/#4-envy-free-fairness","text":"In an envy free assignment every individual does not want to receive the anothers' assignment. E.g cake cutting where the slices have equal size.","title":"4. Envy Free Fairness"},{"location":"Pillars/Fairness/#4-disparate-mistreatment","text":"Classifier might make decisions for people belonging to different social groups with different misclassification rates. When the ground truth for historical decisions is available, disproportionately beneficial outcomes for certain sensitive attribute value groups can be justified and explained by means of the ground truth. Therefore, disparate impact would not be a suitable notion of unfairness in such scenarios. Disparate Mistreatment arises when the false positive rates (FPR) and false negative rates (FNR) are substantially different between groups. Avoiding Disparate Mistreatment requires to establish Group Error Parity For this the prediction error (FPR, FNR) should be similar across groups and therefore independant of protected feature E s=0 [f(x,s), y] = E s=1 [f(x,s), y]","title":"4. Disparate Mistreatment"},{"location":"Pillars/Fairness/#process","text":"Let the user select the variables that he considers to be protected from the training dataset. Some metrics need to be scored by the user. For example the proper class balance and for example the question if the problem or question the model is trying to solve or predict is very hard to be assessed automatically. Therefore the user scores that fairness on a level from [0,1] or [0,100]. Compute metrics based on the protected attributes for example group fairness, disparate treatment score, disparate impact score etc. Based on the taxonomy an overall fairness score can then be created.","title":"Process"},{"location":"Pillars/Fairness/#summaries","text":"Year Author Title Context Content 2012 Martin Glaser \"Racial profiling is a discriminatory practice that undermines fundamental civil rights while failing to promote law enforcement goals\" Racial Profiling Racial profiling is of unjust nature 2015 Amit Datta \"Automated Experiments on Ad Privacy Settings\" Advertising - Setting the gender to famel results in getting fewer instances of an ad related to high paying jobs than setting it to male. - The amoral status of an algorithm does not negate its effects on society. - Manifold interactions can lead to discrimination, whereby not one party can be blamed solely. 2016 John Kleinberg \"Inherent Trade-Offs in the Fair Determination of Risk Scores\" Law Healthcare - Three fairness notions are developed and it is shown that under non trivial conditions it is impossible to satisfy all three simultaneously. 2016 Muhammad Zafar \"Fairness Beyond Disparate Treatment & Disparate Impact: Learning Classification without Disparate Mistreatment\" 2019 Ziad Obermeyer Dissecting racial bias in an algorithm used to manage the health of populations Healthcare - Show that a widely used algorithm, typical of this industry-wide approach and affecting millions of patients, exhibits significant racial bias. - At a given risk score, Black patients are considerably sicker than White patients, as evidenced by signs of uncontrolled illnesses.","title":"Summaries"},{"location":"Pillars/Fairness/#taxonomy","text":"Stage Metric Description Unit Weight 1. Business Understanding Question Fairness Is the question we are trying to answer fair in itself? It would be considered fair to recommend a preferred treatment to a patient, but the application of machine learning for racial profiling would be considered unfair. [0,1] 1/n Context Criticality How important is fairness in the context the model operates in? In a legal context fairness is very important while it is less important for marketing purposes. [0,1] 1/n 2. Data Mining Biased Data Does the data possibly contain a bias which was introduced during the data collection? (e.g selection bias) {0,1} 1/n 3. Data Cleaning Class Balance/ Imbalance To what degree does the sample (training dataset) represent the expected class distribution of the real underlying population? [0,1] 1/n 4. Data Exploration - - - - 5. Feature Engineering - - - - 6. Predictive Modeling Disparate Treatment Depending on the context certain features (gender, religion, race) are considered to be protected. Does the model use a at least one protected feature for its prediction? [0,1] 1/n Disparate Impact A practice that intentionally disadvantages/discriminates a group based on a protected feature (e.g the pay difference between men and women at the same position, ). The treatment or process should not depend on a sensitive feature encoding group membership. [0,1] 1/n Disparate Mistreatment Check if the prediction error (FPR, FNR) is similar across groups and therefore independant of protected features E s=0 [f(x,s), y] = E s=1 [f(x,s), y] [0,1] 1/n Statistical Parity Statistical parity, ensures that the overall proportion of members in a protected group receiving positive (negative) classification are identical to the proportion of the population as a whole. [0,1] 1/n 7. Data Visualization - - - -","title":"Taxonomy"},{"location":"Pillars/Fairness/#references","text":"","title":"References"}]}