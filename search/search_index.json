{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Trusted AI Master Project This site serves as documentation and Overview of the Master Project. Introduction Artificial intelligence (AI) systems are getting more and more relevance as support to human decision-making processes. While AI holds the promise of delivering valuable insights into many application scenarios, the broad adoption of AI systems will rely on the ability to trust their decisions. According to IBM, some key aspects provide information to trust (or not) a decision made by an algorithm. In particular, a decision must: be reliable and fair, be accounted for, not cause harm, not be tampered with, be understandable, and be secure. In this context, the previous aspects have been grouped into the following pillars. \u2022 Fairness. One of the essential aspects to enable trusted AI is to avoid bias across the entire lifecycle of AI applications, as well as across different bias and data types. \u2022 Robustness. AI-based systems may be vulnerable to adversarial attacks. Attackers may poison training data by injecting samples to compromise system decisions eventually. \u2022 Explainability. In many application scenarios, decisions made by algorithms must be explainable and understandable by humans. Various stakeholders require explanations for different purposes, and explanations must be tailored to their needs. \u2022 Accountability. The quality of decisions should be evaluated in terms of accuracy, ability to satisfy users\u2019 preferences, as well as other properties related to the impact of the decision.The previous characteristics are demanded from AI systems. Yet, to achieve trust in AI, more efforts and automatic mechanisms able to calculate and communicate trust levels are required. Project Documentation The whole documentation of the project can be found on our GitHub page mkdocs The documentation is created with mkdocs, which uses simple markdown file and a .yml config file to build and deploy the documentation to GitHub. The relevant files to create the documentation are in the mkdocs folder The file mkdocs.yml is the config file and has to be adapted if the theme should change or the index changes or new markdowns added. The markdown files are in the folder docs and include the whole documentation. installation pip install --upgrade pip pip install mkdocs workflow: At the location of the mkdocs folder open the terminal and enter: mkdocs serve This will build the documentation and host it locally, where the full documentation can be seen. change the config or markdown files. Sinc you are serving the documentation locally you can see the changes in the config or markdown files in real time. You will always see the output and how the documentation will look in the end and therefore minimalize syntax errors in markdown for example. deploy the build on GitHub to the branche gh-pages. For this open another termianl at the localtion of the mkdocs folder and enter: mkdocs gh-deploy","title":"Home"},{"location":"#trusted-ai-master-project","text":"This site serves as documentation and Overview of the Master Project.","title":"Trusted AI Master Project"},{"location":"#introduction","text":"Artificial intelligence (AI) systems are getting more and more relevance as support to human decision-making processes. While AI holds the promise of delivering valuable insights into many application scenarios, the broad adoption of AI systems will rely on the ability to trust their decisions. According to IBM, some key aspects provide information to trust (or not) a decision made by an algorithm. In particular, a decision must: be reliable and fair, be accounted for, not cause harm, not be tampered with, be understandable, and be secure. In this context, the previous aspects have been grouped into the following pillars. \u2022 Fairness. One of the essential aspects to enable trusted AI is to avoid bias across the entire lifecycle of AI applications, as well as across different bias and data types. \u2022 Robustness. AI-based systems may be vulnerable to adversarial attacks. Attackers may poison training data by injecting samples to compromise system decisions eventually. \u2022 Explainability. In many application scenarios, decisions made by algorithms must be explainable and understandable by humans. Various stakeholders require explanations for different purposes, and explanations must be tailored to their needs. \u2022 Accountability. The quality of decisions should be evaluated in terms of accuracy, ability to satisfy users\u2019 preferences, as well as other properties related to the impact of the decision.The previous characteristics are demanded from AI systems. Yet, to achieve trust in AI, more efforts and automatic mechanisms able to calculate and communicate trust levels are required.","title":"Introduction"},{"location":"#project-documentation","text":"The whole documentation of the project can be found on our GitHub page","title":"Project Documentation"},{"location":"#mkdocs","text":"The documentation is created with mkdocs, which uses simple markdown file and a .yml config file to build and deploy the documentation to GitHub. The relevant files to create the documentation are in the mkdocs folder The file mkdocs.yml is the config file and has to be adapted if the theme should change or the index changes or new markdowns added. The markdown files are in the folder docs and include the whole documentation. installation pip install --upgrade pip pip install mkdocs workflow: At the location of the mkdocs folder open the terminal and enter: mkdocs serve This will build the documentation and host it locally, where the full documentation can be seen. change the config or markdown files. Sinc you are serving the documentation locally you can see the changes in the config or markdown files in real time. You will always see the output and how the documentation will look in the end and therefore minimalize syntax errors in markdown for example. deploy the build on GitHub to the branche gh-pages. For this open another termianl at the localtion of the mkdocs folder and enter: mkdocs gh-deploy","title":"mkdocs"},{"location":"AI_algo/","text":"Trusted AI Algorithm Basic Idea Taxonomy Architecture","title":"trusted AI algorithm"},{"location":"AI_algo/#trusted-ai-algorithm","text":"","title":"Trusted AI Algorithm"},{"location":"AI_algo/#basic-idea","text":"","title":"Basic Idea"},{"location":"AI_algo/#taxonomy","text":"","title":"Taxonomy"},{"location":"AI_algo/#architecture","text":"","title":"Architecture"},{"location":"code_documentation/","text":"Implementation of the trusted AI Algorithm General Architecture File Structure Build How it works Evaluation and Results","title":"Implementation"},{"location":"code_documentation/#implementation-of-the-trusted-ai-algorithm","text":"","title":"Implementation of the trusted AI Algorithm"},{"location":"code_documentation/#general-architecture","text":"","title":"General Architecture"},{"location":"code_documentation/#file-structure","text":"","title":"File Structure"},{"location":"code_documentation/#build","text":"","title":"Build"},{"location":"code_documentation/#how-it-works","text":"","title":"How it works"},{"location":"code_documentation/#evaluation-and-results","text":"","title":"Evaluation and Results"},{"location":"sources/","text":"Overview This is a collection of all material that might be interesting or relevant regarding the project. Websites, papers and videos Explainability IBM explainability papers: explainability papers AI Explainability 360: Toolkit page AI Explainability GitHub repo: AIX360 towards Robust Interpretability paper Generating Contrastive Explanations withMonotonic Attribute Functions paper YouTube Videos Add here a list of usefull and interesing YT videos Decison making: Can we trust artificial intelligence to make decisions for us? Websites IBM trusted AI main page: AI Research","title":"sources"},{"location":"sources/#overview","text":"This is a collection of all material that might be interesting or relevant regarding the project.","title":"Overview"},{"location":"sources/#websites-papers-and-videos","text":"","title":"Websites, papers and videos"},{"location":"sources/#explainability","text":"IBM explainability papers: explainability papers AI Explainability 360: Toolkit page AI Explainability GitHub repo: AIX360 towards Robust Interpretability paper Generating Contrastive Explanations withMonotonic Attribute Functions paper","title":"Explainability"},{"location":"sources/#youtube-videos","text":"Add here a list of usefull and interesing YT videos Decison making: Can we trust artificial intelligence to make decisions for us?","title":"YouTube Videos"},{"location":"sources/#websites","text":"IBM trusted AI main page: AI Research","title":"Websites"},{"location":"Pillars/Accountability/","text":"Accountability and Transparency Summary Summary papers Arnold et al. 2019: Arnold, M.; Bellamy, R. K. E.; Hind, M.; Houde, S.; Mehta, S.; Mojsilovi \u0301c, A.; Nair, R.; Natesan- Ramamurthy, K.; Olteanu, A.; Piorkowski, D.; Reimer, D.; Richards, J.; Tsay, J.; and Varshney, K. R. 2019. FactSheets: Increasing trust in AI services through suppliers declarations of conformity. IBM Journal of Research & Development 63(4/5). A FactSheet, as proposed by (Arnold et al. 2019), is a col- lection of relevant information about an AI model or ser- vice that is created during the machine learning life cycle. It includes information from the business owner (e.g., in- tended use and business justification), from the data gather- ing/feature selection/data cleaning phase (e.g., data sets, fea- tures used or created, cleaning operations), from the model training phase (e.g., bias, robustness, and explainability in- formation), and from the model validation and deployment phase (e.g., key performance indicators). A FactSheet is as- sociated with a model (or service) and is meant to be write once, i.e., an update to a model would trigger a new Fact- Sheet for the updated model. FactSheets can be consumed by any role in the ML life cycle to confirm process gov- ernance adherence or model performance, or by the ulti- mate users of a model to provide increased transparency. Links Taxonomy and Metrics","title":"Accountability"},{"location":"Pillars/Accountability/#accountability-and-transparency","text":"","title":"Accountability and Transparency"},{"location":"Pillars/Accountability/#summary","text":"","title":"Summary"},{"location":"Pillars/Accountability/#summary-papers","text":"Arnold et al. 2019: Arnold, M.; Bellamy, R. K. E.; Hind, M.; Houde, S.; Mehta, S.; Mojsilovi \u0301c, A.; Nair, R.; Natesan- Ramamurthy, K.; Olteanu, A.; Piorkowski, D.; Reimer, D.; Richards, J.; Tsay, J.; and Varshney, K. R. 2019. FactSheets: Increasing trust in AI services through suppliers declarations of conformity. IBM Journal of Research & Development 63(4/5). A FactSheet, as proposed by (Arnold et al. 2019), is a col- lection of relevant information about an AI model or ser- vice that is created during the machine learning life cycle. It includes information from the business owner (e.g., in- tended use and business justification), from the data gather- ing/feature selection/data cleaning phase (e.g., data sets, fea- tures used or created, cleaning operations), from the model training phase (e.g., bias, robustness, and explainability in- formation), and from the model validation and deployment phase (e.g., key performance indicators). A FactSheet is as- sociated with a model (or service) and is meant to be write once, i.e., an update to a model would trigger a new Fact- Sheet for the updated model. FactSheets can be consumed by any role in the ML life cycle to confirm process gov- ernance adherence or model performance, or by the ulti- mate users of a model to provide increased transparency.","title":"Summary papers"},{"location":"Pillars/Accountability/#links","text":"","title":"Links"},{"location":"Pillars/Accountability/#taxonomy-and-metrics","text":"","title":"Taxonomy and Metrics"},{"location":"Pillars/Explainability/","text":"Explainability Abstract Interpretability has become a well-recognized goal for machine learning models. The need for interpretable models is certain to increase as machine learning pushes further into domains such as medicine, criminal justice, and business, where such models complement human decision-makers and decisions can have major consequences on human lives. Transparency is thus required for domain experts to understand, critique, and trust models, and reasoning is required to explain individual decisions. Sanjeeb et. Al Introduction \"In many applications, trust in an AI system will come from its ability to \u2018explain itself.\u2019 Yet, when it comes to understanding and explaining the inner workings of an algorithm, one size does not fit all.Different stakeholders require explanations for different purposes and objectives, and explanations must be tailored to their needs. A physician might respond best to seeing examples of patient data similar to their patient\u2019s. On the other hand, a developer training a neural net will benefit from seeing how information flows through the algorithm. While a regulator will aim to understand the system as a whole and probe into its logic, consumers affected by a specific decision will be interested only in factors impacting their case \u2013 for example, in a loan processing application, they will expect an explanation for why the request was denied and want to understand what changes could lead to approval. IBM Research is creating diverse explanations, including training highly optimized directly interpretable models, creating contrastive explanations of black box models, using information flow in a high-performing complex model to train simpler, interpretable classifiers, learning disentangled representations, and visualizing information flows in neural networks.\" IBM Key Takaways, Questions and Limitations Key Takaways There is a tradeoff between explainability and accuracy (complex models with many features tend to be more accurate but at the same time less transparent) The level of explainability is mainly dominated by its model type and the number of features it uses. The importance of model explainability is highly dependent on the context of the model and its consequences. (Revenue, Rate, Rigour, Regulation, Reputation and Risk) Limitations The scope of explainability is not only the model and data selection / pre processing but often has to be embedded in the application as a whole (Build an explianable AI system with an explanation userinterface for example) Explainability needs accessible. Even models which actions could be displayed need to have a method the extract these information and make it the explantaion explicite. Open Qustions How to treat techniques (from outside: model inducion, from inside: deep explanation) which enhance explainability in models which would be less explainable by nature? Summaries of Paper & Website See all papers related to explainability from IBM here: publications Explainable AI The paper is business oriented and talks about why explainability is an advatage to have embedded in AI algorithms and in which usecases explainability should have a high priority. (p.8) The need for Explainability: Working as intended?, How sensitive is the impact?, Are you comfortable with the level of control? (p.9) factors to consider: Revenue, Rate, Rigour, Regulation, Reputation and Risk (p.12) Explainable by design: Explainability needs to be considered up front and embedded into the design of the AI application. It affects the choice ofmachine learning algorithm and mayimpact the way you choose to pre-process data. (p.13) Trade-offs in explainability: Interpretability is a characteristic of a model that is generally considered to come at a cost. As a rule of thumb, the more complex the model, the more accurate it is, but the less interpretable it is. (Decision tree vs. DNN with many layers and features) (p.14) Differnt explaination techniques: Sensitivity analysis,Local Interpretable Model Explanations (LIME), Shapley Additive Explanations (SHAP), Tree interpreters, Neural Network Interpreters (Appendix 2) Subjective scale of explainability of different classes of algorithms and learning techniques (with 1 being the most difficult and 5 being the easiest to explain) see a compriesed version of the table belwo: Learning technique Scale of explainability (1-5) Bayesian belief networks (BNNs) 3.5 Decision trees 4 Logistic regression 3 Support vector machines (SVMs) 2 K-means clustering 3 Neural networks 1 Random forest/boosting 3 Q-learning 2 Hidden Markov models 3 Explainable Artificial Intelligence (XAI) XAI is a project from Defense Advanced Research Projects Agency which aims to produce more explainable models, while maintaining a high level of learning performance (prediction accuracy)and enable human users to understand, appropriately trust, and effectively manage the emerging generation of artificially intelligent partners see their concept here: DARPA slide deck on explainability (two key slides below) See another slide deck on XAI with more examples here: second XAI slide deck Improving transparency of models Context of AI applications were transparency and explainability is key: financial risk assessment (Goyal 2018), medical diagnosis and treatment planning (Strickland 2019), hiring and promotion decisions (Alsever 2017), social services eligibility determination (Fishman, Eggers, and Kishnani 2019), predictive policing (Ensign et al. 2017), and probation and sentencing recom- mendations (Larson et al. 2016). Recent work has outlined the need for increased transparency in AI for data sets(Gebru et al. 2018; Bender and Friedman 2018; Holland et al. 2018), models (Mitchell et al. 2019),and services (Arnold et al. 2019). Taxonomy Construct a set of measurable metrict which can be used to calculate a score that should indicate how good the explainability of a model is. For The calculation of the score differnt weights can be assigned to the metrics. table { width:100%; } metric description unit weight model type some models like linear regression or decision tree are very explainable and models like neural networks less [1,5] 0.6 faithfulness relevance are the features truly relevant or can some be omitted? [0,1] 0.2 monotonicity monotonic attribute functions most important feature for classification [-1,1] 0.2","title":"Explainability"},{"location":"Pillars/Explainability/#explainability","text":"Abstract Interpretability has become a well-recognized goal for machine learning models. The need for interpretable models is certain to increase as machine learning pushes further into domains such as medicine, criminal justice, and business, where such models complement human decision-makers and decisions can have major consequences on human lives. Transparency is thus required for domain experts to understand, critique, and trust models, and reasoning is required to explain individual decisions. Sanjeeb et. Al","title":"Explainability"},{"location":"Pillars/Explainability/#introduction","text":"\"In many applications, trust in an AI system will come from its ability to \u2018explain itself.\u2019 Yet, when it comes to understanding and explaining the inner workings of an algorithm, one size does not fit all.Different stakeholders require explanations for different purposes and objectives, and explanations must be tailored to their needs. A physician might respond best to seeing examples of patient data similar to their patient\u2019s. On the other hand, a developer training a neural net will benefit from seeing how information flows through the algorithm. While a regulator will aim to understand the system as a whole and probe into its logic, consumers affected by a specific decision will be interested only in factors impacting their case \u2013 for example, in a loan processing application, they will expect an explanation for why the request was denied and want to understand what changes could lead to approval. IBM Research is creating diverse explanations, including training highly optimized directly interpretable models, creating contrastive explanations of black box models, using information flow in a high-performing complex model to train simpler, interpretable classifiers, learning disentangled representations, and visualizing information flows in neural networks.\" IBM","title":"Introduction"},{"location":"Pillars/Explainability/#key-takaways-questions-and-limitations","text":"Key Takaways There is a tradeoff between explainability and accuracy (complex models with many features tend to be more accurate but at the same time less transparent) The level of explainability is mainly dominated by its model type and the number of features it uses. The importance of model explainability is highly dependent on the context of the model and its consequences. (Revenue, Rate, Rigour, Regulation, Reputation and Risk) Limitations The scope of explainability is not only the model and data selection / pre processing but often has to be embedded in the application as a whole (Build an explianable AI system with an explanation userinterface for example) Explainability needs accessible. Even models which actions could be displayed need to have a method the extract these information and make it the explantaion explicite. Open Qustions How to treat techniques (from outside: model inducion, from inside: deep explanation) which enhance explainability in models which would be less explainable by nature?","title":"Key Takaways, Questions and Limitations"},{"location":"Pillars/Explainability/#summaries-of-paper-website","text":"See all papers related to explainability from IBM here: publications","title":"Summaries of Paper &amp; Website"},{"location":"Pillars/Explainability/#explainable-ai","text":"The paper is business oriented and talks about why explainability is an advatage to have embedded in AI algorithms and in which usecases explainability should have a high priority. (p.8) The need for Explainability: Working as intended?, How sensitive is the impact?, Are you comfortable with the level of control? (p.9) factors to consider: Revenue, Rate, Rigour, Regulation, Reputation and Risk (p.12) Explainable by design: Explainability needs to be considered up front and embedded into the design of the AI application. It affects the choice ofmachine learning algorithm and mayimpact the way you choose to pre-process data. (p.13) Trade-offs in explainability: Interpretability is a characteristic of a model that is generally considered to come at a cost. As a rule of thumb, the more complex the model, the more accurate it is, but the less interpretable it is. (Decision tree vs. DNN with many layers and features) (p.14) Differnt explaination techniques: Sensitivity analysis,Local Interpretable Model Explanations (LIME), Shapley Additive Explanations (SHAP), Tree interpreters, Neural Network Interpreters (Appendix 2) Subjective scale of explainability of different classes of algorithms and learning techniques (with 1 being the most difficult and 5 being the easiest to explain) see a compriesed version of the table belwo: Learning technique Scale of explainability (1-5) Bayesian belief networks (BNNs) 3.5 Decision trees 4 Logistic regression 3 Support vector machines (SVMs) 2 K-means clustering 3 Neural networks 1 Random forest/boosting 3 Q-learning 2 Hidden Markov models 3","title":"Explainable AI"},{"location":"Pillars/Explainability/#explainable-artificial-intelligence-xai","text":"XAI is a project from Defense Advanced Research Projects Agency which aims to produce more explainable models, while maintaining a high level of learning performance (prediction accuracy)and enable human users to understand, appropriately trust, and effectively manage the emerging generation of artificially intelligent partners see their concept here: DARPA slide deck on explainability (two key slides below) See another slide deck on XAI with more examples here: second XAI slide deck","title":"Explainable Artificial Intelligence (XAI)"},{"location":"Pillars/Explainability/#improving-transparency-of-models","text":"Context of AI applications were transparency and explainability is key: financial risk assessment (Goyal 2018), medical diagnosis and treatment planning (Strickland 2019), hiring and promotion decisions (Alsever 2017), social services eligibility determination (Fishman, Eggers, and Kishnani 2019), predictive policing (Ensign et al. 2017), and probation and sentencing recom- mendations (Larson et al. 2016). Recent work has outlined the need for increased transparency in AI for data sets(Gebru et al. 2018; Bender and Friedman 2018; Holland et al. 2018), models (Mitchell et al. 2019),and services (Arnold et al. 2019).","title":"Improving transparency of models"},{"location":"Pillars/Explainability/#taxonomy","text":"Construct a set of measurable metrict which can be used to calculate a score that should indicate how good the explainability of a model is. For The calculation of the score differnt weights can be assigned to the metrics. table { width:100%; } metric description unit weight model type some models like linear regression or decision tree are very explainable and models like neural networks less [1,5] 0.6 faithfulness relevance are the features truly relevant or can some be omitted? [0,1] 0.2 monotonicity monotonic attribute functions most important feature for classification [-1,1] 0.2","title":"Taxonomy"},{"location":"Pillars/Fairness/","text":"Fairness Introduction An increasing number of information systems take decisions based on statistical inference rules, acquired through machine learning techniques. Especially in decision-critical contexts such as predictive policing, lending decisions in credit scoring or triage and allocation of health care resources discrimination is to be avoided. In order to avoid encoding discrimination in automated decisions, multiple fairness aspects needs to be accounted for. Definition of Fairness equal in outcomes Legally recognized 'protected classes' Depending on the context the stakeholders need to decide during the model creation process which systemic difference between groups should be available to the model and which should be excluded. These sensitive features should either not be used or very carefully, since they are protected by law. Race (Civil Rights Act of 1964) Color (Civil Rights Act of 1964) Sex (Equal Pay Act of 1963; Civil Rights Act of 1964) Religion (Civil Rights Act of 1964) National Origin (Civil Rights Act of 1964) Citizenship (Immigration Reform and Control Act) Age (Age Discrimination in Employment Act of 1967) Pregnancy (Pregnancy Discrimination Act) Family Status (Civil Rights Act of 1968) Disability Status (Rehabilitation Act of 1973; Americans with Disabilities Act of 1990) Veteran Status (Vietnam Era Veterans Readjustment Assistance Act of 1974) Genetic Information (Genetic Information Nondiscrimination Act) Sexual orientation (in some jurisdications) Application Examples Medical Testing and Diagnosis Making decisions about a patient\u2019s treatment may rely on tests providing probability estimates for different diseases and conditions. Here too we can ask whether such decision-making is being applied uniformly across different groups of patients. Hiring Lending School Admission Criminal Justice Examples of Algorithmic Unfairness COMPAS risk tool Assess a defendant\u2019s probability of recidivism. Tool\u2019s errors were asymmetric: African-American defendants were more likely to be incorrectly labeled as higher-risk than they actually were, while white defendants were more likely to be incorrectly labeled as lower-risk than they actually were. White defendants who went on to commit future crimes were assigned risk scores corresponding to lower probability estimates in aggregate. Apple Credit Card Apple Card's Credit Scoring Algorithm was investigated after discriminating against women. Fairness Metrics (1) and (2) are widely used ideas inspired by anti-discrimnation legislation 1. Disparate Treatment A practice that intentionally disadvantages/discriminates a group based on a protected feature ( e.g the pay difference between men and women at the same position, ). The treatment or process should not depend on sensitive group membership. How to check for Disparate Treatment Depending on the context certain attributes are considered to be protected. For hiring decisions in Germany for example the exmployer is not allowed to use certain information e.g (pregnancy status, wish for a child, relationship status). The user of our tool could be presented with all attributes the model is taking into consideration. He could then select every attribute he would consider in this context. On the other hand side it would be possible to define a default list of protected attributes (gender, religion, race) and let their use negatively influence the fairness score. 2. Disparate Impact Is what occurs when an organization\u2019s actions, policies, or some other aspect of their processes inadvertently result in unintentional discrimination against people who are in a protected class. Even though the policy, action, or item in question would otherwise appear to be neutral. What matters is the outcome, not the intent. How to check for Disparate Impact 3. Statistical Parity In some cases statistical parity is a central goal (and in some it is legally mandated). Statistical parity, ensures that the overall proportion of members in a protected group receiving positive (negative) classification are identical to the proportion of the population as a whole. Summaries of Paper & Website year author title content 2016 John Kleinberg Inherent Trade-Offs in the Fair Determination of Risk Scores . Taxonomy metric description unit weight Disparate Treatment Depending on the context certain features (gender, religion, race) are considered to be protected. Does the model take Disparate Impact","title":"Fairness"},{"location":"Pillars/Fairness/#fairness","text":"","title":"Fairness"},{"location":"Pillars/Fairness/#introduction","text":"An increasing number of information systems take decisions based on statistical inference rules, acquired through machine learning techniques. Especially in decision-critical contexts such as predictive policing, lending decisions in credit scoring or triage and allocation of health care resources discrimination is to be avoided. In order to avoid encoding discrimination in automated decisions, multiple fairness aspects needs to be accounted for.","title":"Introduction"},{"location":"Pillars/Fairness/#definition-of-fairness","text":"equal in outcomes","title":"Definition of Fairness"},{"location":"Pillars/Fairness/#_1","text":"","title":""},{"location":"Pillars/Fairness/#legally-recognized-protected-classes","text":"Depending on the context the stakeholders need to decide during the model creation process which systemic difference between groups should be available to the model and which should be excluded. These sensitive features should either not be used or very carefully, since they are protected by law. Race (Civil Rights Act of 1964) Color (Civil Rights Act of 1964) Sex (Equal Pay Act of 1963; Civil Rights Act of 1964) Religion (Civil Rights Act of 1964) National Origin (Civil Rights Act of 1964) Citizenship (Immigration Reform and Control Act) Age (Age Discrimination in Employment Act of 1967) Pregnancy (Pregnancy Discrimination Act) Family Status (Civil Rights Act of 1968) Disability Status (Rehabilitation Act of 1973; Americans with Disabilities Act of 1990) Veteran Status (Vietnam Era Veterans Readjustment Assistance Act of 1974) Genetic Information (Genetic Information Nondiscrimination Act) Sexual orientation (in some jurisdications)","title":"Legally recognized 'protected classes'"},{"location":"Pillars/Fairness/#application-examples","text":"Medical Testing and Diagnosis Making decisions about a patient\u2019s treatment may rely on tests providing probability estimates for different diseases and conditions. Here too we can ask whether such decision-making is being applied uniformly across different groups of patients. Hiring Lending School Admission Criminal Justice","title":"Application Examples"},{"location":"Pillars/Fairness/#examples-of-algorithmic-unfairness","text":"COMPAS risk tool Assess a defendant\u2019s probability of recidivism. Tool\u2019s errors were asymmetric: African-American defendants were more likely to be incorrectly labeled as higher-risk than they actually were, while white defendants were more likely to be incorrectly labeled as lower-risk than they actually were. White defendants who went on to commit future crimes were assigned risk scores corresponding to lower probability estimates in aggregate. Apple Credit Card Apple Card's Credit Scoring Algorithm was investigated after discriminating against women.","title":"Examples of Algorithmic Unfairness"},{"location":"Pillars/Fairness/#fairness-metrics","text":"(1) and (2) are widely used ideas inspired by anti-discrimnation legislation","title":"Fairness Metrics"},{"location":"Pillars/Fairness/#1-disparate-treatment","text":"A practice that intentionally disadvantages/discriminates a group based on a protected feature ( e.g the pay difference between men and women at the same position, ). The treatment or process should not depend on sensitive group membership. How to check for Disparate Treatment Depending on the context certain attributes are considered to be protected. For hiring decisions in Germany for example the exmployer is not allowed to use certain information e.g (pregnancy status, wish for a child, relationship status). The user of our tool could be presented with all attributes the model is taking into consideration. He could then select every attribute he would consider in this context. On the other hand side it would be possible to define a default list of protected attributes (gender, religion, race) and let their use negatively influence the fairness score.","title":"1. Disparate Treatment"},{"location":"Pillars/Fairness/#2-disparate-impact","text":"Is what occurs when an organization\u2019s actions, policies, or some other aspect of their processes inadvertently result in unintentional discrimination against people who are in a protected class. Even though the policy, action, or item in question would otherwise appear to be neutral. What matters is the outcome, not the intent. How to check for Disparate Impact","title":"2. Disparate Impact"},{"location":"Pillars/Fairness/#3-statistical-parity","text":"In some cases statistical parity is a central goal (and in some it is legally mandated). Statistical parity, ensures that the overall proportion of members in a protected group receiving positive (negative) classification are identical to the proportion of the population as a whole.","title":"3. Statistical Parity"},{"location":"Pillars/Fairness/#summaries-of-paper-website","text":"year author title content 2016 John Kleinberg Inherent Trade-Offs in the Fair Determination of Risk Scores .","title":"Summaries of Paper &amp; Website"},{"location":"Pillars/Fairness/#taxonomy","text":"metric description unit weight Disparate Treatment Depending on the context certain features (gender, religion, race) are considered to be protected. Does the model take Disparate Impact","title":"Taxonomy"},{"location":"Pillars/Robustness/","text":"Robustness Summary Machine Learning models might not be robust against adversarial perturbations. With visually imperceptible adversarial perturbations, attackers may cause an input to be misclassified. These attacks can be generated in both the black-box setting, where the parameters of the model are unknown to the attacker, and the white-box settings, where attacker have the all necessary information about the model. To trust AI we need to be sure that it is robust against adversarial attacks. Sources Websites IBM Robustness: Robustness main page Adversarial Robustness 360: Toolbox Adversarial Robustness Toolbox Github repo: ART v1.5 CLEVER Github repo: Robustness Score Papers Adversarial Robustness Toolbox v1.0.0 Evaluating The Robustness of Neural Networks: An Extreme Value Theory Approach Efficient Neural Network Robustness Certification with General Activation Functions Robust Local Features For Improving The Generalization of Adversarial Training Analyzing Federated Learning through an Adversarial Lens Formal Guarantees on the Robustness of a Classifier against Adversarial Manipulation Metrics The main idea of the robustness metrics is to calculate the minimal perturbation that is required to get an input misclassified. In other words to calculate the average sensitivity of the model\u2019s loss function with respect to changes in the inputs. Empirical Robustness Assesses the robustness of a given classifier with respect to a specific attack and test data set. It is equivalent to the average minimal perturbation that the attacker needs to introduce for a successful attack. To be able to calculate empirical robustness we need to know the type and the parameters of the attack. Loss Sensitivity Local loss sensitivity quantify the smoothness of a model by estimating its Lipschitz continuity constant. Lipschitz constant measures the largest variation of the output of the model under a small change in its input. The smaller the value, the smoother the function. CLEVER Score For a given input CLEVER estimates the minimal perturbation that is required to change the classification. The derivation is based on a Lipschitz constant. The CLEVER algorithm uses an estimate based on extreme value theory. The CLEVER score can be calculated for both untargeted and targeted attacks. A higher CLEVER score indicates better robustness. Loss Sensitivity VS CLEVER Score CLEVER Score and Loss Sensitivity both uses the same idea of estimating the smoothness by Lipschitz constant. They both are attack-independent. But they have a couple of differences. Loss sensitivity uses Local Lipschitz constant estimation meanwhile CLEVER uses Cross Lipschitz constant estimation. Even though they try to estimate similar constants, Cross Lipschitz constant calculation includes an additional difference operation. Hence a model with a high CLEVER score would have a low loss sensitivity. For Loss sensitivity; the smaller the value, the smoother the function. For CLEVER score; the greater the value, the more robust the model. As CLEVER uses a sampling based method, the scores may vary slightly for different runs while Loss Sensitivity calculation is determined. Clique Method Robustness Verification for Decision Tree Ensembles For decision-tree ensemble models like Gradient Boosted Decision Trees, Random Forest, or Extra Trees robustness can be verified based on clique method. For our purposes this measure might be too specific. CROWN framework for certifying neural networks CROWN is a framework for efficiently computing a certified lower bound of minimum adversarial distortion given any input data point for neural networks. To calculate the lower bound one must know about the activation function used in the Neural Network. Then check where the activation function is concave or convex and choose lower/upper bound functions accordingly. Not a fully designed metric, just gives an upper bound with respect to the given data point.","title":"Robustness"},{"location":"Pillars/Robustness/#robustness","text":"","title":"Robustness"},{"location":"Pillars/Robustness/#summary","text":"Machine Learning models might not be robust against adversarial perturbations. With visually imperceptible adversarial perturbations, attackers may cause an input to be misclassified. These attacks can be generated in both the black-box setting, where the parameters of the model are unknown to the attacker, and the white-box settings, where attacker have the all necessary information about the model. To trust AI we need to be sure that it is robust against adversarial attacks.","title":"Summary"},{"location":"Pillars/Robustness/#sources","text":"","title":"Sources"},{"location":"Pillars/Robustness/#websites","text":"IBM Robustness: Robustness main page Adversarial Robustness 360: Toolbox Adversarial Robustness Toolbox Github repo: ART v1.5 CLEVER Github repo: Robustness Score","title":"Websites"},{"location":"Pillars/Robustness/#papers","text":"Adversarial Robustness Toolbox v1.0.0 Evaluating The Robustness of Neural Networks: An Extreme Value Theory Approach Efficient Neural Network Robustness Certification with General Activation Functions Robust Local Features For Improving The Generalization of Adversarial Training Analyzing Federated Learning through an Adversarial Lens Formal Guarantees on the Robustness of a Classifier against Adversarial Manipulation","title":"Papers"},{"location":"Pillars/Robustness/#metrics","text":"The main idea of the robustness metrics is to calculate the minimal perturbation that is required to get an input misclassified. In other words to calculate the average sensitivity of the model\u2019s loss function with respect to changes in the inputs.","title":"Metrics"},{"location":"Pillars/Robustness/#empirical-robustness","text":"Assesses the robustness of a given classifier with respect to a specific attack and test data set. It is equivalent to the average minimal perturbation that the attacker needs to introduce for a successful attack. To be able to calculate empirical robustness we need to know the type and the parameters of the attack.","title":"Empirical Robustness"},{"location":"Pillars/Robustness/#loss-sensitivity","text":"Local loss sensitivity quantify the smoothness of a model by estimating its Lipschitz continuity constant. Lipschitz constant measures the largest variation of the output of the model under a small change in its input. The smaller the value, the smoother the function.","title":"Loss Sensitivity"},{"location":"Pillars/Robustness/#clever-score","text":"For a given input CLEVER estimates the minimal perturbation that is required to change the classification. The derivation is based on a Lipschitz constant. The CLEVER algorithm uses an estimate based on extreme value theory. The CLEVER score can be calculated for both untargeted and targeted attacks. A higher CLEVER score indicates better robustness.","title":"CLEVER Score"},{"location":"Pillars/Robustness/#loss-sensitivity-vs-clever-score","text":"CLEVER Score and Loss Sensitivity both uses the same idea of estimating the smoothness by Lipschitz constant. They both are attack-independent. But they have a couple of differences. Loss sensitivity uses Local Lipschitz constant estimation meanwhile CLEVER uses Cross Lipschitz constant estimation. Even though they try to estimate similar constants, Cross Lipschitz constant calculation includes an additional difference operation. Hence a model with a high CLEVER score would have a low loss sensitivity. For Loss sensitivity; the smaller the value, the smoother the function. For CLEVER score; the greater the value, the more robust the model. As CLEVER uses a sampling based method, the scores may vary slightly for different runs while Loss Sensitivity calculation is determined.","title":"Loss Sensitivity VS CLEVER Score"},{"location":"Pillars/Robustness/#clique-method-robustness-verification-for-decision-tree-ensembles","text":"For decision-tree ensemble models like Gradient Boosted Decision Trees, Random Forest, or Extra Trees robustness can be verified based on clique method. For our purposes this measure might be too specific.","title":"Clique Method Robustness Verification for Decision Tree Ensembles"},{"location":"Pillars/Robustness/#crown-framework-for-certifying-neural-networks","text":"CROWN is a framework for efficiently computing a certified lower bound of minimum adversarial distortion given any input data point for neural networks. To calculate the lower bound one must know about the activation function used in the Neural Network. Then check where the activation function is concave or convex and choose lower/upper bound functions accordingly. Not a fully designed metric, just gives an upper bound with respect to the given data point.","title":"CROWN framework for certifying neural networks"}]}