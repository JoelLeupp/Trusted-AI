{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Trusted AI Master Project This site serves as documentation and Overview of the Master Project. Introduction Artificial intelligence (AI) systems are getting more and more relevance as support to human decision-making processes. While AI holds the promise of delivering valuable insights into many application scenarios, the broad adoption of AI systems will rely on the ability to trust their decisions. According to IBM, some key aspects provide information to trust (or not) a decision made by an algorithm. In particular, a decision must: be reliable and fair, be accounted for, not cause harm, not be tampered with, be understandable, and be secure. In this context, the previous aspects have been grouped into the following pillars. \u2022 Fairness. One of the essential aspects to enable trusted AI is to avoid bias across the entire lifecycle of AI applications, as well as across different bias and data types. \u2022 Robustness. AI-based systems may be vulnerable to adversarial attacks. Attackers may poison training data by injecting samples to compromise system decisions eventually. \u2022 Explainability. In many application scenarios, decisions made by algorithms must be explainable and understandable by humans. Various stakeholders require explanations for different purposes, and explanations must be tailored to their needs. \u2022 Accountability. The quality of decisions should be evaluated in terms of accuracy, ability to satisfy users\u2019 preferences, as well as other properties related to the impact of the decision.The previous characteristics are demanded from AI systems. Yet, to achieve trust in AI, more efforts and automatic mechanisms able to calculate and communicate trust levels are required. Project Documentation The whole documentation of the project can be found on our GitHub page mkdocs The documentation is created with mkdocs, which uses simple markdown file and a .yml config file to build and deploy the documentation to GitHub. The relevant files to create the documentation are in the mkdocs folder The file mkdocs.yml is the config file and has to be adapted if the theme should change or the index changes or new markdowns added. The markdown files are in the folder docs and include the whole documentation. installation pip install --upgrade pip pip install mkdocs workflow: At the location of the mkdocs folder open the terminal and enter: mkdocs serve This will build the documentation and host it locally, where the full documentation can be seen. change the config or markdown files. Sinc you are serving the documentation locally you can see the changes in the config or markdown files in real time. You will always see the output and how the documentation will look in the end and therefore minimalize syntax errors in markdown for example. deploy the build on GitHub to the branche gh-pages. For this open another termianl at the localtion of the mkdocs folder and enter: mkdocs gh-deploy","title":"Home"},{"location":"#trusted-ai-master-project","text":"This site serves as documentation and Overview of the Master Project.","title":"Trusted AI Master Project"},{"location":"#introduction","text":"Artificial intelligence (AI) systems are getting more and more relevance as support to human decision-making processes. While AI holds the promise of delivering valuable insights into many application scenarios, the broad adoption of AI systems will rely on the ability to trust their decisions. According to IBM, some key aspects provide information to trust (or not) a decision made by an algorithm. In particular, a decision must: be reliable and fair, be accounted for, not cause harm, not be tampered with, be understandable, and be secure. In this context, the previous aspects have been grouped into the following pillars. \u2022 Fairness. One of the essential aspects to enable trusted AI is to avoid bias across the entire lifecycle of AI applications, as well as across different bias and data types. \u2022 Robustness. AI-based systems may be vulnerable to adversarial attacks. Attackers may poison training data by injecting samples to compromise system decisions eventually. \u2022 Explainability. In many application scenarios, decisions made by algorithms must be explainable and understandable by humans. Various stakeholders require explanations for different purposes, and explanations must be tailored to their needs. \u2022 Accountability. The quality of decisions should be evaluated in terms of accuracy, ability to satisfy users\u2019 preferences, as well as other properties related to the impact of the decision.The previous characteristics are demanded from AI systems. Yet, to achieve trust in AI, more efforts and automatic mechanisms able to calculate and communicate trust levels are required.","title":"Introduction"},{"location":"#project-documentation","text":"The whole documentation of the project can be found on our GitHub page","title":"Project Documentation"},{"location":"#mkdocs","text":"The documentation is created with mkdocs, which uses simple markdown file and a .yml config file to build and deploy the documentation to GitHub. The relevant files to create the documentation are in the mkdocs folder The file mkdocs.yml is the config file and has to be adapted if the theme should change or the index changes or new markdowns added. The markdown files are in the folder docs and include the whole documentation. installation pip install --upgrade pip pip install mkdocs workflow: At the location of the mkdocs folder open the terminal and enter: mkdocs serve This will build the documentation and host it locally, where the full documentation can be seen. change the config or markdown files. Sinc you are serving the documentation locally you can see the changes in the config or markdown files in real time. You will always see the output and how the documentation will look in the end and therefore minimalize syntax errors in markdown for example. deploy the build on GitHub to the branche gh-pages. For this open another termianl at the localtion of the mkdocs folder and enter: mkdocs gh-deploy","title":"mkdocs"},{"location":"AI_algo/","text":"Trusted AI Algorithm Basic Idea Taxonomy Architecture","title":"trusted AI algorithm"},{"location":"AI_algo/#trusted-ai-algorithm","text":"","title":"Trusted AI Algorithm"},{"location":"AI_algo/#basic-idea","text":"","title":"Basic Idea"},{"location":"AI_algo/#taxonomy","text":"","title":"Taxonomy"},{"location":"AI_algo/#architecture","text":"","title":"Architecture"},{"location":"code_documentation/","text":"Implementation of the trusted AI Algorithm General Architecture File Structure Build How it works Evaluation and Results","title":"Implementation"},{"location":"code_documentation/#implementation-of-the-trusted-ai-algorithm","text":"","title":"Implementation of the trusted AI Algorithm"},{"location":"code_documentation/#general-architecture","text":"","title":"General Architecture"},{"location":"code_documentation/#file-structure","text":"","title":"File Structure"},{"location":"code_documentation/#build","text":"","title":"Build"},{"location":"code_documentation/#how-it-works","text":"","title":"How it works"},{"location":"code_documentation/#evaluation-and-results","text":"","title":"Evaluation and Results"},{"location":"sources/","text":"Overview This is a collection of all material that might be interesting or relevant regarding the project. Papers AI fairness 360: Bellamy, R. K., et al. (2018). AI Fairness 360: An Extensible Toolkit for Detecting, Understanding, and Mitigating Unwanted Algorithmic Bias. arXiv, October 2018, arXiv:1810.01943. pdf version YouTube Videos Add here a list of usefull and interesing YT videos Decison making: Can we trust artificial intelligence to make decisions for us? Websites IBM trusted AI main page: AI Research","title":"sources"},{"location":"sources/#overview","text":"This is a collection of all material that might be interesting or relevant regarding the project.","title":"Overview"},{"location":"sources/#papers","text":"AI fairness 360: Bellamy, R. K., et al. (2018). AI Fairness 360: An Extensible Toolkit for Detecting, Understanding, and Mitigating Unwanted Algorithmic Bias. arXiv, October 2018, arXiv:1810.01943. pdf version","title":"Papers"},{"location":"sources/#youtube-videos","text":"Add here a list of usefull and interesing YT videos Decison making: Can we trust artificial intelligence to make decisions for us?","title":"YouTube Videos"},{"location":"sources/#websites","text":"IBM trusted AI main page: AI Research","title":"Websites"},{"location":"Pillars/Accountability/","text":"Accountability Summary Links Taxonomy and Metrics","title":"Accountability"},{"location":"Pillars/Accountability/#accountability","text":"","title":"Accountability"},{"location":"Pillars/Accountability/#summary","text":"","title":"Summary"},{"location":"Pillars/Accountability/#links","text":"","title":"Links"},{"location":"Pillars/Accountability/#taxonomy-and-metrics","text":"","title":"Taxonomy and Metrics"},{"location":"Pillars/Explainability/","text":"Explainability Summary In many applications, trust in an AI system will come from its ability to \u2018explain itself.\u2019 Yet, when it comes to understanding and explaining the inner workings of an algorithm, one size does not fit all. Different stakeholders require explanations for different purposes and objectives, and explanations must be tailored to their needs. A physician might respond best to seeing examples of patient data similar to their patient\u2019s. On the other hand, a developer training a neural net will benefit from seeing how information flows through the algorithm. While a regulator will aim to understand the system as a whole and probe into its logic, consumers affected by a specific decision will be interested only in factors impacting their case \u2013 for example, in a loan processing application, they will expect an explanation for why the request was denied and want to understand what changes could lead to approval. IBM Research is creating diverse explanations, including training highly optimized directly interpretable models, creating contrastive explanations of black box models, using information flow in a high-performing complex model to train simpler, interpretable classifiers, learning disentangled representations, and visualizing information flows in neural networks. sources Websites IBM explainability: explainability main page AI Explainability 360: Toolkit page AI Explainability GitHub repo: AIX360 Papers towards Robust Interpretability paper Generating Contrastive Explanations withMonotonic Attribute Functions paper Videos Taxonomy Construct a set of measurable metrict which can be used to calculate a score that should indicate how good the explainability of a model is. For The calculation of the score differnt weights can be assigned to the metrics. table { width:100%; } metric description unit weight model type some models like linear regression or decision tree are very explainable and models like neural networks rather not boolean) 0.6 faithfulness relevance are the features truly relevant or can some be omitted? [0,1] 0.2 monotonicity monotonic attribute functions most important feature for classification [-1,1] 0.2","title":"Explainability"},{"location":"Pillars/Explainability/#explainability","text":"","title":"Explainability"},{"location":"Pillars/Explainability/#summary","text":"In many applications, trust in an AI system will come from its ability to \u2018explain itself.\u2019 Yet, when it comes to understanding and explaining the inner workings of an algorithm, one size does not fit all. Different stakeholders require explanations for different purposes and objectives, and explanations must be tailored to their needs. A physician might respond best to seeing examples of patient data similar to their patient\u2019s. On the other hand, a developer training a neural net will benefit from seeing how information flows through the algorithm. While a regulator will aim to understand the system as a whole and probe into its logic, consumers affected by a specific decision will be interested only in factors impacting their case \u2013 for example, in a loan processing application, they will expect an explanation for why the request was denied and want to understand what changes could lead to approval. IBM Research is creating diverse explanations, including training highly optimized directly interpretable models, creating contrastive explanations of black box models, using information flow in a high-performing complex model to train simpler, interpretable classifiers, learning disentangled representations, and visualizing information flows in neural networks.","title":"Summary"},{"location":"Pillars/Explainability/#sources","text":"","title":"sources"},{"location":"Pillars/Explainability/#websites","text":"IBM explainability: explainability main page AI Explainability 360: Toolkit page AI Explainability GitHub repo: AIX360","title":"Websites"},{"location":"Pillars/Explainability/#papers","text":"towards Robust Interpretability paper Generating Contrastive Explanations withMonotonic Attribute Functions paper","title":"Papers"},{"location":"Pillars/Explainability/#videos","text":"","title":"Videos"},{"location":"Pillars/Explainability/#taxonomy","text":"Construct a set of measurable metrict which can be used to calculate a score that should indicate how good the explainability of a model is. For The calculation of the score differnt weights can be assigned to the metrics. table { width:100%; } metric description unit weight model type some models like linear regression or decision tree are very explainable and models like neural networks rather not boolean) 0.6 faithfulness relevance are the features truly relevant or can some be omitted? [0,1] 0.2 monotonicity monotonic attribute functions most important feature for classification [-1,1] 0.2","title":"Taxonomy"},{"location":"Pillars/Fairness/","text":"Fairness Summary Some text sanity checks link Links Metrics","title":"Fairness"},{"location":"Pillars/Fairness/#fairness","text":"","title":"Fairness"},{"location":"Pillars/Fairness/#summary","text":"Some text sanity checks link","title":"Summary"},{"location":"Pillars/Fairness/#links","text":"","title":"Links"},{"location":"Pillars/Fairness/#metrics","text":"","title":"Metrics"},{"location":"Pillars/Robustness/","text":"Robustness Summary Machine Learning models might not be robust against adversarial perturbations. With visually imperceptible adversarial perturbations, attackers may cause an input to be misclassified. These attacks can be generated in both the black-box setting, where the parameters of the model are unknown to the attacker, and the white-box settings, where attacker have the all necessary information about the model. To trust AI we need to be sure that it is robust against adversarial attacks. Sources Websites IBM Robustness: Robustness main page Adversarial Robustness 360: Toolbox Adversarial Robustness Toolbox Github repo: ART v1.5 CLEVER Github repo: Robustness Score Papers Adversarial Robustness Toolbox v1.0.0 Evaluating The Robustness of Neural Networks: An Extreme Value Theory Approach Efficient Neural Network Robustness Certification with General Activation Functions Robust Local Features For Improving The Generalization of Adversarial Training Analyzing Federated Learning through an Adversarial Lens Formal Guarantees on the Robustness of a Classifier against Adversarial Manipulation Metrics The main idea of the robustness metrics is to calculate the minimal perturbation that is required to get an input misclassified. In other words to calculate the average sensitivity of the model\u2019s loss function with respect to changes in the inputs. Empirical Robustness Assesses the robustness of a given classifier with respect to a specific attack and test data set. It is equivalent to the average minimal perturbation that the attacker needs to introduce for a successful attack. To be able to calculate empirical robustness we need to know the type and the parameters of the attack. Loss Sensitivity Local loss sensitivity quantify the smoothness of a model by estimating its Lipschitz continuity constant. Lipschitz constant measures the largest variation of the output of the model under a small change in its input. The smaller the value, the smoother the function. CLEVER Score For a given input CLEVER estimates the minimal perturbation that is required to change the classification. The derivation is based on a Lipschitz constant. The CLEVER algorithm uses an estimate based on extreme value theory. The CLEVER score can be calculated for both untargeted and targeted attacks. A higher CLEVER score indicates better robustness. Loss Sensitivity VS CLEVER Score CLEVER Score and Loss Sensitivity both uses the same idea of estimating the smoothness by Lipschitz constant. They both are attack-independent. But they have a couple of differences. Loss sensitivity uses Local Lipschitz constant estimation meanwhile CLEVER uses Cross Lipschitz constant estimation. Even though they try to estimate similar constants, Cross Lipschitz constant calculation includes an additional difference operation. Hence a model with a high CLEVER score would have a low loss sensitivity. For Loss sensitivity; the smaller the value, the smoother the function. For CLEVER score; the greater the value, the more robust the model. As CLEVER uses a sampling based method, the scores may vary slightly for different runs while Loss Sensitivity calculation is determined. Clique Method Robustness Verification for Decision Tree Ensembles For decision-tree ensemble models like Gradient Boosted Decision Trees, Random Forest, or Extra Trees robustness can be verified based on clique method. For our purposes this measure might be too specific. CROWN framework for certifying neural networks CROWN is a framework for efficiently computing a certified lower bound of minimum adversarial distortion given any input data point for neural networks. To calculate the lower bound one must know about the activation function used in the Neural Network. Then check where the activation function is concave or convex and choose lower/upper bound functions accordingly. Not a fully designed metric, just gives an upper bound with respect to the given data point.","title":"Robustness"},{"location":"Pillars/Robustness/#robustness","text":"","title":"Robustness"},{"location":"Pillars/Robustness/#summary","text":"Machine Learning models might not be robust against adversarial perturbations. With visually imperceptible adversarial perturbations, attackers may cause an input to be misclassified. These attacks can be generated in both the black-box setting, where the parameters of the model are unknown to the attacker, and the white-box settings, where attacker have the all necessary information about the model. To trust AI we need to be sure that it is robust against adversarial attacks.","title":"Summary"},{"location":"Pillars/Robustness/#sources","text":"","title":"Sources"},{"location":"Pillars/Robustness/#websites","text":"IBM Robustness: Robustness main page Adversarial Robustness 360: Toolbox Adversarial Robustness Toolbox Github repo: ART v1.5 CLEVER Github repo: Robustness Score","title":"Websites"},{"location":"Pillars/Robustness/#papers","text":"Adversarial Robustness Toolbox v1.0.0 Evaluating The Robustness of Neural Networks: An Extreme Value Theory Approach Efficient Neural Network Robustness Certification with General Activation Functions Robust Local Features For Improving The Generalization of Adversarial Training Analyzing Federated Learning through an Adversarial Lens Formal Guarantees on the Robustness of a Classifier against Adversarial Manipulation","title":"Papers"},{"location":"Pillars/Robustness/#metrics","text":"The main idea of the robustness metrics is to calculate the minimal perturbation that is required to get an input misclassified. In other words to calculate the average sensitivity of the model\u2019s loss function with respect to changes in the inputs.","title":"Metrics"},{"location":"Pillars/Robustness/#empirical-robustness","text":"Assesses the robustness of a given classifier with respect to a specific attack and test data set. It is equivalent to the average minimal perturbation that the attacker needs to introduce for a successful attack. To be able to calculate empirical robustness we need to know the type and the parameters of the attack.","title":"Empirical Robustness"},{"location":"Pillars/Robustness/#loss-sensitivity","text":"Local loss sensitivity quantify the smoothness of a model by estimating its Lipschitz continuity constant. Lipschitz constant measures the largest variation of the output of the model under a small change in its input. The smaller the value, the smoother the function.","title":"Loss Sensitivity"},{"location":"Pillars/Robustness/#clever-score","text":"For a given input CLEVER estimates the minimal perturbation that is required to change the classification. The derivation is based on a Lipschitz constant. The CLEVER algorithm uses an estimate based on extreme value theory. The CLEVER score can be calculated for both untargeted and targeted attacks. A higher CLEVER score indicates better robustness.","title":"CLEVER Score"},{"location":"Pillars/Robustness/#loss-sensitivity-vs-clever-score","text":"CLEVER Score and Loss Sensitivity both uses the same idea of estimating the smoothness by Lipschitz constant. They both are attack-independent. But they have a couple of differences. Loss sensitivity uses Local Lipschitz constant estimation meanwhile CLEVER uses Cross Lipschitz constant estimation. Even though they try to estimate similar constants, Cross Lipschitz constant calculation includes an additional difference operation. Hence a model with a high CLEVER score would have a low loss sensitivity. For Loss sensitivity; the smaller the value, the smoother the function. For CLEVER score; the greater the value, the more robust the model. As CLEVER uses a sampling based method, the scores may vary slightly for different runs while Loss Sensitivity calculation is determined.","title":"Loss Sensitivity VS CLEVER Score"},{"location":"Pillars/Robustness/#clique-method-robustness-verification-for-decision-tree-ensembles","text":"For decision-tree ensemble models like Gradient Boosted Decision Trees, Random Forest, or Extra Trees robustness can be verified based on clique method. For our purposes this measure might be too specific.","title":"Clique Method Robustness Verification for Decision Tree Ensembles"},{"location":"Pillars/Robustness/#crown-framework-for-certifying-neural-networks","text":"CROWN is a framework for efficiently computing a certified lower bound of minimum adversarial distortion given any input data point for neural networks. To calculate the lower bound one must know about the activation function used in the Neural Network. Then check where the activation function is concave or convex and choose lower/upper bound functions accordingly. Not a fully designed metric, just gives an upper bound with respect to the given data point.","title":"CROWN framework for certifying neural networks"}]}